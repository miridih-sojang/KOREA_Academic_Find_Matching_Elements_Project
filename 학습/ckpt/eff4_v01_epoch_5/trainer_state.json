{
  "best_metric": 1.9116169214248657,
  "best_model_checkpoint": "./ckpt/eff4_v01_epoch:5/checkpoint-3950",
  "epoch": 5.0,
  "eval_steps": 395,
  "global_step": 3950,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0012658227848101266,
      "grad_norm": 2.905583620071411,
      "learning_rate": 1.2658227848101266e-07,
      "loss": 3.9171,
      "step": 1
    },
    {
      "epoch": 0.002531645569620253,
      "grad_norm": 2.798473358154297,
      "learning_rate": 2.5316455696202533e-07,
      "loss": 3.8718,
      "step": 2
    },
    {
      "epoch": 0.0037974683544303796,
      "grad_norm": 2.8599720001220703,
      "learning_rate": 3.79746835443038e-07,
      "loss": 3.8844,
      "step": 3
    },
    {
      "epoch": 0.005063291139240506,
      "grad_norm": 2.8425495624542236,
      "learning_rate": 5.063291139240507e-07,
      "loss": 3.8524,
      "step": 4
    },
    {
      "epoch": 0.006329113924050633,
      "grad_norm": 2.840237617492676,
      "learning_rate": 6.329113924050633e-07,
      "loss": 3.8604,
      "step": 5
    },
    {
      "epoch": 0.007594936708860759,
      "grad_norm": 2.8532681465148926,
      "learning_rate": 7.59493670886076e-07,
      "loss": 3.8975,
      "step": 6
    },
    {
      "epoch": 0.008860759493670886,
      "grad_norm": 2.9072930812835693,
      "learning_rate": 8.860759493670887e-07,
      "loss": 3.8991,
      "step": 7
    },
    {
      "epoch": 0.010126582278481013,
      "grad_norm": 2.814236879348755,
      "learning_rate": 1.0126582278481013e-06,
      "loss": 3.8405,
      "step": 8
    },
    {
      "epoch": 0.01139240506329114,
      "grad_norm": 2.745098114013672,
      "learning_rate": 1.139240506329114e-06,
      "loss": 3.9022,
      "step": 9
    },
    {
      "epoch": 0.012658227848101266,
      "grad_norm": 2.9326133728027344,
      "learning_rate": 1.2658227848101265e-06,
      "loss": 3.8852,
      "step": 10
    },
    {
      "epoch": 0.013924050632911392,
      "grad_norm": 2.902797222137451,
      "learning_rate": 1.3924050632911392e-06,
      "loss": 3.9395,
      "step": 11
    },
    {
      "epoch": 0.015189873417721518,
      "grad_norm": 2.888711452484131,
      "learning_rate": 1.518987341772152e-06,
      "loss": 3.8867,
      "step": 12
    },
    {
      "epoch": 0.016455696202531647,
      "grad_norm": 2.7016804218292236,
      "learning_rate": 1.6455696202531647e-06,
      "loss": 3.9294,
      "step": 13
    },
    {
      "epoch": 0.017721518987341773,
      "grad_norm": 2.7301719188690186,
      "learning_rate": 1.7721518987341774e-06,
      "loss": 3.8577,
      "step": 14
    },
    {
      "epoch": 0.0189873417721519,
      "grad_norm": 2.835475444793701,
      "learning_rate": 1.8987341772151901e-06,
      "loss": 3.8742,
      "step": 15
    },
    {
      "epoch": 0.020253164556962026,
      "grad_norm": 2.763671636581421,
      "learning_rate": 2.0253164556962026e-06,
      "loss": 3.8775,
      "step": 16
    },
    {
      "epoch": 0.021518987341772152,
      "grad_norm": 2.6188690662384033,
      "learning_rate": 2.1518987341772153e-06,
      "loss": 3.8599,
      "step": 17
    },
    {
      "epoch": 0.02278481012658228,
      "grad_norm": 2.6933534145355225,
      "learning_rate": 2.278481012658228e-06,
      "loss": 3.833,
      "step": 18
    },
    {
      "epoch": 0.024050632911392405,
      "grad_norm": 2.623138427734375,
      "learning_rate": 2.4050632911392408e-06,
      "loss": 3.8159,
      "step": 19
    },
    {
      "epoch": 0.02531645569620253,
      "grad_norm": 2.8409650325775146,
      "learning_rate": 2.531645569620253e-06,
      "loss": 3.8718,
      "step": 20
    },
    {
      "epoch": 0.026582278481012658,
      "grad_norm": 2.589780330657959,
      "learning_rate": 2.6582278481012658e-06,
      "loss": 3.8487,
      "step": 21
    },
    {
      "epoch": 0.027848101265822784,
      "grad_norm": 2.668313980102539,
      "learning_rate": 2.7848101265822785e-06,
      "loss": 3.8345,
      "step": 22
    },
    {
      "epoch": 0.02911392405063291,
      "grad_norm": 2.708763360977173,
      "learning_rate": 2.9113924050632912e-06,
      "loss": 3.8545,
      "step": 23
    },
    {
      "epoch": 0.030379746835443037,
      "grad_norm": 2.494757890701294,
      "learning_rate": 3.037974683544304e-06,
      "loss": 3.787,
      "step": 24
    },
    {
      "epoch": 0.03164556962025317,
      "grad_norm": 2.6630289554595947,
      "learning_rate": 3.1645569620253167e-06,
      "loss": 3.8378,
      "step": 25
    },
    {
      "epoch": 0.03291139240506329,
      "grad_norm": 2.4473683834075928,
      "learning_rate": 3.2911392405063294e-06,
      "loss": 3.7931,
      "step": 26
    },
    {
      "epoch": 0.03417721518987342,
      "grad_norm": 2.5152831077575684,
      "learning_rate": 3.417721518987342e-06,
      "loss": 3.7853,
      "step": 27
    },
    {
      "epoch": 0.035443037974683546,
      "grad_norm": 2.5315709114074707,
      "learning_rate": 3.544303797468355e-06,
      "loss": 3.8539,
      "step": 28
    },
    {
      "epoch": 0.03670886075949367,
      "grad_norm": 2.5223870277404785,
      "learning_rate": 3.6708860759493675e-06,
      "loss": 3.8169,
      "step": 29
    },
    {
      "epoch": 0.0379746835443038,
      "grad_norm": 2.440462589263916,
      "learning_rate": 3.7974683544303802e-06,
      "loss": 3.7894,
      "step": 30
    },
    {
      "epoch": 0.039240506329113925,
      "grad_norm": 2.440858840942383,
      "learning_rate": 3.924050632911393e-06,
      "loss": 3.7888,
      "step": 31
    },
    {
      "epoch": 0.04050632911392405,
      "grad_norm": 2.4142370223999023,
      "learning_rate": 4.050632911392405e-06,
      "loss": 3.8287,
      "step": 32
    },
    {
      "epoch": 0.04177215189873418,
      "grad_norm": 2.394562005996704,
      "learning_rate": 4.177215189873418e-06,
      "loss": 3.7826,
      "step": 33
    },
    {
      "epoch": 0.043037974683544304,
      "grad_norm": 2.458601474761963,
      "learning_rate": 4.303797468354431e-06,
      "loss": 3.8145,
      "step": 34
    },
    {
      "epoch": 0.04430379746835443,
      "grad_norm": 2.3302807807922363,
      "learning_rate": 4.430379746835443e-06,
      "loss": 3.7574,
      "step": 35
    },
    {
      "epoch": 0.04556962025316456,
      "grad_norm": 2.389187812805176,
      "learning_rate": 4.556962025316456e-06,
      "loss": 3.7704,
      "step": 36
    },
    {
      "epoch": 0.04683544303797468,
      "grad_norm": 2.3500194549560547,
      "learning_rate": 4.683544303797468e-06,
      "loss": 3.7373,
      "step": 37
    },
    {
      "epoch": 0.04810126582278481,
      "grad_norm": 2.3300817012786865,
      "learning_rate": 4.8101265822784815e-06,
      "loss": 3.7367,
      "step": 38
    },
    {
      "epoch": 0.049367088607594936,
      "grad_norm": 2.237638473510742,
      "learning_rate": 4.936708860759494e-06,
      "loss": 3.7084,
      "step": 39
    },
    {
      "epoch": 0.05063291139240506,
      "grad_norm": 2.362610101699829,
      "learning_rate": 5.063291139240506e-06,
      "loss": 3.7672,
      "step": 40
    },
    {
      "epoch": 0.05189873417721519,
      "grad_norm": 2.2785251140594482,
      "learning_rate": 5.189873417721519e-06,
      "loss": 3.7008,
      "step": 41
    },
    {
      "epoch": 0.053164556962025315,
      "grad_norm": 2.224292039871216,
      "learning_rate": 5.3164556962025316e-06,
      "loss": 3.7304,
      "step": 42
    },
    {
      "epoch": 0.05443037974683544,
      "grad_norm": 2.246321201324463,
      "learning_rate": 5.443037974683545e-06,
      "loss": 3.7053,
      "step": 43
    },
    {
      "epoch": 0.05569620253164557,
      "grad_norm": 2.1794655323028564,
      "learning_rate": 5.569620253164557e-06,
      "loss": 3.6836,
      "step": 44
    },
    {
      "epoch": 0.056962025316455694,
      "grad_norm": 2.2400028705596924,
      "learning_rate": 5.69620253164557e-06,
      "loss": 3.6907,
      "step": 45
    },
    {
      "epoch": 0.05822784810126582,
      "grad_norm": 2.3072409629821777,
      "learning_rate": 5.8227848101265824e-06,
      "loss": 3.7249,
      "step": 46
    },
    {
      "epoch": 0.05949367088607595,
      "grad_norm": 2.2192416191101074,
      "learning_rate": 5.949367088607595e-06,
      "loss": 3.6829,
      "step": 47
    },
    {
      "epoch": 0.060759493670886074,
      "grad_norm": 2.1521615982055664,
      "learning_rate": 6.075949367088608e-06,
      "loss": 3.7171,
      "step": 48
    },
    {
      "epoch": 0.0620253164556962,
      "grad_norm": 2.252589702606201,
      "learning_rate": 6.20253164556962e-06,
      "loss": 3.6704,
      "step": 49
    },
    {
      "epoch": 0.06329113924050633,
      "grad_norm": 2.224806308746338,
      "learning_rate": 6.329113924050633e-06,
      "loss": 3.6675,
      "step": 50
    },
    {
      "epoch": 0.06455696202531645,
      "grad_norm": 2.1747798919677734,
      "learning_rate": 6.455696202531646e-06,
      "loss": 3.6597,
      "step": 51
    },
    {
      "epoch": 0.06582278481012659,
      "grad_norm": 2.165693521499634,
      "learning_rate": 6.582278481012659e-06,
      "loss": 3.6442,
      "step": 52
    },
    {
      "epoch": 0.0670886075949367,
      "grad_norm": 2.1918163299560547,
      "learning_rate": 6.708860759493671e-06,
      "loss": 3.5945,
      "step": 53
    },
    {
      "epoch": 0.06835443037974684,
      "grad_norm": 2.2083537578582764,
      "learning_rate": 6.835443037974684e-06,
      "loss": 3.713,
      "step": 54
    },
    {
      "epoch": 0.06962025316455696,
      "grad_norm": 2.188096046447754,
      "learning_rate": 6.9620253164556965e-06,
      "loss": 3.7022,
      "step": 55
    },
    {
      "epoch": 0.07088607594936709,
      "grad_norm": 2.217633008956909,
      "learning_rate": 7.08860759493671e-06,
      "loss": 3.6513,
      "step": 56
    },
    {
      "epoch": 0.07215189873417721,
      "grad_norm": 2.094730854034424,
      "learning_rate": 7.215189873417721e-06,
      "loss": 3.6203,
      "step": 57
    },
    {
      "epoch": 0.07341772151898734,
      "grad_norm": 2.288480758666992,
      "learning_rate": 7.341772151898735e-06,
      "loss": 3.669,
      "step": 58
    },
    {
      "epoch": 0.07468354430379746,
      "grad_norm": 2.0938189029693604,
      "learning_rate": 7.4683544303797465e-06,
      "loss": 3.6222,
      "step": 59
    },
    {
      "epoch": 0.0759493670886076,
      "grad_norm": 2.0701122283935547,
      "learning_rate": 7.5949367088607605e-06,
      "loss": 3.6072,
      "step": 60
    },
    {
      "epoch": 0.07721518987341772,
      "grad_norm": 2.0933544635772705,
      "learning_rate": 7.721518987341773e-06,
      "loss": 3.6378,
      "step": 61
    },
    {
      "epoch": 0.07848101265822785,
      "grad_norm": 2.077876567840576,
      "learning_rate": 7.848101265822786e-06,
      "loss": 3.6107,
      "step": 62
    },
    {
      "epoch": 0.07974683544303797,
      "grad_norm": 2.0541069507598877,
      "learning_rate": 7.974683544303797e-06,
      "loss": 3.5735,
      "step": 63
    },
    {
      "epoch": 0.0810126582278481,
      "grad_norm": 2.0846211910247803,
      "learning_rate": 8.10126582278481e-06,
      "loss": 3.6293,
      "step": 64
    },
    {
      "epoch": 0.08227848101265822,
      "grad_norm": 2.110057830810547,
      "learning_rate": 8.227848101265822e-06,
      "loss": 3.5788,
      "step": 65
    },
    {
      "epoch": 0.08354430379746836,
      "grad_norm": 2.122549295425415,
      "learning_rate": 8.354430379746837e-06,
      "loss": 3.6469,
      "step": 66
    },
    {
      "epoch": 0.08481012658227848,
      "grad_norm": 2.1061158180236816,
      "learning_rate": 8.481012658227848e-06,
      "loss": 3.6187,
      "step": 67
    },
    {
      "epoch": 0.08607594936708861,
      "grad_norm": 2.066627264022827,
      "learning_rate": 8.607594936708861e-06,
      "loss": 3.556,
      "step": 68
    },
    {
      "epoch": 0.08734177215189873,
      "grad_norm": 2.082672119140625,
      "learning_rate": 8.734177215189873e-06,
      "loss": 3.5775,
      "step": 69
    },
    {
      "epoch": 0.08860759493670886,
      "grad_norm": 2.152125835418701,
      "learning_rate": 8.860759493670886e-06,
      "loss": 3.6024,
      "step": 70
    },
    {
      "epoch": 0.08987341772151898,
      "grad_norm": 2.047595500946045,
      "learning_rate": 8.987341772151899e-06,
      "loss": 3.5901,
      "step": 71
    },
    {
      "epoch": 0.09113924050632911,
      "grad_norm": 2.1001737117767334,
      "learning_rate": 9.113924050632912e-06,
      "loss": 3.5839,
      "step": 72
    },
    {
      "epoch": 0.09240506329113925,
      "grad_norm": 2.0980706214904785,
      "learning_rate": 9.240506329113925e-06,
      "loss": 3.6345,
      "step": 73
    },
    {
      "epoch": 0.09367088607594937,
      "grad_norm": 2.078474998474121,
      "learning_rate": 9.367088607594937e-06,
      "loss": 3.5276,
      "step": 74
    },
    {
      "epoch": 0.0949367088607595,
      "grad_norm": 2.095402479171753,
      "learning_rate": 9.49367088607595e-06,
      "loss": 3.5549,
      "step": 75
    },
    {
      "epoch": 0.09620253164556962,
      "grad_norm": 2.14804744720459,
      "learning_rate": 9.620253164556963e-06,
      "loss": 3.5331,
      "step": 76
    },
    {
      "epoch": 0.09746835443037975,
      "grad_norm": 2.1524527072906494,
      "learning_rate": 9.746835443037976e-06,
      "loss": 3.5867,
      "step": 77
    },
    {
      "epoch": 0.09873417721518987,
      "grad_norm": 2.1271579265594482,
      "learning_rate": 9.873417721518988e-06,
      "loss": 3.5474,
      "step": 78
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.040296792984009,
      "learning_rate": 1e-05,
      "loss": 3.527,
      "step": 79
    },
    {
      "epoch": 0.10126582278481013,
      "grad_norm": 2.0181491374969482,
      "learning_rate": 1.0126582278481012e-05,
      "loss": 3.5151,
      "step": 80
    },
    {
      "epoch": 0.10253164556962026,
      "grad_norm": 2.0581533908843994,
      "learning_rate": 1.0253164556962027e-05,
      "loss": 3.52,
      "step": 81
    },
    {
      "epoch": 0.10379746835443038,
      "grad_norm": 2.047852039337158,
      "learning_rate": 1.0379746835443039e-05,
      "loss": 3.4822,
      "step": 82
    },
    {
      "epoch": 0.10506329113924051,
      "grad_norm": 2.029505729675293,
      "learning_rate": 1.0506329113924052e-05,
      "loss": 3.5304,
      "step": 83
    },
    {
      "epoch": 0.10632911392405063,
      "grad_norm": 2.0626564025878906,
      "learning_rate": 1.0632911392405063e-05,
      "loss": 3.5041,
      "step": 84
    },
    {
      "epoch": 0.10759493670886076,
      "grad_norm": 2.036571979522705,
      "learning_rate": 1.0759493670886076e-05,
      "loss": 3.5196,
      "step": 85
    },
    {
      "epoch": 0.10886075949367088,
      "grad_norm": 2.109434127807617,
      "learning_rate": 1.088607594936709e-05,
      "loss": 3.5036,
      "step": 86
    },
    {
      "epoch": 0.11012658227848102,
      "grad_norm": 2.014617919921875,
      "learning_rate": 1.1012658227848103e-05,
      "loss": 3.5246,
      "step": 87
    },
    {
      "epoch": 0.11139240506329114,
      "grad_norm": 2.0941052436828613,
      "learning_rate": 1.1139240506329114e-05,
      "loss": 3.5169,
      "step": 88
    },
    {
      "epoch": 0.11265822784810127,
      "grad_norm": 2.035475969314575,
      "learning_rate": 1.1265822784810127e-05,
      "loss": 3.527,
      "step": 89
    },
    {
      "epoch": 0.11392405063291139,
      "grad_norm": 1.9894758462905884,
      "learning_rate": 1.139240506329114e-05,
      "loss": 3.4927,
      "step": 90
    },
    {
      "epoch": 0.11518987341772152,
      "grad_norm": 2.0595321655273438,
      "learning_rate": 1.1518987341772153e-05,
      "loss": 3.4566,
      "step": 91
    },
    {
      "epoch": 0.11645569620253164,
      "grad_norm": 2.034400701522827,
      "learning_rate": 1.1645569620253165e-05,
      "loss": 3.5108,
      "step": 92
    },
    {
      "epoch": 0.11772151898734177,
      "grad_norm": 2.0363857746124268,
      "learning_rate": 1.1772151898734178e-05,
      "loss": 3.4578,
      "step": 93
    },
    {
      "epoch": 0.1189873417721519,
      "grad_norm": 1.9550750255584717,
      "learning_rate": 1.189873417721519e-05,
      "loss": 3.4785,
      "step": 94
    },
    {
      "epoch": 0.12025316455696203,
      "grad_norm": 1.9798699617385864,
      "learning_rate": 1.2025316455696203e-05,
      "loss": 3.4614,
      "step": 95
    },
    {
      "epoch": 0.12151898734177215,
      "grad_norm": 2.0265393257141113,
      "learning_rate": 1.2151898734177216e-05,
      "loss": 3.4678,
      "step": 96
    },
    {
      "epoch": 0.12278481012658228,
      "grad_norm": 1.9515970945358276,
      "learning_rate": 1.2278481012658229e-05,
      "loss": 3.4347,
      "step": 97
    },
    {
      "epoch": 0.1240506329113924,
      "grad_norm": 2.075155735015869,
      "learning_rate": 1.240506329113924e-05,
      "loss": 3.4345,
      "step": 98
    },
    {
      "epoch": 0.12531645569620253,
      "grad_norm": 2.0226902961730957,
      "learning_rate": 1.2531645569620253e-05,
      "loss": 3.473,
      "step": 99
    },
    {
      "epoch": 0.12658227848101267,
      "grad_norm": 1.9861787557601929,
      "learning_rate": 1.2658227848101267e-05,
      "loss": 3.4179,
      "step": 100
    },
    {
      "epoch": 0.12784810126582277,
      "grad_norm": 2.0143277645111084,
      "learning_rate": 1.2784810126582278e-05,
      "loss": 3.409,
      "step": 101
    },
    {
      "epoch": 0.1291139240506329,
      "grad_norm": 2.027277708053589,
      "learning_rate": 1.2911392405063291e-05,
      "loss": 3.3944,
      "step": 102
    },
    {
      "epoch": 0.13037974683544304,
      "grad_norm": 2.021470069885254,
      "learning_rate": 1.3037974683544304e-05,
      "loss": 3.4701,
      "step": 103
    },
    {
      "epoch": 0.13164556962025317,
      "grad_norm": 2.0364890098571777,
      "learning_rate": 1.3164556962025317e-05,
      "loss": 3.4449,
      "step": 104
    },
    {
      "epoch": 0.13291139240506328,
      "grad_norm": 1.9620475769042969,
      "learning_rate": 1.3291139240506329e-05,
      "loss": 3.4087,
      "step": 105
    },
    {
      "epoch": 0.1341772151898734,
      "grad_norm": 2.0575199127197266,
      "learning_rate": 1.3417721518987342e-05,
      "loss": 3.4293,
      "step": 106
    },
    {
      "epoch": 0.13544303797468354,
      "grad_norm": 1.9510688781738281,
      "learning_rate": 1.3544303797468355e-05,
      "loss": 3.4042,
      "step": 107
    },
    {
      "epoch": 0.13670886075949368,
      "grad_norm": 1.9813671112060547,
      "learning_rate": 1.3670886075949368e-05,
      "loss": 3.4131,
      "step": 108
    },
    {
      "epoch": 0.1379746835443038,
      "grad_norm": 2.0144636631011963,
      "learning_rate": 1.3797468354430381e-05,
      "loss": 3.3864,
      "step": 109
    },
    {
      "epoch": 0.13924050632911392,
      "grad_norm": 1.9599744081497192,
      "learning_rate": 1.3924050632911393e-05,
      "loss": 3.3783,
      "step": 110
    },
    {
      "epoch": 0.14050632911392405,
      "grad_norm": 2.0339763164520264,
      "learning_rate": 1.4050632911392406e-05,
      "loss": 3.4355,
      "step": 111
    },
    {
      "epoch": 0.14177215189873418,
      "grad_norm": 2.0646276473999023,
      "learning_rate": 1.417721518987342e-05,
      "loss": 3.4237,
      "step": 112
    },
    {
      "epoch": 0.14303797468354432,
      "grad_norm": 1.9801652431488037,
      "learning_rate": 1.4303797468354432e-05,
      "loss": 3.4057,
      "step": 113
    },
    {
      "epoch": 0.14430379746835442,
      "grad_norm": 1.980863094329834,
      "learning_rate": 1.4430379746835442e-05,
      "loss": 3.3234,
      "step": 114
    },
    {
      "epoch": 0.14556962025316456,
      "grad_norm": 2.0504164695739746,
      "learning_rate": 1.4556962025316457e-05,
      "loss": 3.4031,
      "step": 115
    },
    {
      "epoch": 0.1468354430379747,
      "grad_norm": 2.0145254135131836,
      "learning_rate": 1.468354430379747e-05,
      "loss": 3.3913,
      "step": 116
    },
    {
      "epoch": 0.14810126582278482,
      "grad_norm": 1.9797513484954834,
      "learning_rate": 1.4810126582278483e-05,
      "loss": 3.3475,
      "step": 117
    },
    {
      "epoch": 0.14936708860759493,
      "grad_norm": 1.964335322380066,
      "learning_rate": 1.4936708860759493e-05,
      "loss": 3.3383,
      "step": 118
    },
    {
      "epoch": 0.15063291139240506,
      "grad_norm": 2.0827629566192627,
      "learning_rate": 1.5063291139240506e-05,
      "loss": 3.3625,
      "step": 119
    },
    {
      "epoch": 0.1518987341772152,
      "grad_norm": 2.010366916656494,
      "learning_rate": 1.5189873417721521e-05,
      "loss": 3.3698,
      "step": 120
    },
    {
      "epoch": 0.15316455696202533,
      "grad_norm": 2.022794723510742,
      "learning_rate": 1.5316455696202534e-05,
      "loss": 3.3498,
      "step": 121
    },
    {
      "epoch": 0.15443037974683543,
      "grad_norm": 1.962856411933899,
      "learning_rate": 1.5443037974683546e-05,
      "loss": 3.3439,
      "step": 122
    },
    {
      "epoch": 0.15569620253164557,
      "grad_norm": 1.979225993156433,
      "learning_rate": 1.5569620253164557e-05,
      "loss": 3.3513,
      "step": 123
    },
    {
      "epoch": 0.1569620253164557,
      "grad_norm": 1.981372356414795,
      "learning_rate": 1.5696202531645572e-05,
      "loss": 3.2777,
      "step": 124
    },
    {
      "epoch": 0.15822784810126583,
      "grad_norm": 1.9317454099655151,
      "learning_rate": 1.5822784810126583e-05,
      "loss": 3.2945,
      "step": 125
    },
    {
      "epoch": 0.15949367088607594,
      "grad_norm": 1.9758402109146118,
      "learning_rate": 1.5949367088607595e-05,
      "loss": 3.3155,
      "step": 126
    },
    {
      "epoch": 0.16075949367088607,
      "grad_norm": 2.0230872631073,
      "learning_rate": 1.607594936708861e-05,
      "loss": 3.3073,
      "step": 127
    },
    {
      "epoch": 0.1620253164556962,
      "grad_norm": 1.9706144332885742,
      "learning_rate": 1.620253164556962e-05,
      "loss": 3.337,
      "step": 128
    },
    {
      "epoch": 0.16329113924050634,
      "grad_norm": 1.9481788873672485,
      "learning_rate": 1.6329113924050636e-05,
      "loss": 3.2363,
      "step": 129
    },
    {
      "epoch": 0.16455696202531644,
      "grad_norm": 1.9204624891281128,
      "learning_rate": 1.6455696202531644e-05,
      "loss": 3.3408,
      "step": 130
    },
    {
      "epoch": 0.16582278481012658,
      "grad_norm": 1.9483674764633179,
      "learning_rate": 1.658227848101266e-05,
      "loss": 3.2972,
      "step": 131
    },
    {
      "epoch": 0.1670886075949367,
      "grad_norm": 2.062450408935547,
      "learning_rate": 1.6708860759493674e-05,
      "loss": 3.2494,
      "step": 132
    },
    {
      "epoch": 0.16835443037974684,
      "grad_norm": 1.9519096612930298,
      "learning_rate": 1.6835443037974685e-05,
      "loss": 3.2573,
      "step": 133
    },
    {
      "epoch": 0.16962025316455695,
      "grad_norm": 1.9828916788101196,
      "learning_rate": 1.6962025316455696e-05,
      "loss": 3.3076,
      "step": 134
    },
    {
      "epoch": 0.17088607594936708,
      "grad_norm": 2.0824134349823,
      "learning_rate": 1.7088607594936708e-05,
      "loss": 3.2942,
      "step": 135
    },
    {
      "epoch": 0.17215189873417722,
      "grad_norm": 1.9363347291946411,
      "learning_rate": 1.7215189873417723e-05,
      "loss": 3.3126,
      "step": 136
    },
    {
      "epoch": 0.17341772151898735,
      "grad_norm": 1.9641138315200806,
      "learning_rate": 1.7341772151898734e-05,
      "loss": 3.2628,
      "step": 137
    },
    {
      "epoch": 0.17468354430379746,
      "grad_norm": 1.8952488899230957,
      "learning_rate": 1.7468354430379746e-05,
      "loss": 3.2962,
      "step": 138
    },
    {
      "epoch": 0.1759493670886076,
      "grad_norm": 1.9538259506225586,
      "learning_rate": 1.759493670886076e-05,
      "loss": 3.2774,
      "step": 139
    },
    {
      "epoch": 0.17721518987341772,
      "grad_norm": 1.943772554397583,
      "learning_rate": 1.7721518987341772e-05,
      "loss": 3.2972,
      "step": 140
    },
    {
      "epoch": 0.17848101265822786,
      "grad_norm": 1.9901827573776245,
      "learning_rate": 1.7848101265822787e-05,
      "loss": 3.2566,
      "step": 141
    },
    {
      "epoch": 0.17974683544303796,
      "grad_norm": 1.912224292755127,
      "learning_rate": 1.7974683544303798e-05,
      "loss": 3.2411,
      "step": 142
    },
    {
      "epoch": 0.1810126582278481,
      "grad_norm": 1.9550267457962036,
      "learning_rate": 1.810126582278481e-05,
      "loss": 3.2283,
      "step": 143
    },
    {
      "epoch": 0.18227848101265823,
      "grad_norm": 1.9417258501052856,
      "learning_rate": 1.8227848101265824e-05,
      "loss": 3.2655,
      "step": 144
    },
    {
      "epoch": 0.18354430379746836,
      "grad_norm": 1.9637830257415771,
      "learning_rate": 1.8354430379746836e-05,
      "loss": 3.173,
      "step": 145
    },
    {
      "epoch": 0.1848101265822785,
      "grad_norm": 1.9472965002059937,
      "learning_rate": 1.848101265822785e-05,
      "loss": 3.235,
      "step": 146
    },
    {
      "epoch": 0.1860759493670886,
      "grad_norm": 1.9822044372558594,
      "learning_rate": 1.8607594936708862e-05,
      "loss": 3.2034,
      "step": 147
    },
    {
      "epoch": 0.18734177215189873,
      "grad_norm": 1.8927384614944458,
      "learning_rate": 1.8734177215189874e-05,
      "loss": 3.2157,
      "step": 148
    },
    {
      "epoch": 0.18860759493670887,
      "grad_norm": 1.9211585521697998,
      "learning_rate": 1.886075949367089e-05,
      "loss": 3.2393,
      "step": 149
    },
    {
      "epoch": 0.189873417721519,
      "grad_norm": 1.8984376192092896,
      "learning_rate": 1.89873417721519e-05,
      "loss": 3.133,
      "step": 150
    },
    {
      "epoch": 0.1911392405063291,
      "grad_norm": 1.9185667037963867,
      "learning_rate": 1.911392405063291e-05,
      "loss": 3.1653,
      "step": 151
    },
    {
      "epoch": 0.19240506329113924,
      "grad_norm": 1.9738861322402954,
      "learning_rate": 1.9240506329113926e-05,
      "loss": 3.2216,
      "step": 152
    },
    {
      "epoch": 0.19367088607594937,
      "grad_norm": 1.9184578657150269,
      "learning_rate": 1.9367088607594938e-05,
      "loss": 3.2355,
      "step": 153
    },
    {
      "epoch": 0.1949367088607595,
      "grad_norm": 1.959328055381775,
      "learning_rate": 1.9493670886075952e-05,
      "loss": 3.2167,
      "step": 154
    },
    {
      "epoch": 0.1962025316455696,
      "grad_norm": 1.9420289993286133,
      "learning_rate": 1.962025316455696e-05,
      "loss": 3.2019,
      "step": 155
    },
    {
      "epoch": 0.19746835443037974,
      "grad_norm": 2.0152292251586914,
      "learning_rate": 1.9746835443037975e-05,
      "loss": 3.1962,
      "step": 156
    },
    {
      "epoch": 0.19873417721518988,
      "grad_norm": 1.8820372819900513,
      "learning_rate": 1.987341772151899e-05,
      "loss": 3.1405,
      "step": 157
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.8900800943374634,
      "learning_rate": 2e-05,
      "loss": 3.124,
      "step": 158
    },
    {
      "epoch": 0.20126582278481012,
      "grad_norm": 1.9357028007507324,
      "learning_rate": 2.0126582278481013e-05,
      "loss": 3.1715,
      "step": 159
    },
    {
      "epoch": 0.20253164556962025,
      "grad_norm": 1.9953703880310059,
      "learning_rate": 2.0253164556962025e-05,
      "loss": 3.0656,
      "step": 160
    },
    {
      "epoch": 0.20379746835443038,
      "grad_norm": 1.9637479782104492,
      "learning_rate": 2.037974683544304e-05,
      "loss": 3.1782,
      "step": 161
    },
    {
      "epoch": 0.20506329113924052,
      "grad_norm": 1.9002211093902588,
      "learning_rate": 2.0506329113924054e-05,
      "loss": 3.1172,
      "step": 162
    },
    {
      "epoch": 0.20632911392405062,
      "grad_norm": 1.8824726343154907,
      "learning_rate": 2.0632911392405062e-05,
      "loss": 3.1074,
      "step": 163
    },
    {
      "epoch": 0.20759493670886076,
      "grad_norm": 1.9429399967193604,
      "learning_rate": 2.0759493670886077e-05,
      "loss": 3.1424,
      "step": 164
    },
    {
      "epoch": 0.2088607594936709,
      "grad_norm": 1.8975728750228882,
      "learning_rate": 2.088607594936709e-05,
      "loss": 3.1247,
      "step": 165
    },
    {
      "epoch": 0.21012658227848102,
      "grad_norm": 1.9482040405273438,
      "learning_rate": 2.1012658227848103e-05,
      "loss": 3.1607,
      "step": 166
    },
    {
      "epoch": 0.21139240506329113,
      "grad_norm": 1.9491270780563354,
      "learning_rate": 2.1139240506329115e-05,
      "loss": 3.1545,
      "step": 167
    },
    {
      "epoch": 0.21265822784810126,
      "grad_norm": 1.8876922130584717,
      "learning_rate": 2.1265822784810126e-05,
      "loss": 3.098,
      "step": 168
    },
    {
      "epoch": 0.2139240506329114,
      "grad_norm": 1.930180311203003,
      "learning_rate": 2.139240506329114e-05,
      "loss": 3.1093,
      "step": 169
    },
    {
      "epoch": 0.21518987341772153,
      "grad_norm": 1.945277452468872,
      "learning_rate": 2.1518987341772153e-05,
      "loss": 3.1712,
      "step": 170
    },
    {
      "epoch": 0.21645569620253163,
      "grad_norm": 1.8902666568756104,
      "learning_rate": 2.1645569620253164e-05,
      "loss": 3.1414,
      "step": 171
    },
    {
      "epoch": 0.21772151898734177,
      "grad_norm": 1.9330134391784668,
      "learning_rate": 2.177215189873418e-05,
      "loss": 3.0891,
      "step": 172
    },
    {
      "epoch": 0.2189873417721519,
      "grad_norm": 1.908499836921692,
      "learning_rate": 2.189873417721519e-05,
      "loss": 3.1371,
      "step": 173
    },
    {
      "epoch": 0.22025316455696203,
      "grad_norm": 2.0066025257110596,
      "learning_rate": 2.2025316455696205e-05,
      "loss": 3.0752,
      "step": 174
    },
    {
      "epoch": 0.22151898734177214,
      "grad_norm": 1.960012674331665,
      "learning_rate": 2.2151898734177217e-05,
      "loss": 3.0342,
      "step": 175
    },
    {
      "epoch": 0.22278481012658227,
      "grad_norm": 1.9848732948303223,
      "learning_rate": 2.2278481012658228e-05,
      "loss": 3.0633,
      "step": 176
    },
    {
      "epoch": 0.2240506329113924,
      "grad_norm": 1.9373866319656372,
      "learning_rate": 2.2405063291139243e-05,
      "loss": 3.0571,
      "step": 177
    },
    {
      "epoch": 0.22531645569620254,
      "grad_norm": 1.9222949743270874,
      "learning_rate": 2.2531645569620254e-05,
      "loss": 3.1646,
      "step": 178
    },
    {
      "epoch": 0.22658227848101264,
      "grad_norm": 1.9705818891525269,
      "learning_rate": 2.2658227848101266e-05,
      "loss": 3.0505,
      "step": 179
    },
    {
      "epoch": 0.22784810126582278,
      "grad_norm": 1.8828593492507935,
      "learning_rate": 2.278481012658228e-05,
      "loss": 3.0738,
      "step": 180
    },
    {
      "epoch": 0.2291139240506329,
      "grad_norm": 1.8943310976028442,
      "learning_rate": 2.2911392405063292e-05,
      "loss": 3.0714,
      "step": 181
    },
    {
      "epoch": 0.23037974683544304,
      "grad_norm": 1.9932951927185059,
      "learning_rate": 2.3037974683544307e-05,
      "loss": 3.0292,
      "step": 182
    },
    {
      "epoch": 0.23164556962025318,
      "grad_norm": 1.891943097114563,
      "learning_rate": 2.3164556962025318e-05,
      "loss": 3.0453,
      "step": 183
    },
    {
      "epoch": 0.23291139240506328,
      "grad_norm": 1.8593964576721191,
      "learning_rate": 2.329113924050633e-05,
      "loss": 3.0865,
      "step": 184
    },
    {
      "epoch": 0.23417721518987342,
      "grad_norm": 1.9120107889175415,
      "learning_rate": 2.341772151898734e-05,
      "loss": 3.0553,
      "step": 185
    },
    {
      "epoch": 0.23544303797468355,
      "grad_norm": 1.8796796798706055,
      "learning_rate": 2.3544303797468356e-05,
      "loss": 3.0223,
      "step": 186
    },
    {
      "epoch": 0.23670886075949368,
      "grad_norm": 1.9081604480743408,
      "learning_rate": 2.367088607594937e-05,
      "loss": 3.0187,
      "step": 187
    },
    {
      "epoch": 0.2379746835443038,
      "grad_norm": 1.8893523216247559,
      "learning_rate": 2.379746835443038e-05,
      "loss": 2.9998,
      "step": 188
    },
    {
      "epoch": 0.23924050632911392,
      "grad_norm": 1.8745211362838745,
      "learning_rate": 2.3924050632911394e-05,
      "loss": 3.0443,
      "step": 189
    },
    {
      "epoch": 0.24050632911392406,
      "grad_norm": 1.892987847328186,
      "learning_rate": 2.4050632911392405e-05,
      "loss": 3.018,
      "step": 190
    },
    {
      "epoch": 0.2417721518987342,
      "grad_norm": 1.8554446697235107,
      "learning_rate": 2.417721518987342e-05,
      "loss": 3.0093,
      "step": 191
    },
    {
      "epoch": 0.2430379746835443,
      "grad_norm": 1.9974721670150757,
      "learning_rate": 2.430379746835443e-05,
      "loss": 3.0365,
      "step": 192
    },
    {
      "epoch": 0.24430379746835443,
      "grad_norm": 1.8659191131591797,
      "learning_rate": 2.4430379746835443e-05,
      "loss": 3.0584,
      "step": 193
    },
    {
      "epoch": 0.24556962025316456,
      "grad_norm": 1.8330061435699463,
      "learning_rate": 2.4556962025316458e-05,
      "loss": 2.9697,
      "step": 194
    },
    {
      "epoch": 0.2468354430379747,
      "grad_norm": 1.8671718835830688,
      "learning_rate": 2.468354430379747e-05,
      "loss": 3.0047,
      "step": 195
    },
    {
      "epoch": 0.2481012658227848,
      "grad_norm": 1.9017552137374878,
      "learning_rate": 2.481012658227848e-05,
      "loss": 2.9366,
      "step": 196
    },
    {
      "epoch": 0.24936708860759493,
      "grad_norm": 1.8996286392211914,
      "learning_rate": 2.4936708860759495e-05,
      "loss": 3.0375,
      "step": 197
    },
    {
      "epoch": 0.25063291139240507,
      "grad_norm": 1.9048869609832764,
      "learning_rate": 2.5063291139240507e-05,
      "loss": 2.9941,
      "step": 198
    },
    {
      "epoch": 0.2518987341772152,
      "grad_norm": 1.941124677658081,
      "learning_rate": 2.5189873417721522e-05,
      "loss": 2.999,
      "step": 199
    },
    {
      "epoch": 0.25316455696202533,
      "grad_norm": 1.9246562719345093,
      "learning_rate": 2.5316455696202533e-05,
      "loss": 3.0428,
      "step": 200
    },
    {
      "epoch": 0.25443037974683547,
      "grad_norm": 1.8820627927780151,
      "learning_rate": 2.5443037974683548e-05,
      "loss": 2.9747,
      "step": 201
    },
    {
      "epoch": 0.25569620253164554,
      "grad_norm": 1.865714192390442,
      "learning_rate": 2.5569620253164556e-05,
      "loss": 3.0024,
      "step": 202
    },
    {
      "epoch": 0.2569620253164557,
      "grad_norm": 1.8333649635314941,
      "learning_rate": 2.5696202531645568e-05,
      "loss": 2.9509,
      "step": 203
    },
    {
      "epoch": 0.2582278481012658,
      "grad_norm": 1.8431085348129272,
      "learning_rate": 2.5822784810126582e-05,
      "loss": 2.9205,
      "step": 204
    },
    {
      "epoch": 0.25949367088607594,
      "grad_norm": 1.8178675174713135,
      "learning_rate": 2.5949367088607597e-05,
      "loss": 2.9971,
      "step": 205
    },
    {
      "epoch": 0.2607594936708861,
      "grad_norm": 1.8515448570251465,
      "learning_rate": 2.607594936708861e-05,
      "loss": 2.9852,
      "step": 206
    },
    {
      "epoch": 0.2620253164556962,
      "grad_norm": 1.815292239189148,
      "learning_rate": 2.6202531645569623e-05,
      "loss": 2.9746,
      "step": 207
    },
    {
      "epoch": 0.26329113924050634,
      "grad_norm": 1.8182101249694824,
      "learning_rate": 2.6329113924050635e-05,
      "loss": 2.9378,
      "step": 208
    },
    {
      "epoch": 0.2645569620253165,
      "grad_norm": 1.8422404527664185,
      "learning_rate": 2.645569620253165e-05,
      "loss": 2.926,
      "step": 209
    },
    {
      "epoch": 0.26582278481012656,
      "grad_norm": 1.8346894979476929,
      "learning_rate": 2.6582278481012658e-05,
      "loss": 2.9754,
      "step": 210
    },
    {
      "epoch": 0.2670886075949367,
      "grad_norm": 1.9264030456542969,
      "learning_rate": 2.670886075949367e-05,
      "loss": 2.9631,
      "step": 211
    },
    {
      "epoch": 0.2683544303797468,
      "grad_norm": 1.8342199325561523,
      "learning_rate": 2.6835443037974684e-05,
      "loss": 2.9325,
      "step": 212
    },
    {
      "epoch": 0.26962025316455696,
      "grad_norm": 1.874375343322754,
      "learning_rate": 2.6962025316455696e-05,
      "loss": 2.9353,
      "step": 213
    },
    {
      "epoch": 0.2708860759493671,
      "grad_norm": 1.8262265920639038,
      "learning_rate": 2.708860759493671e-05,
      "loss": 2.9222,
      "step": 214
    },
    {
      "epoch": 0.2721518987341772,
      "grad_norm": 1.8504060506820679,
      "learning_rate": 2.7215189873417722e-05,
      "loss": 2.9248,
      "step": 215
    },
    {
      "epoch": 0.27341772151898736,
      "grad_norm": 1.8506391048431396,
      "learning_rate": 2.7341772151898737e-05,
      "loss": 2.9539,
      "step": 216
    },
    {
      "epoch": 0.2746835443037975,
      "grad_norm": 1.849169373512268,
      "learning_rate": 2.746835443037975e-05,
      "loss": 2.8779,
      "step": 217
    },
    {
      "epoch": 0.2759493670886076,
      "grad_norm": 1.817820429801941,
      "learning_rate": 2.7594936708860763e-05,
      "loss": 2.8969,
      "step": 218
    },
    {
      "epoch": 0.2772151898734177,
      "grad_norm": 1.842079520225525,
      "learning_rate": 2.772151898734177e-05,
      "loss": 2.8834,
      "step": 219
    },
    {
      "epoch": 0.27848101265822783,
      "grad_norm": 1.835823893547058,
      "learning_rate": 2.7848101265822786e-05,
      "loss": 2.8723,
      "step": 220
    },
    {
      "epoch": 0.27974683544303797,
      "grad_norm": 1.820674180984497,
      "learning_rate": 2.7974683544303797e-05,
      "loss": 2.8544,
      "step": 221
    },
    {
      "epoch": 0.2810126582278481,
      "grad_norm": 1.8062714338302612,
      "learning_rate": 2.8101265822784812e-05,
      "loss": 2.808,
      "step": 222
    },
    {
      "epoch": 0.28227848101265823,
      "grad_norm": 1.7654718160629272,
      "learning_rate": 2.8227848101265824e-05,
      "loss": 2.859,
      "step": 223
    },
    {
      "epoch": 0.28354430379746837,
      "grad_norm": 1.873944640159607,
      "learning_rate": 2.835443037974684e-05,
      "loss": 2.8949,
      "step": 224
    },
    {
      "epoch": 0.2848101265822785,
      "grad_norm": 1.8919163942337036,
      "learning_rate": 2.848101265822785e-05,
      "loss": 2.8614,
      "step": 225
    },
    {
      "epoch": 0.28607594936708863,
      "grad_norm": 1.830263614654541,
      "learning_rate": 2.8607594936708865e-05,
      "loss": 2.8422,
      "step": 226
    },
    {
      "epoch": 0.2873417721518987,
      "grad_norm": 1.8551592826843262,
      "learning_rate": 2.8734177215189873e-05,
      "loss": 2.9642,
      "step": 227
    },
    {
      "epoch": 0.28860759493670884,
      "grad_norm": 1.85234797000885,
      "learning_rate": 2.8860759493670884e-05,
      "loss": 2.9035,
      "step": 228
    },
    {
      "epoch": 0.289873417721519,
      "grad_norm": 1.863875150680542,
      "learning_rate": 2.89873417721519e-05,
      "loss": 2.912,
      "step": 229
    },
    {
      "epoch": 0.2911392405063291,
      "grad_norm": 1.8883018493652344,
      "learning_rate": 2.9113924050632914e-05,
      "loss": 2.8915,
      "step": 230
    },
    {
      "epoch": 0.29240506329113924,
      "grad_norm": 1.7972993850708008,
      "learning_rate": 2.9240506329113925e-05,
      "loss": 2.8475,
      "step": 231
    },
    {
      "epoch": 0.2936708860759494,
      "grad_norm": 1.7916254997253418,
      "learning_rate": 2.936708860759494e-05,
      "loss": 2.8915,
      "step": 232
    },
    {
      "epoch": 0.2949367088607595,
      "grad_norm": 1.7550677061080933,
      "learning_rate": 2.949367088607595e-05,
      "loss": 2.7496,
      "step": 233
    },
    {
      "epoch": 0.29620253164556964,
      "grad_norm": 1.8440691232681274,
      "learning_rate": 2.9620253164556966e-05,
      "loss": 2.9161,
      "step": 234
    },
    {
      "epoch": 0.2974683544303797,
      "grad_norm": 1.8778178691864014,
      "learning_rate": 2.9746835443037974e-05,
      "loss": 2.8462,
      "step": 235
    },
    {
      "epoch": 0.29873417721518986,
      "grad_norm": 1.9463682174682617,
      "learning_rate": 2.9873417721518986e-05,
      "loss": 2.8873,
      "step": 236
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.8255867958068848,
      "learning_rate": 3e-05,
      "loss": 2.8421,
      "step": 237
    },
    {
      "epoch": 0.3012658227848101,
      "grad_norm": 1.7715044021606445,
      "learning_rate": 3.0126582278481012e-05,
      "loss": 2.807,
      "step": 238
    },
    {
      "epoch": 0.30253164556962026,
      "grad_norm": 1.7638965845108032,
      "learning_rate": 3.0253164556962027e-05,
      "loss": 2.8409,
      "step": 239
    },
    {
      "epoch": 0.3037974683544304,
      "grad_norm": 1.8107682466506958,
      "learning_rate": 3.0379746835443042e-05,
      "loss": 2.8998,
      "step": 240
    },
    {
      "epoch": 0.3050632911392405,
      "grad_norm": 1.8403154611587524,
      "learning_rate": 3.0506329113924053e-05,
      "loss": 2.774,
      "step": 241
    },
    {
      "epoch": 0.30632911392405066,
      "grad_norm": 1.8486309051513672,
      "learning_rate": 3.063291139240507e-05,
      "loss": 2.7947,
      "step": 242
    },
    {
      "epoch": 0.30759493670886073,
      "grad_norm": 1.8002145290374756,
      "learning_rate": 3.075949367088607e-05,
      "loss": 2.8184,
      "step": 243
    },
    {
      "epoch": 0.30886075949367087,
      "grad_norm": 1.7359408140182495,
      "learning_rate": 3.088607594936709e-05,
      "loss": 2.7948,
      "step": 244
    },
    {
      "epoch": 0.310126582278481,
      "grad_norm": 1.8135831356048584,
      "learning_rate": 3.10126582278481e-05,
      "loss": 2.8645,
      "step": 245
    },
    {
      "epoch": 0.31139240506329113,
      "grad_norm": 1.7950599193572998,
      "learning_rate": 3.1139240506329114e-05,
      "loss": 2.8227,
      "step": 246
    },
    {
      "epoch": 0.31265822784810127,
      "grad_norm": 1.7998783588409424,
      "learning_rate": 3.1265822784810125e-05,
      "loss": 2.7706,
      "step": 247
    },
    {
      "epoch": 0.3139240506329114,
      "grad_norm": 1.7922039031982422,
      "learning_rate": 3.1392405063291144e-05,
      "loss": 2.8154,
      "step": 248
    },
    {
      "epoch": 0.31518987341772153,
      "grad_norm": 1.8349545001983643,
      "learning_rate": 3.1518987341772155e-05,
      "loss": 2.7487,
      "step": 249
    },
    {
      "epoch": 0.31645569620253167,
      "grad_norm": 1.7284812927246094,
      "learning_rate": 3.1645569620253167e-05,
      "loss": 2.7426,
      "step": 250
    },
    {
      "epoch": 0.31772151898734174,
      "grad_norm": 1.8163084983825684,
      "learning_rate": 3.177215189873418e-05,
      "loss": 2.8135,
      "step": 251
    },
    {
      "epoch": 0.3189873417721519,
      "grad_norm": 1.8033692836761475,
      "learning_rate": 3.189873417721519e-05,
      "loss": 2.8069,
      "step": 252
    },
    {
      "epoch": 0.320253164556962,
      "grad_norm": 1.747923493385315,
      "learning_rate": 3.20253164556962e-05,
      "loss": 2.7556,
      "step": 253
    },
    {
      "epoch": 0.32151898734177214,
      "grad_norm": 1.8166438341140747,
      "learning_rate": 3.215189873417722e-05,
      "loss": 2.7952,
      "step": 254
    },
    {
      "epoch": 0.3227848101265823,
      "grad_norm": 1.7983793020248413,
      "learning_rate": 3.227848101265823e-05,
      "loss": 2.7947,
      "step": 255
    },
    {
      "epoch": 0.3240506329113924,
      "grad_norm": 1.7653816938400269,
      "learning_rate": 3.240506329113924e-05,
      "loss": 2.6825,
      "step": 256
    },
    {
      "epoch": 0.32531645569620254,
      "grad_norm": 1.7683932781219482,
      "learning_rate": 3.2531645569620253e-05,
      "loss": 2.7485,
      "step": 257
    },
    {
      "epoch": 0.3265822784810127,
      "grad_norm": 1.7671869993209839,
      "learning_rate": 3.265822784810127e-05,
      "loss": 2.802,
      "step": 258
    },
    {
      "epoch": 0.3278481012658228,
      "grad_norm": 1.7464706897735596,
      "learning_rate": 3.278481012658228e-05,
      "loss": 2.8119,
      "step": 259
    },
    {
      "epoch": 0.3291139240506329,
      "grad_norm": 1.8228946924209595,
      "learning_rate": 3.291139240506329e-05,
      "loss": 2.7733,
      "step": 260
    },
    {
      "epoch": 0.330379746835443,
      "grad_norm": 1.807992696762085,
      "learning_rate": 3.3037974683544306e-05,
      "loss": 2.7638,
      "step": 261
    },
    {
      "epoch": 0.33164556962025316,
      "grad_norm": 1.764796257019043,
      "learning_rate": 3.316455696202532e-05,
      "loss": 2.6759,
      "step": 262
    },
    {
      "epoch": 0.3329113924050633,
      "grad_norm": 1.7588664293289185,
      "learning_rate": 3.329113924050633e-05,
      "loss": 2.6994,
      "step": 263
    },
    {
      "epoch": 0.3341772151898734,
      "grad_norm": 1.7712161540985107,
      "learning_rate": 3.341772151898735e-05,
      "loss": 2.7455,
      "step": 264
    },
    {
      "epoch": 0.33544303797468356,
      "grad_norm": 1.7481368780136108,
      "learning_rate": 3.354430379746836e-05,
      "loss": 2.6985,
      "step": 265
    },
    {
      "epoch": 0.3367088607594937,
      "grad_norm": 1.8022876977920532,
      "learning_rate": 3.367088607594937e-05,
      "loss": 2.7841,
      "step": 266
    },
    {
      "epoch": 0.3379746835443038,
      "grad_norm": 1.7587916851043701,
      "learning_rate": 3.379746835443038e-05,
      "loss": 2.7388,
      "step": 267
    },
    {
      "epoch": 0.3392405063291139,
      "grad_norm": 1.9796056747436523,
      "learning_rate": 3.392405063291139e-05,
      "loss": 2.7877,
      "step": 268
    },
    {
      "epoch": 0.34050632911392403,
      "grad_norm": 1.7557168006896973,
      "learning_rate": 3.4050632911392404e-05,
      "loss": 2.7244,
      "step": 269
    },
    {
      "epoch": 0.34177215189873417,
      "grad_norm": 1.7772635221481323,
      "learning_rate": 3.4177215189873416e-05,
      "loss": 2.7507,
      "step": 270
    },
    {
      "epoch": 0.3430379746835443,
      "grad_norm": 1.7541545629501343,
      "learning_rate": 3.4303797468354434e-05,
      "loss": 2.7808,
      "step": 271
    },
    {
      "epoch": 0.34430379746835443,
      "grad_norm": 1.8666162490844727,
      "learning_rate": 3.4430379746835445e-05,
      "loss": 2.7956,
      "step": 272
    },
    {
      "epoch": 0.34556962025316457,
      "grad_norm": 1.7392559051513672,
      "learning_rate": 3.455696202531646e-05,
      "loss": 2.7116,
      "step": 273
    },
    {
      "epoch": 0.3468354430379747,
      "grad_norm": 1.8494646549224854,
      "learning_rate": 3.468354430379747e-05,
      "loss": 2.7528,
      "step": 274
    },
    {
      "epoch": 0.34810126582278483,
      "grad_norm": 1.7677332162857056,
      "learning_rate": 3.4810126582278487e-05,
      "loss": 2.6991,
      "step": 275
    },
    {
      "epoch": 0.3493670886075949,
      "grad_norm": 1.7507574558258057,
      "learning_rate": 3.493670886075949e-05,
      "loss": 2.6288,
      "step": 276
    },
    {
      "epoch": 0.35063291139240504,
      "grad_norm": 1.7171320915222168,
      "learning_rate": 3.506329113924051e-05,
      "loss": 2.7221,
      "step": 277
    },
    {
      "epoch": 0.3518987341772152,
      "grad_norm": 1.7352392673492432,
      "learning_rate": 3.518987341772152e-05,
      "loss": 2.623,
      "step": 278
    },
    {
      "epoch": 0.3531645569620253,
      "grad_norm": 1.8247565031051636,
      "learning_rate": 3.531645569620253e-05,
      "loss": 2.7126,
      "step": 279
    },
    {
      "epoch": 0.35443037974683544,
      "grad_norm": 1.755336880683899,
      "learning_rate": 3.5443037974683544e-05,
      "loss": 2.6375,
      "step": 280
    },
    {
      "epoch": 0.3556962025316456,
      "grad_norm": 1.779907464981079,
      "learning_rate": 3.556962025316456e-05,
      "loss": 2.7174,
      "step": 281
    },
    {
      "epoch": 0.3569620253164557,
      "grad_norm": 1.7641932964324951,
      "learning_rate": 3.5696202531645573e-05,
      "loss": 2.6075,
      "step": 282
    },
    {
      "epoch": 0.35822784810126584,
      "grad_norm": 1.7892333269119263,
      "learning_rate": 3.5822784810126585e-05,
      "loss": 2.7438,
      "step": 283
    },
    {
      "epoch": 0.3594936708860759,
      "grad_norm": 1.7165659666061401,
      "learning_rate": 3.5949367088607596e-05,
      "loss": 2.6948,
      "step": 284
    },
    {
      "epoch": 0.36075949367088606,
      "grad_norm": 1.6892709732055664,
      "learning_rate": 3.607594936708861e-05,
      "loss": 2.6666,
      "step": 285
    },
    {
      "epoch": 0.3620253164556962,
      "grad_norm": 1.7550181150436401,
      "learning_rate": 3.620253164556962e-05,
      "loss": 2.7238,
      "step": 286
    },
    {
      "epoch": 0.3632911392405063,
      "grad_norm": 1.7466920614242554,
      "learning_rate": 3.632911392405063e-05,
      "loss": 2.6999,
      "step": 287
    },
    {
      "epoch": 0.36455696202531646,
      "grad_norm": 1.7516416311264038,
      "learning_rate": 3.645569620253165e-05,
      "loss": 2.717,
      "step": 288
    },
    {
      "epoch": 0.3658227848101266,
      "grad_norm": 1.7338063716888428,
      "learning_rate": 3.658227848101266e-05,
      "loss": 2.6744,
      "step": 289
    },
    {
      "epoch": 0.3670886075949367,
      "grad_norm": 1.7433398962020874,
      "learning_rate": 3.670886075949367e-05,
      "loss": 2.7166,
      "step": 290
    },
    {
      "epoch": 0.36835443037974686,
      "grad_norm": 1.7240681648254395,
      "learning_rate": 3.683544303797469e-05,
      "loss": 2.607,
      "step": 291
    },
    {
      "epoch": 0.369620253164557,
      "grad_norm": 1.6973366737365723,
      "learning_rate": 3.69620253164557e-05,
      "loss": 2.6603,
      "step": 292
    },
    {
      "epoch": 0.37088607594936707,
      "grad_norm": 1.7582801580429077,
      "learning_rate": 3.7088607594936706e-05,
      "loss": 2.687,
      "step": 293
    },
    {
      "epoch": 0.3721518987341772,
      "grad_norm": 1.7077769041061401,
      "learning_rate": 3.7215189873417724e-05,
      "loss": 2.579,
      "step": 294
    },
    {
      "epoch": 0.37341772151898733,
      "grad_norm": 1.7341868877410889,
      "learning_rate": 3.7341772151898736e-05,
      "loss": 2.7007,
      "step": 295
    },
    {
      "epoch": 0.37468354430379747,
      "grad_norm": 1.725717306137085,
      "learning_rate": 3.746835443037975e-05,
      "loss": 2.6349,
      "step": 296
    },
    {
      "epoch": 0.3759493670886076,
      "grad_norm": 1.7667665481567383,
      "learning_rate": 3.759493670886076e-05,
      "loss": 2.6859,
      "step": 297
    },
    {
      "epoch": 0.37721518987341773,
      "grad_norm": 1.7555445432662964,
      "learning_rate": 3.772151898734178e-05,
      "loss": 2.6726,
      "step": 298
    },
    {
      "epoch": 0.37848101265822787,
      "grad_norm": 1.762328863143921,
      "learning_rate": 3.784810126582279e-05,
      "loss": 2.6205,
      "step": 299
    },
    {
      "epoch": 0.379746835443038,
      "grad_norm": 1.7133220434188843,
      "learning_rate": 3.79746835443038e-05,
      "loss": 2.66,
      "step": 300
    },
    {
      "epoch": 0.3810126582278481,
      "grad_norm": 1.7161747217178345,
      "learning_rate": 3.810126582278481e-05,
      "loss": 2.5959,
      "step": 301
    },
    {
      "epoch": 0.3822784810126582,
      "grad_norm": 1.7340075969696045,
      "learning_rate": 3.822784810126582e-05,
      "loss": 2.6558,
      "step": 302
    },
    {
      "epoch": 0.38354430379746834,
      "grad_norm": 1.7149851322174072,
      "learning_rate": 3.8354430379746834e-05,
      "loss": 2.6186,
      "step": 303
    },
    {
      "epoch": 0.3848101265822785,
      "grad_norm": 1.7119362354278564,
      "learning_rate": 3.848101265822785e-05,
      "loss": 2.669,
      "step": 304
    },
    {
      "epoch": 0.3860759493670886,
      "grad_norm": 1.6932138204574585,
      "learning_rate": 3.8607594936708864e-05,
      "loss": 2.6158,
      "step": 305
    },
    {
      "epoch": 0.38734177215189874,
      "grad_norm": 1.6914256811141968,
      "learning_rate": 3.8734177215189875e-05,
      "loss": 2.6293,
      "step": 306
    },
    {
      "epoch": 0.3886075949367089,
      "grad_norm": 1.72072172164917,
      "learning_rate": 3.886075949367089e-05,
      "loss": 2.6644,
      "step": 307
    },
    {
      "epoch": 0.389873417721519,
      "grad_norm": 1.7309409379959106,
      "learning_rate": 3.8987341772151905e-05,
      "loss": 2.5978,
      "step": 308
    },
    {
      "epoch": 0.3911392405063291,
      "grad_norm": 1.680428147315979,
      "learning_rate": 3.911392405063291e-05,
      "loss": 2.5409,
      "step": 309
    },
    {
      "epoch": 0.3924050632911392,
      "grad_norm": 1.7626373767852783,
      "learning_rate": 3.924050632911392e-05,
      "loss": 2.599,
      "step": 310
    },
    {
      "epoch": 0.39367088607594936,
      "grad_norm": 1.685671329498291,
      "learning_rate": 3.936708860759494e-05,
      "loss": 2.6103,
      "step": 311
    },
    {
      "epoch": 0.3949367088607595,
      "grad_norm": 1.7858775854110718,
      "learning_rate": 3.949367088607595e-05,
      "loss": 2.7079,
      "step": 312
    },
    {
      "epoch": 0.3962025316455696,
      "grad_norm": 1.7421613931655884,
      "learning_rate": 3.962025316455696e-05,
      "loss": 2.5856,
      "step": 313
    },
    {
      "epoch": 0.39746835443037976,
      "grad_norm": 1.6893532276153564,
      "learning_rate": 3.974683544303798e-05,
      "loss": 2.6086,
      "step": 314
    },
    {
      "epoch": 0.3987341772151899,
      "grad_norm": 1.7353863716125488,
      "learning_rate": 3.987341772151899e-05,
      "loss": 2.6132,
      "step": 315
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.6725229024887085,
      "learning_rate": 4e-05,
      "loss": 2.5538,
      "step": 316
    },
    {
      "epoch": 0.4012658227848101,
      "grad_norm": 1.7781482934951782,
      "learning_rate": 4.0126582278481015e-05,
      "loss": 2.5244,
      "step": 317
    },
    {
      "epoch": 0.40253164556962023,
      "grad_norm": 1.7406302690505981,
      "learning_rate": 4.0253164556962026e-05,
      "loss": 2.6347,
      "step": 318
    },
    {
      "epoch": 0.40379746835443037,
      "grad_norm": 1.7193034887313843,
      "learning_rate": 4.037974683544304e-05,
      "loss": 2.646,
      "step": 319
    },
    {
      "epoch": 0.4050632911392405,
      "grad_norm": 1.6732292175292969,
      "learning_rate": 4.050632911392405e-05,
      "loss": 2.5056,
      "step": 320
    },
    {
      "epoch": 0.40632911392405063,
      "grad_norm": 1.7147446870803833,
      "learning_rate": 4.063291139240507e-05,
      "loss": 2.5278,
      "step": 321
    },
    {
      "epoch": 0.40759493670886077,
      "grad_norm": 1.7004932165145874,
      "learning_rate": 4.075949367088608e-05,
      "loss": 2.6377,
      "step": 322
    },
    {
      "epoch": 0.4088607594936709,
      "grad_norm": 1.719261646270752,
      "learning_rate": 4.088607594936709e-05,
      "loss": 2.5882,
      "step": 323
    },
    {
      "epoch": 0.41012658227848103,
      "grad_norm": 1.7005194425582886,
      "learning_rate": 4.101265822784811e-05,
      "loss": 2.6466,
      "step": 324
    },
    {
      "epoch": 0.41139240506329117,
      "grad_norm": 1.682564616203308,
      "learning_rate": 4.113924050632912e-05,
      "loss": 2.5307,
      "step": 325
    },
    {
      "epoch": 0.41265822784810124,
      "grad_norm": 1.672641634941101,
      "learning_rate": 4.1265822784810125e-05,
      "loss": 2.5629,
      "step": 326
    },
    {
      "epoch": 0.4139240506329114,
      "grad_norm": 1.6520404815673828,
      "learning_rate": 4.139240506329114e-05,
      "loss": 2.5594,
      "step": 327
    },
    {
      "epoch": 0.4151898734177215,
      "grad_norm": 1.651808738708496,
      "learning_rate": 4.1518987341772154e-05,
      "loss": 2.5991,
      "step": 328
    },
    {
      "epoch": 0.41645569620253164,
      "grad_norm": 1.6458308696746826,
      "learning_rate": 4.1645569620253166e-05,
      "loss": 2.5908,
      "step": 329
    },
    {
      "epoch": 0.4177215189873418,
      "grad_norm": 1.6815789937973022,
      "learning_rate": 4.177215189873418e-05,
      "loss": 2.4963,
      "step": 330
    },
    {
      "epoch": 0.4189873417721519,
      "grad_norm": 1.6555185317993164,
      "learning_rate": 4.1898734177215195e-05,
      "loss": 2.5622,
      "step": 331
    },
    {
      "epoch": 0.42025316455696204,
      "grad_norm": 1.6962625980377197,
      "learning_rate": 4.202531645569621e-05,
      "loss": 2.5489,
      "step": 332
    },
    {
      "epoch": 0.4215189873417722,
      "grad_norm": 1.6664836406707764,
      "learning_rate": 4.215189873417722e-05,
      "loss": 2.4715,
      "step": 333
    },
    {
      "epoch": 0.42278481012658226,
      "grad_norm": 1.6482815742492676,
      "learning_rate": 4.227848101265823e-05,
      "loss": 2.5774,
      "step": 334
    },
    {
      "epoch": 0.4240506329113924,
      "grad_norm": 1.6981120109558105,
      "learning_rate": 4.240506329113924e-05,
      "loss": 2.5686,
      "step": 335
    },
    {
      "epoch": 0.4253164556962025,
      "grad_norm": 1.7297499179840088,
      "learning_rate": 4.253164556962025e-05,
      "loss": 2.5989,
      "step": 336
    },
    {
      "epoch": 0.42658227848101266,
      "grad_norm": 1.7747303247451782,
      "learning_rate": 4.265822784810127e-05,
      "loss": 2.5246,
      "step": 337
    },
    {
      "epoch": 0.4278481012658228,
      "grad_norm": 1.634889841079712,
      "learning_rate": 4.278481012658228e-05,
      "loss": 2.4431,
      "step": 338
    },
    {
      "epoch": 0.4291139240506329,
      "grad_norm": 1.665174126625061,
      "learning_rate": 4.2911392405063294e-05,
      "loss": 2.4824,
      "step": 339
    },
    {
      "epoch": 0.43037974683544306,
      "grad_norm": 1.628347635269165,
      "learning_rate": 4.3037974683544305e-05,
      "loss": 2.5802,
      "step": 340
    },
    {
      "epoch": 0.4316455696202532,
      "grad_norm": 1.6775611639022827,
      "learning_rate": 4.316455696202532e-05,
      "loss": 2.5513,
      "step": 341
    },
    {
      "epoch": 0.43291139240506327,
      "grad_norm": 1.6558016538619995,
      "learning_rate": 4.329113924050633e-05,
      "loss": 2.582,
      "step": 342
    },
    {
      "epoch": 0.4341772151898734,
      "grad_norm": 1.7188048362731934,
      "learning_rate": 4.341772151898734e-05,
      "loss": 2.5518,
      "step": 343
    },
    {
      "epoch": 0.43544303797468353,
      "grad_norm": 1.7232714891433716,
      "learning_rate": 4.354430379746836e-05,
      "loss": 2.5142,
      "step": 344
    },
    {
      "epoch": 0.43670886075949367,
      "grad_norm": 1.6774052381515503,
      "learning_rate": 4.367088607594937e-05,
      "loss": 2.544,
      "step": 345
    },
    {
      "epoch": 0.4379746835443038,
      "grad_norm": 1.651743769645691,
      "learning_rate": 4.379746835443038e-05,
      "loss": 2.519,
      "step": 346
    },
    {
      "epoch": 0.43924050632911393,
      "grad_norm": 1.7071917057037354,
      "learning_rate": 4.39240506329114e-05,
      "loss": 2.4442,
      "step": 347
    },
    {
      "epoch": 0.44050632911392407,
      "grad_norm": 1.639562726020813,
      "learning_rate": 4.405063291139241e-05,
      "loss": 2.4618,
      "step": 348
    },
    {
      "epoch": 0.4417721518987342,
      "grad_norm": 1.6199244260787964,
      "learning_rate": 4.417721518987342e-05,
      "loss": 2.4648,
      "step": 349
    },
    {
      "epoch": 0.4430379746835443,
      "grad_norm": 1.6113970279693604,
      "learning_rate": 4.430379746835443e-05,
      "loss": 2.5181,
      "step": 350
    },
    {
      "epoch": 0.4443037974683544,
      "grad_norm": 1.6601017713546753,
      "learning_rate": 4.4430379746835445e-05,
      "loss": 2.5302,
      "step": 351
    },
    {
      "epoch": 0.44556962025316454,
      "grad_norm": 1.7177473306655884,
      "learning_rate": 4.4556962025316456e-05,
      "loss": 2.5473,
      "step": 352
    },
    {
      "epoch": 0.4468354430379747,
      "grad_norm": 1.6227599382400513,
      "learning_rate": 4.468354430379747e-05,
      "loss": 2.5234,
      "step": 353
    },
    {
      "epoch": 0.4481012658227848,
      "grad_norm": 1.6576178073883057,
      "learning_rate": 4.4810126582278486e-05,
      "loss": 2.4918,
      "step": 354
    },
    {
      "epoch": 0.44936708860759494,
      "grad_norm": 1.6153966188430786,
      "learning_rate": 4.49367088607595e-05,
      "loss": 2.4584,
      "step": 355
    },
    {
      "epoch": 0.4506329113924051,
      "grad_norm": 1.6243510246276855,
      "learning_rate": 4.506329113924051e-05,
      "loss": 2.5144,
      "step": 356
    },
    {
      "epoch": 0.4518987341772152,
      "grad_norm": 1.6378865242004395,
      "learning_rate": 4.518987341772152e-05,
      "loss": 2.5087,
      "step": 357
    },
    {
      "epoch": 0.4531645569620253,
      "grad_norm": 1.6301227807998657,
      "learning_rate": 4.531645569620253e-05,
      "loss": 2.5241,
      "step": 358
    },
    {
      "epoch": 0.4544303797468354,
      "grad_norm": 1.657922625541687,
      "learning_rate": 4.544303797468354e-05,
      "loss": 2.5114,
      "step": 359
    },
    {
      "epoch": 0.45569620253164556,
      "grad_norm": 1.6529971361160278,
      "learning_rate": 4.556962025316456e-05,
      "loss": 2.4866,
      "step": 360
    },
    {
      "epoch": 0.4569620253164557,
      "grad_norm": 1.6724616289138794,
      "learning_rate": 4.569620253164557e-05,
      "loss": 2.54,
      "step": 361
    },
    {
      "epoch": 0.4582278481012658,
      "grad_norm": 1.6336225271224976,
      "learning_rate": 4.5822784810126584e-05,
      "loss": 2.4407,
      "step": 362
    },
    {
      "epoch": 0.45949367088607596,
      "grad_norm": 1.6746271848678589,
      "learning_rate": 4.5949367088607595e-05,
      "loss": 2.5191,
      "step": 363
    },
    {
      "epoch": 0.4607594936708861,
      "grad_norm": 1.6259639263153076,
      "learning_rate": 4.6075949367088614e-05,
      "loss": 2.448,
      "step": 364
    },
    {
      "epoch": 0.4620253164556962,
      "grad_norm": 1.635961651802063,
      "learning_rate": 4.6202531645569625e-05,
      "loss": 2.4871,
      "step": 365
    },
    {
      "epoch": 0.46329113924050636,
      "grad_norm": 1.6030538082122803,
      "learning_rate": 4.6329113924050637e-05,
      "loss": 2.4791,
      "step": 366
    },
    {
      "epoch": 0.46455696202531643,
      "grad_norm": 1.64846670627594,
      "learning_rate": 4.645569620253165e-05,
      "loss": 2.5129,
      "step": 367
    },
    {
      "epoch": 0.46582278481012657,
      "grad_norm": 1.6027053594589233,
      "learning_rate": 4.658227848101266e-05,
      "loss": 2.4558,
      "step": 368
    },
    {
      "epoch": 0.4670886075949367,
      "grad_norm": 1.6744080781936646,
      "learning_rate": 4.670886075949367e-05,
      "loss": 2.514,
      "step": 369
    },
    {
      "epoch": 0.46835443037974683,
      "grad_norm": 1.6018460988998413,
      "learning_rate": 4.683544303797468e-05,
      "loss": 2.5243,
      "step": 370
    },
    {
      "epoch": 0.46962025316455697,
      "grad_norm": 1.5994285345077515,
      "learning_rate": 4.69620253164557e-05,
      "loss": 2.5254,
      "step": 371
    },
    {
      "epoch": 0.4708860759493671,
      "grad_norm": 1.6617594957351685,
      "learning_rate": 4.708860759493671e-05,
      "loss": 2.4582,
      "step": 372
    },
    {
      "epoch": 0.47215189873417723,
      "grad_norm": 1.584665298461914,
      "learning_rate": 4.7215189873417723e-05,
      "loss": 2.4182,
      "step": 373
    },
    {
      "epoch": 0.47341772151898737,
      "grad_norm": 1.582659125328064,
      "learning_rate": 4.734177215189874e-05,
      "loss": 2.4344,
      "step": 374
    },
    {
      "epoch": 0.47468354430379744,
      "grad_norm": 1.583743929862976,
      "learning_rate": 4.7468354430379746e-05,
      "loss": 2.4364,
      "step": 375
    },
    {
      "epoch": 0.4759493670886076,
      "grad_norm": 1.621604561805725,
      "learning_rate": 4.759493670886076e-05,
      "loss": 2.4422,
      "step": 376
    },
    {
      "epoch": 0.4772151898734177,
      "grad_norm": 1.6525497436523438,
      "learning_rate": 4.7721518987341776e-05,
      "loss": 2.4702,
      "step": 377
    },
    {
      "epoch": 0.47848101265822784,
      "grad_norm": 1.6119078397750854,
      "learning_rate": 4.784810126582279e-05,
      "loss": 2.4802,
      "step": 378
    },
    {
      "epoch": 0.479746835443038,
      "grad_norm": 1.6304550170898438,
      "learning_rate": 4.79746835443038e-05,
      "loss": 2.4399,
      "step": 379
    },
    {
      "epoch": 0.4810126582278481,
      "grad_norm": 1.5994795560836792,
      "learning_rate": 4.810126582278481e-05,
      "loss": 2.4548,
      "step": 380
    },
    {
      "epoch": 0.48227848101265824,
      "grad_norm": 1.640616774559021,
      "learning_rate": 4.822784810126583e-05,
      "loss": 2.5198,
      "step": 381
    },
    {
      "epoch": 0.4835443037974684,
      "grad_norm": 1.5751734972000122,
      "learning_rate": 4.835443037974684e-05,
      "loss": 2.437,
      "step": 382
    },
    {
      "epoch": 0.48481012658227846,
      "grad_norm": 1.649900197982788,
      "learning_rate": 4.8481012658227845e-05,
      "loss": 2.4665,
      "step": 383
    },
    {
      "epoch": 0.4860759493670886,
      "grad_norm": 1.5988941192626953,
      "learning_rate": 4.860759493670886e-05,
      "loss": 2.4541,
      "step": 384
    },
    {
      "epoch": 0.4873417721518987,
      "grad_norm": 1.5934171676635742,
      "learning_rate": 4.8734177215189874e-05,
      "loss": 2.4796,
      "step": 385
    },
    {
      "epoch": 0.48860759493670886,
      "grad_norm": 1.573717713356018,
      "learning_rate": 4.8860759493670886e-05,
      "loss": 2.4171,
      "step": 386
    },
    {
      "epoch": 0.489873417721519,
      "grad_norm": 1.5516082048416138,
      "learning_rate": 4.8987341772151904e-05,
      "loss": 2.41,
      "step": 387
    },
    {
      "epoch": 0.4911392405063291,
      "grad_norm": 1.5865497589111328,
      "learning_rate": 4.9113924050632915e-05,
      "loss": 2.4066,
      "step": 388
    },
    {
      "epoch": 0.49240506329113926,
      "grad_norm": 1.6097311973571777,
      "learning_rate": 4.924050632911393e-05,
      "loss": 2.3843,
      "step": 389
    },
    {
      "epoch": 0.4936708860759494,
      "grad_norm": 1.6134854555130005,
      "learning_rate": 4.936708860759494e-05,
      "loss": 2.4519,
      "step": 390
    },
    {
      "epoch": 0.49493670886075947,
      "grad_norm": 1.593025803565979,
      "learning_rate": 4.949367088607595e-05,
      "loss": 2.416,
      "step": 391
    },
    {
      "epoch": 0.4962025316455696,
      "grad_norm": 1.5736299753189087,
      "learning_rate": 4.962025316455696e-05,
      "loss": 2.4105,
      "step": 392
    },
    {
      "epoch": 0.49746835443037973,
      "grad_norm": 1.5855903625488281,
      "learning_rate": 4.974683544303797e-05,
      "loss": 2.4016,
      "step": 393
    },
    {
      "epoch": 0.49873417721518987,
      "grad_norm": 1.5771225690841675,
      "learning_rate": 4.987341772151899e-05,
      "loss": 2.4101,
      "step": 394
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.603467583656311,
      "learning_rate": 5e-05,
      "loss": 2.4401,
      "step": 395
    },
    {
      "epoch": 0.5,
      "eval_loss": 2.6278257369995117,
      "eval_runtime": 175.17,
      "eval_samples_per_second": 513.101,
      "eval_steps_per_second": 4.013,
      "step": 395
    },
    {
      "epoch": 0.5012658227848101,
      "grad_norm": 1.5382626056671143,
      "learning_rate": 5e-05,
      "loss": 2.3438,
      "step": 396
    },
    {
      "epoch": 0.5025316455696203,
      "grad_norm": 1.5735352039337158,
      "learning_rate": 5e-05,
      "loss": 2.3484,
      "step": 397
    },
    {
      "epoch": 0.5037974683544304,
      "grad_norm": 1.5589778423309326,
      "learning_rate": 5e-05,
      "loss": 2.3966,
      "step": 398
    },
    {
      "epoch": 0.5050632911392405,
      "grad_norm": 1.535295844078064,
      "learning_rate": 5e-05,
      "loss": 2.3533,
      "step": 399
    },
    {
      "epoch": 0.5063291139240507,
      "grad_norm": 1.5822553634643555,
      "learning_rate": 5e-05,
      "loss": 2.4073,
      "step": 400
    },
    {
      "epoch": 0.5075949367088608,
      "grad_norm": 1.6132413148880005,
      "learning_rate": 5e-05,
      "loss": 2.4098,
      "step": 401
    },
    {
      "epoch": 0.5088607594936709,
      "grad_norm": 1.584959626197815,
      "learning_rate": 5e-05,
      "loss": 2.439,
      "step": 402
    },
    {
      "epoch": 0.5101265822784811,
      "grad_norm": 1.5735905170440674,
      "learning_rate": 5e-05,
      "loss": 2.3845,
      "step": 403
    },
    {
      "epoch": 0.5113924050632911,
      "grad_norm": 1.5301357507705688,
      "learning_rate": 5e-05,
      "loss": 2.3666,
      "step": 404
    },
    {
      "epoch": 0.5126582278481012,
      "grad_norm": 1.5474457740783691,
      "learning_rate": 5e-05,
      "loss": 2.3803,
      "step": 405
    },
    {
      "epoch": 0.5139240506329114,
      "grad_norm": 1.5635454654693604,
      "learning_rate": 5e-05,
      "loss": 2.4026,
      "step": 406
    },
    {
      "epoch": 0.5151898734177215,
      "grad_norm": 1.5740916728973389,
      "learning_rate": 5e-05,
      "loss": 2.3851,
      "step": 407
    },
    {
      "epoch": 0.5164556962025316,
      "grad_norm": 1.5503044128417969,
      "learning_rate": 5e-05,
      "loss": 2.4041,
      "step": 408
    },
    {
      "epoch": 0.5177215189873418,
      "grad_norm": 1.670659065246582,
      "learning_rate": 5e-05,
      "loss": 2.3592,
      "step": 409
    },
    {
      "epoch": 0.5189873417721519,
      "grad_norm": 1.590610384941101,
      "learning_rate": 5e-05,
      "loss": 2.2401,
      "step": 410
    },
    {
      "epoch": 0.520253164556962,
      "grad_norm": 1.6085604429244995,
      "learning_rate": 5e-05,
      "loss": 2.4641,
      "step": 411
    },
    {
      "epoch": 0.5215189873417722,
      "grad_norm": 1.5653672218322754,
      "learning_rate": 5e-05,
      "loss": 2.3311,
      "step": 412
    },
    {
      "epoch": 0.5227848101265823,
      "grad_norm": 1.5836442708969116,
      "learning_rate": 5e-05,
      "loss": 2.3402,
      "step": 413
    },
    {
      "epoch": 0.5240506329113924,
      "grad_norm": 1.5489132404327393,
      "learning_rate": 5e-05,
      "loss": 2.3533,
      "step": 414
    },
    {
      "epoch": 0.5253164556962026,
      "grad_norm": 1.5854034423828125,
      "learning_rate": 5e-05,
      "loss": 2.3865,
      "step": 415
    },
    {
      "epoch": 0.5265822784810127,
      "grad_norm": 1.529162049293518,
      "learning_rate": 5e-05,
      "loss": 2.3487,
      "step": 416
    },
    {
      "epoch": 0.5278481012658228,
      "grad_norm": 1.5390770435333252,
      "learning_rate": 5e-05,
      "loss": 2.3101,
      "step": 417
    },
    {
      "epoch": 0.529113924050633,
      "grad_norm": 1.5799190998077393,
      "learning_rate": 5e-05,
      "loss": 2.4669,
      "step": 418
    },
    {
      "epoch": 0.5303797468354431,
      "grad_norm": 1.620621919631958,
      "learning_rate": 5e-05,
      "loss": 2.3331,
      "step": 419
    },
    {
      "epoch": 0.5316455696202531,
      "grad_norm": 1.645871877670288,
      "learning_rate": 5e-05,
      "loss": 2.3595,
      "step": 420
    },
    {
      "epoch": 0.5329113924050632,
      "grad_norm": 1.5547829866409302,
      "learning_rate": 5e-05,
      "loss": 2.3948,
      "step": 421
    },
    {
      "epoch": 0.5341772151898734,
      "grad_norm": 1.5305215120315552,
      "learning_rate": 5e-05,
      "loss": 2.3583,
      "step": 422
    },
    {
      "epoch": 0.5354430379746835,
      "grad_norm": 1.5824955701828003,
      "learning_rate": 5e-05,
      "loss": 2.3802,
      "step": 423
    },
    {
      "epoch": 0.5367088607594936,
      "grad_norm": 1.5496914386749268,
      "learning_rate": 5e-05,
      "loss": 2.4193,
      "step": 424
    },
    {
      "epoch": 0.5379746835443038,
      "grad_norm": 1.561272382736206,
      "learning_rate": 5e-05,
      "loss": 2.3948,
      "step": 425
    },
    {
      "epoch": 0.5392405063291139,
      "grad_norm": 1.5585931539535522,
      "learning_rate": 5e-05,
      "loss": 2.402,
      "step": 426
    },
    {
      "epoch": 0.540506329113924,
      "grad_norm": 1.5780459642410278,
      "learning_rate": 5e-05,
      "loss": 2.3512,
      "step": 427
    },
    {
      "epoch": 0.5417721518987342,
      "grad_norm": 1.6152187585830688,
      "learning_rate": 5e-05,
      "loss": 2.3443,
      "step": 428
    },
    {
      "epoch": 0.5430379746835443,
      "grad_norm": 1.5627319812774658,
      "learning_rate": 5e-05,
      "loss": 2.3681,
      "step": 429
    },
    {
      "epoch": 0.5443037974683544,
      "grad_norm": 1.5175256729125977,
      "learning_rate": 5e-05,
      "loss": 2.3,
      "step": 430
    },
    {
      "epoch": 0.5455696202531646,
      "grad_norm": 1.4894129037857056,
      "learning_rate": 5e-05,
      "loss": 2.326,
      "step": 431
    },
    {
      "epoch": 0.5468354430379747,
      "grad_norm": 1.558251142501831,
      "learning_rate": 5e-05,
      "loss": 2.3282,
      "step": 432
    },
    {
      "epoch": 0.5481012658227848,
      "grad_norm": 1.5095056295394897,
      "learning_rate": 5e-05,
      "loss": 2.2449,
      "step": 433
    },
    {
      "epoch": 0.549367088607595,
      "grad_norm": 1.5623260736465454,
      "learning_rate": 5e-05,
      "loss": 2.3351,
      "step": 434
    },
    {
      "epoch": 0.5506329113924051,
      "grad_norm": 1.5638397932052612,
      "learning_rate": 5e-05,
      "loss": 2.3579,
      "step": 435
    },
    {
      "epoch": 0.5518987341772152,
      "grad_norm": 1.5033153295516968,
      "learning_rate": 5e-05,
      "loss": 2.2591,
      "step": 436
    },
    {
      "epoch": 0.5531645569620253,
      "grad_norm": 1.5473052263259888,
      "learning_rate": 5e-05,
      "loss": 2.3316,
      "step": 437
    },
    {
      "epoch": 0.5544303797468354,
      "grad_norm": 1.527398705482483,
      "learning_rate": 5e-05,
      "loss": 2.29,
      "step": 438
    },
    {
      "epoch": 0.5556962025316455,
      "grad_norm": 1.5603669881820679,
      "learning_rate": 5e-05,
      "loss": 2.3376,
      "step": 439
    },
    {
      "epoch": 0.5569620253164557,
      "grad_norm": 1.47902250289917,
      "learning_rate": 5e-05,
      "loss": 2.2266,
      "step": 440
    },
    {
      "epoch": 0.5582278481012658,
      "grad_norm": 1.5385394096374512,
      "learning_rate": 5e-05,
      "loss": 2.2897,
      "step": 441
    },
    {
      "epoch": 0.5594936708860759,
      "grad_norm": 1.5360084772109985,
      "learning_rate": 5e-05,
      "loss": 2.349,
      "step": 442
    },
    {
      "epoch": 0.5607594936708861,
      "grad_norm": 1.5468502044677734,
      "learning_rate": 5e-05,
      "loss": 2.3658,
      "step": 443
    },
    {
      "epoch": 0.5620253164556962,
      "grad_norm": 1.5268787145614624,
      "learning_rate": 5e-05,
      "loss": 2.3174,
      "step": 444
    },
    {
      "epoch": 0.5632911392405063,
      "grad_norm": 1.4883922338485718,
      "learning_rate": 5e-05,
      "loss": 2.2704,
      "step": 445
    },
    {
      "epoch": 0.5645569620253165,
      "grad_norm": 1.5461409091949463,
      "learning_rate": 5e-05,
      "loss": 2.3582,
      "step": 446
    },
    {
      "epoch": 0.5658227848101266,
      "grad_norm": 1.5642858743667603,
      "learning_rate": 5e-05,
      "loss": 2.2703,
      "step": 447
    },
    {
      "epoch": 0.5670886075949367,
      "grad_norm": 1.4967950582504272,
      "learning_rate": 5e-05,
      "loss": 2.2451,
      "step": 448
    },
    {
      "epoch": 0.5683544303797469,
      "grad_norm": 1.5226367712020874,
      "learning_rate": 5e-05,
      "loss": 2.2959,
      "step": 449
    },
    {
      "epoch": 0.569620253164557,
      "grad_norm": 1.5789148807525635,
      "learning_rate": 5e-05,
      "loss": 2.3205,
      "step": 450
    },
    {
      "epoch": 0.5708860759493671,
      "grad_norm": 1.5497461557388306,
      "learning_rate": 5e-05,
      "loss": 2.3916,
      "step": 451
    },
    {
      "epoch": 0.5721518987341773,
      "grad_norm": 1.5386422872543335,
      "learning_rate": 5e-05,
      "loss": 2.3301,
      "step": 452
    },
    {
      "epoch": 0.5734177215189873,
      "grad_norm": 1.4865394830703735,
      "learning_rate": 5e-05,
      "loss": 2.2225,
      "step": 453
    },
    {
      "epoch": 0.5746835443037974,
      "grad_norm": 1.4614589214324951,
      "learning_rate": 5e-05,
      "loss": 2.2653,
      "step": 454
    },
    {
      "epoch": 0.5759493670886076,
      "grad_norm": 1.538985252380371,
      "learning_rate": 5e-05,
      "loss": 2.3014,
      "step": 455
    },
    {
      "epoch": 0.5772151898734177,
      "grad_norm": 1.50347101688385,
      "learning_rate": 5e-05,
      "loss": 2.311,
      "step": 456
    },
    {
      "epoch": 0.5784810126582278,
      "grad_norm": 1.527516484260559,
      "learning_rate": 5e-05,
      "loss": 2.197,
      "step": 457
    },
    {
      "epoch": 0.579746835443038,
      "grad_norm": 1.537571907043457,
      "learning_rate": 5e-05,
      "loss": 2.2706,
      "step": 458
    },
    {
      "epoch": 0.5810126582278481,
      "grad_norm": 1.474377155303955,
      "learning_rate": 5e-05,
      "loss": 2.243,
      "step": 459
    },
    {
      "epoch": 0.5822784810126582,
      "grad_norm": 1.517013669013977,
      "learning_rate": 5e-05,
      "loss": 2.3181,
      "step": 460
    },
    {
      "epoch": 0.5835443037974684,
      "grad_norm": 1.5195542573928833,
      "learning_rate": 5e-05,
      "loss": 2.3151,
      "step": 461
    },
    {
      "epoch": 0.5848101265822785,
      "grad_norm": 1.5629066228866577,
      "learning_rate": 5e-05,
      "loss": 2.3064,
      "step": 462
    },
    {
      "epoch": 0.5860759493670886,
      "grad_norm": 1.5197923183441162,
      "learning_rate": 5e-05,
      "loss": 2.2774,
      "step": 463
    },
    {
      "epoch": 0.5873417721518988,
      "grad_norm": 1.5337142944335938,
      "learning_rate": 5e-05,
      "loss": 2.3052,
      "step": 464
    },
    {
      "epoch": 0.5886075949367089,
      "grad_norm": 1.4760454893112183,
      "learning_rate": 5e-05,
      "loss": 2.2092,
      "step": 465
    },
    {
      "epoch": 0.589873417721519,
      "grad_norm": 1.4880716800689697,
      "learning_rate": 5e-05,
      "loss": 2.2388,
      "step": 466
    },
    {
      "epoch": 0.5911392405063292,
      "grad_norm": 1.5210717916488647,
      "learning_rate": 5e-05,
      "loss": 2.3346,
      "step": 467
    },
    {
      "epoch": 0.5924050632911393,
      "grad_norm": 1.4990288019180298,
      "learning_rate": 5e-05,
      "loss": 2.3303,
      "step": 468
    },
    {
      "epoch": 0.5936708860759494,
      "grad_norm": 1.5170373916625977,
      "learning_rate": 5e-05,
      "loss": 2.2619,
      "step": 469
    },
    {
      "epoch": 0.5949367088607594,
      "grad_norm": 1.5162866115570068,
      "learning_rate": 5e-05,
      "loss": 2.2833,
      "step": 470
    },
    {
      "epoch": 0.5962025316455696,
      "grad_norm": 1.5824886560440063,
      "learning_rate": 5e-05,
      "loss": 2.288,
      "step": 471
    },
    {
      "epoch": 0.5974683544303797,
      "grad_norm": 1.5373014211654663,
      "learning_rate": 5e-05,
      "loss": 2.308,
      "step": 472
    },
    {
      "epoch": 0.5987341772151898,
      "grad_norm": 1.4987868070602417,
      "learning_rate": 5e-05,
      "loss": 2.2406,
      "step": 473
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.5017151832580566,
      "learning_rate": 5e-05,
      "loss": 2.2424,
      "step": 474
    },
    {
      "epoch": 0.6012658227848101,
      "grad_norm": 1.5216377973556519,
      "learning_rate": 5e-05,
      "loss": 2.2768,
      "step": 475
    },
    {
      "epoch": 0.6025316455696202,
      "grad_norm": 1.5211610794067383,
      "learning_rate": 5e-05,
      "loss": 2.3191,
      "step": 476
    },
    {
      "epoch": 0.6037974683544304,
      "grad_norm": 1.4845714569091797,
      "learning_rate": 5e-05,
      "loss": 2.2629,
      "step": 477
    },
    {
      "epoch": 0.6050632911392405,
      "grad_norm": 1.4653795957565308,
      "learning_rate": 5e-05,
      "loss": 2.2297,
      "step": 478
    },
    {
      "epoch": 0.6063291139240506,
      "grad_norm": 1.451793909072876,
      "learning_rate": 5e-05,
      "loss": 2.1362,
      "step": 479
    },
    {
      "epoch": 0.6075949367088608,
      "grad_norm": 1.4985082149505615,
      "learning_rate": 5e-05,
      "loss": 2.271,
      "step": 480
    },
    {
      "epoch": 0.6088607594936709,
      "grad_norm": 1.5027302503585815,
      "learning_rate": 5e-05,
      "loss": 2.2596,
      "step": 481
    },
    {
      "epoch": 0.610126582278481,
      "grad_norm": 1.4896093606948853,
      "learning_rate": 5e-05,
      "loss": 2.2225,
      "step": 482
    },
    {
      "epoch": 0.6113924050632912,
      "grad_norm": 1.4607517719268799,
      "learning_rate": 5e-05,
      "loss": 2.2614,
      "step": 483
    },
    {
      "epoch": 0.6126582278481013,
      "grad_norm": 1.5175542831420898,
      "learning_rate": 5e-05,
      "loss": 2.2725,
      "step": 484
    },
    {
      "epoch": 0.6139240506329114,
      "grad_norm": 1.488243579864502,
      "learning_rate": 5e-05,
      "loss": 2.2667,
      "step": 485
    },
    {
      "epoch": 0.6151898734177215,
      "grad_norm": 1.5078366994857788,
      "learning_rate": 5e-05,
      "loss": 2.2381,
      "step": 486
    },
    {
      "epoch": 0.6164556962025316,
      "grad_norm": 1.592624306678772,
      "learning_rate": 5e-05,
      "loss": 2.2619,
      "step": 487
    },
    {
      "epoch": 0.6177215189873417,
      "grad_norm": 1.506427526473999,
      "learning_rate": 5e-05,
      "loss": 2.2991,
      "step": 488
    },
    {
      "epoch": 0.6189873417721519,
      "grad_norm": 1.4918105602264404,
      "learning_rate": 5e-05,
      "loss": 2.1749,
      "step": 489
    },
    {
      "epoch": 0.620253164556962,
      "grad_norm": 1.4432275295257568,
      "learning_rate": 5e-05,
      "loss": 2.2117,
      "step": 490
    },
    {
      "epoch": 0.6215189873417721,
      "grad_norm": 1.4814984798431396,
      "learning_rate": 5e-05,
      "loss": 2.1378,
      "step": 491
    },
    {
      "epoch": 0.6227848101265823,
      "grad_norm": 1.4804000854492188,
      "learning_rate": 5e-05,
      "loss": 2.2386,
      "step": 492
    },
    {
      "epoch": 0.6240506329113924,
      "grad_norm": 1.4736214876174927,
      "learning_rate": 5e-05,
      "loss": 2.2052,
      "step": 493
    },
    {
      "epoch": 0.6253164556962025,
      "grad_norm": 1.4505120515823364,
      "learning_rate": 5e-05,
      "loss": 2.1843,
      "step": 494
    },
    {
      "epoch": 0.6265822784810127,
      "grad_norm": 1.5476078987121582,
      "learning_rate": 5e-05,
      "loss": 2.2455,
      "step": 495
    },
    {
      "epoch": 0.6278481012658228,
      "grad_norm": 1.5114178657531738,
      "learning_rate": 5e-05,
      "loss": 2.2517,
      "step": 496
    },
    {
      "epoch": 0.6291139240506329,
      "grad_norm": 1.5188299417495728,
      "learning_rate": 5e-05,
      "loss": 2.1896,
      "step": 497
    },
    {
      "epoch": 0.6303797468354431,
      "grad_norm": 1.4808958768844604,
      "learning_rate": 5e-05,
      "loss": 2.1805,
      "step": 498
    },
    {
      "epoch": 0.6316455696202532,
      "grad_norm": 1.484978199005127,
      "learning_rate": 5e-05,
      "loss": 2.2854,
      "step": 499
    },
    {
      "epoch": 0.6329113924050633,
      "grad_norm": 1.4940290451049805,
      "learning_rate": 5e-05,
      "loss": 2.287,
      "step": 500
    },
    {
      "epoch": 0.6341772151898735,
      "grad_norm": 1.4630600214004517,
      "learning_rate": 5e-05,
      "loss": 2.2559,
      "step": 501
    },
    {
      "epoch": 0.6354430379746835,
      "grad_norm": 1.4843406677246094,
      "learning_rate": 5e-05,
      "loss": 2.1762,
      "step": 502
    },
    {
      "epoch": 0.6367088607594936,
      "grad_norm": 1.5111864805221558,
      "learning_rate": 5e-05,
      "loss": 2.2763,
      "step": 503
    },
    {
      "epoch": 0.6379746835443038,
      "grad_norm": 1.459041714668274,
      "learning_rate": 5e-05,
      "loss": 2.1406,
      "step": 504
    },
    {
      "epoch": 0.6392405063291139,
      "grad_norm": 1.5244489908218384,
      "learning_rate": 5e-05,
      "loss": 2.2233,
      "step": 505
    },
    {
      "epoch": 0.640506329113924,
      "grad_norm": 1.4996395111083984,
      "learning_rate": 5e-05,
      "loss": 2.2275,
      "step": 506
    },
    {
      "epoch": 0.6417721518987342,
      "grad_norm": 1.4423142671585083,
      "learning_rate": 5e-05,
      "loss": 2.1781,
      "step": 507
    },
    {
      "epoch": 0.6430379746835443,
      "grad_norm": 1.413597583770752,
      "learning_rate": 5e-05,
      "loss": 2.1337,
      "step": 508
    },
    {
      "epoch": 0.6443037974683544,
      "grad_norm": 1.4922926425933838,
      "learning_rate": 5e-05,
      "loss": 2.2193,
      "step": 509
    },
    {
      "epoch": 0.6455696202531646,
      "grad_norm": 1.442061424255371,
      "learning_rate": 5e-05,
      "loss": 2.122,
      "step": 510
    },
    {
      "epoch": 0.6468354430379747,
      "grad_norm": 1.4833751916885376,
      "learning_rate": 5e-05,
      "loss": 2.1806,
      "step": 511
    },
    {
      "epoch": 0.6481012658227848,
      "grad_norm": 1.4990758895874023,
      "learning_rate": 5e-05,
      "loss": 2.1781,
      "step": 512
    },
    {
      "epoch": 0.649367088607595,
      "grad_norm": 1.4347155094146729,
      "learning_rate": 5e-05,
      "loss": 2.1912,
      "step": 513
    },
    {
      "epoch": 0.6506329113924051,
      "grad_norm": 1.4319170713424683,
      "learning_rate": 5e-05,
      "loss": 2.1803,
      "step": 514
    },
    {
      "epoch": 0.6518987341772152,
      "grad_norm": 1.4881253242492676,
      "learning_rate": 5e-05,
      "loss": 2.2218,
      "step": 515
    },
    {
      "epoch": 0.6531645569620254,
      "grad_norm": 1.4196281433105469,
      "learning_rate": 5e-05,
      "loss": 2.1218,
      "step": 516
    },
    {
      "epoch": 0.6544303797468355,
      "grad_norm": 1.4481053352355957,
      "learning_rate": 5e-05,
      "loss": 2.2129,
      "step": 517
    },
    {
      "epoch": 0.6556962025316456,
      "grad_norm": 1.4386992454528809,
      "learning_rate": 5e-05,
      "loss": 2.188,
      "step": 518
    },
    {
      "epoch": 0.6569620253164556,
      "grad_norm": 1.4726324081420898,
      "learning_rate": 5e-05,
      "loss": 2.2025,
      "step": 519
    },
    {
      "epoch": 0.6582278481012658,
      "grad_norm": 1.4463129043579102,
      "learning_rate": 5e-05,
      "loss": 2.121,
      "step": 520
    },
    {
      "epoch": 0.6594936708860759,
      "grad_norm": 1.4981588125228882,
      "learning_rate": 5e-05,
      "loss": 2.2346,
      "step": 521
    },
    {
      "epoch": 0.660759493670886,
      "grad_norm": 1.4858404397964478,
      "learning_rate": 5e-05,
      "loss": 2.2002,
      "step": 522
    },
    {
      "epoch": 0.6620253164556962,
      "grad_norm": 1.498050332069397,
      "learning_rate": 5e-05,
      "loss": 2.2185,
      "step": 523
    },
    {
      "epoch": 0.6632911392405063,
      "grad_norm": 1.459643840789795,
      "learning_rate": 5e-05,
      "loss": 2.2225,
      "step": 524
    },
    {
      "epoch": 0.6645569620253164,
      "grad_norm": 1.4254064559936523,
      "learning_rate": 5e-05,
      "loss": 2.2129,
      "step": 525
    },
    {
      "epoch": 0.6658227848101266,
      "grad_norm": 1.5156223773956299,
      "learning_rate": 5e-05,
      "loss": 2.2343,
      "step": 526
    },
    {
      "epoch": 0.6670886075949367,
      "grad_norm": 1.5021101236343384,
      "learning_rate": 5e-05,
      "loss": 2.191,
      "step": 527
    },
    {
      "epoch": 0.6683544303797468,
      "grad_norm": 1.4555246829986572,
      "learning_rate": 5e-05,
      "loss": 2.1371,
      "step": 528
    },
    {
      "epoch": 0.669620253164557,
      "grad_norm": 1.4218058586120605,
      "learning_rate": 5e-05,
      "loss": 2.1153,
      "step": 529
    },
    {
      "epoch": 0.6708860759493671,
      "grad_norm": 1.4465687274932861,
      "learning_rate": 5e-05,
      "loss": 2.1628,
      "step": 530
    },
    {
      "epoch": 0.6721518987341772,
      "grad_norm": 1.5009353160858154,
      "learning_rate": 5e-05,
      "loss": 2.1559,
      "step": 531
    },
    {
      "epoch": 0.6734177215189874,
      "grad_norm": 1.4439122676849365,
      "learning_rate": 5e-05,
      "loss": 2.1106,
      "step": 532
    },
    {
      "epoch": 0.6746835443037975,
      "grad_norm": 1.4538235664367676,
      "learning_rate": 5e-05,
      "loss": 2.1067,
      "step": 533
    },
    {
      "epoch": 0.6759493670886076,
      "grad_norm": 1.420234203338623,
      "learning_rate": 5e-05,
      "loss": 2.162,
      "step": 534
    },
    {
      "epoch": 0.6772151898734177,
      "grad_norm": 1.4239097833633423,
      "learning_rate": 5e-05,
      "loss": 2.1406,
      "step": 535
    },
    {
      "epoch": 0.6784810126582278,
      "grad_norm": 1.4483586549758911,
      "learning_rate": 5e-05,
      "loss": 2.1647,
      "step": 536
    },
    {
      "epoch": 0.6797468354430379,
      "grad_norm": 1.4843755960464478,
      "learning_rate": 5e-05,
      "loss": 2.1165,
      "step": 537
    },
    {
      "epoch": 0.6810126582278481,
      "grad_norm": 1.4418781995773315,
      "learning_rate": 5e-05,
      "loss": 2.2224,
      "step": 538
    },
    {
      "epoch": 0.6822784810126582,
      "grad_norm": 1.485898494720459,
      "learning_rate": 5e-05,
      "loss": 2.1611,
      "step": 539
    },
    {
      "epoch": 0.6835443037974683,
      "grad_norm": 1.4731392860412598,
      "learning_rate": 5e-05,
      "loss": 2.2155,
      "step": 540
    },
    {
      "epoch": 0.6848101265822785,
      "grad_norm": 1.465959906578064,
      "learning_rate": 5e-05,
      "loss": 2.1824,
      "step": 541
    },
    {
      "epoch": 0.6860759493670886,
      "grad_norm": 1.55293869972229,
      "learning_rate": 5e-05,
      "loss": 2.1243,
      "step": 542
    },
    {
      "epoch": 0.6873417721518987,
      "grad_norm": 1.4671635627746582,
      "learning_rate": 5e-05,
      "loss": 2.1445,
      "step": 543
    },
    {
      "epoch": 0.6886075949367089,
      "grad_norm": 1.4723292589187622,
      "learning_rate": 5e-05,
      "loss": 2.1673,
      "step": 544
    },
    {
      "epoch": 0.689873417721519,
      "grad_norm": 1.4455410242080688,
      "learning_rate": 5e-05,
      "loss": 2.1594,
      "step": 545
    },
    {
      "epoch": 0.6911392405063291,
      "grad_norm": 1.4567136764526367,
      "learning_rate": 5e-05,
      "loss": 2.0831,
      "step": 546
    },
    {
      "epoch": 0.6924050632911393,
      "grad_norm": 1.4776445627212524,
      "learning_rate": 5e-05,
      "loss": 2.1808,
      "step": 547
    },
    {
      "epoch": 0.6936708860759494,
      "grad_norm": 1.3944417238235474,
      "learning_rate": 5e-05,
      "loss": 2.0991,
      "step": 548
    },
    {
      "epoch": 0.6949367088607595,
      "grad_norm": 1.4555010795593262,
      "learning_rate": 5e-05,
      "loss": 2.1082,
      "step": 549
    },
    {
      "epoch": 0.6962025316455697,
      "grad_norm": 1.3878703117370605,
      "learning_rate": 5e-05,
      "loss": 2.08,
      "step": 550
    },
    {
      "epoch": 0.6974683544303798,
      "grad_norm": 1.4424831867218018,
      "learning_rate": 5e-05,
      "loss": 2.117,
      "step": 551
    },
    {
      "epoch": 0.6987341772151898,
      "grad_norm": 1.4721617698669434,
      "learning_rate": 5e-05,
      "loss": 2.1632,
      "step": 552
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.4497883319854736,
      "learning_rate": 5e-05,
      "loss": 2.2234,
      "step": 553
    },
    {
      "epoch": 0.7012658227848101,
      "grad_norm": 1.396102786064148,
      "learning_rate": 5e-05,
      "loss": 2.0138,
      "step": 554
    },
    {
      "epoch": 0.7025316455696202,
      "grad_norm": 1.4258140325546265,
      "learning_rate": 5e-05,
      "loss": 2.1152,
      "step": 555
    },
    {
      "epoch": 0.7037974683544304,
      "grad_norm": 1.3959137201309204,
      "learning_rate": 5e-05,
      "loss": 2.0908,
      "step": 556
    },
    {
      "epoch": 0.7050632911392405,
      "grad_norm": 1.459261417388916,
      "learning_rate": 5e-05,
      "loss": 2.1472,
      "step": 557
    },
    {
      "epoch": 0.7063291139240506,
      "grad_norm": 1.4349676370620728,
      "learning_rate": 5e-05,
      "loss": 2.1762,
      "step": 558
    },
    {
      "epoch": 0.7075949367088608,
      "grad_norm": 1.46059250831604,
      "learning_rate": 5e-05,
      "loss": 2.1791,
      "step": 559
    },
    {
      "epoch": 0.7088607594936709,
      "grad_norm": 1.4023945331573486,
      "learning_rate": 5e-05,
      "loss": 2.0419,
      "step": 560
    },
    {
      "epoch": 0.710126582278481,
      "grad_norm": 1.4441107511520386,
      "learning_rate": 5e-05,
      "loss": 2.1566,
      "step": 561
    },
    {
      "epoch": 0.7113924050632912,
      "grad_norm": 1.4680049419403076,
      "learning_rate": 5e-05,
      "loss": 2.1114,
      "step": 562
    },
    {
      "epoch": 0.7126582278481013,
      "grad_norm": 1.4550012350082397,
      "learning_rate": 5e-05,
      "loss": 2.1172,
      "step": 563
    },
    {
      "epoch": 0.7139240506329114,
      "grad_norm": 1.393700361251831,
      "learning_rate": 5e-05,
      "loss": 1.995,
      "step": 564
    },
    {
      "epoch": 0.7151898734177216,
      "grad_norm": 1.4307224750518799,
      "learning_rate": 5e-05,
      "loss": 2.0887,
      "step": 565
    },
    {
      "epoch": 0.7164556962025317,
      "grad_norm": 1.4763504266738892,
      "learning_rate": 5e-05,
      "loss": 2.1567,
      "step": 566
    },
    {
      "epoch": 0.7177215189873418,
      "grad_norm": 1.4907070398330688,
      "learning_rate": 5e-05,
      "loss": 2.1421,
      "step": 567
    },
    {
      "epoch": 0.7189873417721518,
      "grad_norm": 1.4459466934204102,
      "learning_rate": 5e-05,
      "loss": 2.1052,
      "step": 568
    },
    {
      "epoch": 0.720253164556962,
      "grad_norm": 1.470920443534851,
      "learning_rate": 5e-05,
      "loss": 2.1732,
      "step": 569
    },
    {
      "epoch": 0.7215189873417721,
      "grad_norm": 1.3974721431732178,
      "learning_rate": 5e-05,
      "loss": 2.0606,
      "step": 570
    },
    {
      "epoch": 0.7227848101265822,
      "grad_norm": 1.3965145349502563,
      "learning_rate": 5e-05,
      "loss": 2.0793,
      "step": 571
    },
    {
      "epoch": 0.7240506329113924,
      "grad_norm": 1.4404217004776,
      "learning_rate": 5e-05,
      "loss": 2.1997,
      "step": 572
    },
    {
      "epoch": 0.7253164556962025,
      "grad_norm": 1.4551631212234497,
      "learning_rate": 5e-05,
      "loss": 2.1775,
      "step": 573
    },
    {
      "epoch": 0.7265822784810126,
      "grad_norm": 1.4275461435317993,
      "learning_rate": 5e-05,
      "loss": 2.1166,
      "step": 574
    },
    {
      "epoch": 0.7278481012658228,
      "grad_norm": 1.4102238416671753,
      "learning_rate": 5e-05,
      "loss": 2.0769,
      "step": 575
    },
    {
      "epoch": 0.7291139240506329,
      "grad_norm": 1.443498969078064,
      "learning_rate": 5e-05,
      "loss": 2.1257,
      "step": 576
    },
    {
      "epoch": 0.730379746835443,
      "grad_norm": 1.4072747230529785,
      "learning_rate": 5e-05,
      "loss": 2.1048,
      "step": 577
    },
    {
      "epoch": 0.7316455696202532,
      "grad_norm": 1.4240846633911133,
      "learning_rate": 5e-05,
      "loss": 2.0518,
      "step": 578
    },
    {
      "epoch": 0.7329113924050633,
      "grad_norm": 1.3993511199951172,
      "learning_rate": 5e-05,
      "loss": 2.0449,
      "step": 579
    },
    {
      "epoch": 0.7341772151898734,
      "grad_norm": 1.4363969564437866,
      "learning_rate": 5e-05,
      "loss": 2.1352,
      "step": 580
    },
    {
      "epoch": 0.7354430379746836,
      "grad_norm": 1.4683475494384766,
      "learning_rate": 5e-05,
      "loss": 2.0864,
      "step": 581
    },
    {
      "epoch": 0.7367088607594937,
      "grad_norm": 1.3676807880401611,
      "learning_rate": 5e-05,
      "loss": 2.007,
      "step": 582
    },
    {
      "epoch": 0.7379746835443038,
      "grad_norm": 1.3964506387710571,
      "learning_rate": 5e-05,
      "loss": 2.0931,
      "step": 583
    },
    {
      "epoch": 0.739240506329114,
      "grad_norm": 1.4298310279846191,
      "learning_rate": 5e-05,
      "loss": 2.0483,
      "step": 584
    },
    {
      "epoch": 0.740506329113924,
      "grad_norm": 1.472258448600769,
      "learning_rate": 5e-05,
      "loss": 2.1435,
      "step": 585
    },
    {
      "epoch": 0.7417721518987341,
      "grad_norm": 1.455911636352539,
      "learning_rate": 5e-05,
      "loss": 2.0771,
      "step": 586
    },
    {
      "epoch": 0.7430379746835443,
      "grad_norm": 1.4011427164077759,
      "learning_rate": 5e-05,
      "loss": 2.0448,
      "step": 587
    },
    {
      "epoch": 0.7443037974683544,
      "grad_norm": 1.4216266870498657,
      "learning_rate": 5e-05,
      "loss": 2.0577,
      "step": 588
    },
    {
      "epoch": 0.7455696202531645,
      "grad_norm": 1.457254409790039,
      "learning_rate": 5e-05,
      "loss": 2.1931,
      "step": 589
    },
    {
      "epoch": 0.7468354430379747,
      "grad_norm": 1.4343122243881226,
      "learning_rate": 5e-05,
      "loss": 2.1781,
      "step": 590
    },
    {
      "epoch": 0.7481012658227848,
      "grad_norm": 1.430513620376587,
      "learning_rate": 5e-05,
      "loss": 2.0794,
      "step": 591
    },
    {
      "epoch": 0.7493670886075949,
      "grad_norm": 1.4665831327438354,
      "learning_rate": 5e-05,
      "loss": 2.1237,
      "step": 592
    },
    {
      "epoch": 0.7506329113924051,
      "grad_norm": 1.4295909404754639,
      "learning_rate": 5e-05,
      "loss": 2.0656,
      "step": 593
    },
    {
      "epoch": 0.7518987341772152,
      "grad_norm": 1.4138948917388916,
      "learning_rate": 5e-05,
      "loss": 2.0678,
      "step": 594
    },
    {
      "epoch": 0.7531645569620253,
      "grad_norm": 1.4056731462478638,
      "learning_rate": 5e-05,
      "loss": 1.9743,
      "step": 595
    },
    {
      "epoch": 0.7544303797468355,
      "grad_norm": 1.4087655544281006,
      "learning_rate": 5e-05,
      "loss": 2.1099,
      "step": 596
    },
    {
      "epoch": 0.7556962025316456,
      "grad_norm": 1.4156471490859985,
      "learning_rate": 5e-05,
      "loss": 2.0726,
      "step": 597
    },
    {
      "epoch": 0.7569620253164557,
      "grad_norm": 1.430812120437622,
      "learning_rate": 5e-05,
      "loss": 2.0761,
      "step": 598
    },
    {
      "epoch": 0.7582278481012659,
      "grad_norm": 1.4201223850250244,
      "learning_rate": 5e-05,
      "loss": 2.1503,
      "step": 599
    },
    {
      "epoch": 0.759493670886076,
      "grad_norm": 1.4378103017807007,
      "learning_rate": 5e-05,
      "loss": 2.1681,
      "step": 600
    },
    {
      "epoch": 0.760759493670886,
      "grad_norm": 1.3872579336166382,
      "learning_rate": 5e-05,
      "loss": 2.0154,
      "step": 601
    },
    {
      "epoch": 0.7620253164556962,
      "grad_norm": 1.4188406467437744,
      "learning_rate": 5e-05,
      "loss": 2.049,
      "step": 602
    },
    {
      "epoch": 0.7632911392405063,
      "grad_norm": 1.432478666305542,
      "learning_rate": 5e-05,
      "loss": 2.1016,
      "step": 603
    },
    {
      "epoch": 0.7645569620253164,
      "grad_norm": 1.4099828004837036,
      "learning_rate": 5e-05,
      "loss": 2.0754,
      "step": 604
    },
    {
      "epoch": 0.7658227848101266,
      "grad_norm": 1.3860563039779663,
      "learning_rate": 5e-05,
      "loss": 2.0503,
      "step": 605
    },
    {
      "epoch": 0.7670886075949367,
      "grad_norm": 1.3904160261154175,
      "learning_rate": 5e-05,
      "loss": 2.049,
      "step": 606
    },
    {
      "epoch": 0.7683544303797468,
      "grad_norm": 1.4288387298583984,
      "learning_rate": 5e-05,
      "loss": 1.9847,
      "step": 607
    },
    {
      "epoch": 0.769620253164557,
      "grad_norm": 1.4020477533340454,
      "learning_rate": 5e-05,
      "loss": 2.1157,
      "step": 608
    },
    {
      "epoch": 0.7708860759493671,
      "grad_norm": 1.4036552906036377,
      "learning_rate": 5e-05,
      "loss": 2.0372,
      "step": 609
    },
    {
      "epoch": 0.7721518987341772,
      "grad_norm": 1.464194893836975,
      "learning_rate": 5e-05,
      "loss": 2.0587,
      "step": 610
    },
    {
      "epoch": 0.7734177215189874,
      "grad_norm": 1.4126577377319336,
      "learning_rate": 5e-05,
      "loss": 2.0609,
      "step": 611
    },
    {
      "epoch": 0.7746835443037975,
      "grad_norm": 1.4198963642120361,
      "learning_rate": 5e-05,
      "loss": 2.0942,
      "step": 612
    },
    {
      "epoch": 0.7759493670886076,
      "grad_norm": 1.4444032907485962,
      "learning_rate": 5e-05,
      "loss": 2.1719,
      "step": 613
    },
    {
      "epoch": 0.7772151898734178,
      "grad_norm": 1.368922472000122,
      "learning_rate": 5e-05,
      "loss": 2.0762,
      "step": 614
    },
    {
      "epoch": 0.7784810126582279,
      "grad_norm": 1.4126601219177246,
      "learning_rate": 5e-05,
      "loss": 2.0806,
      "step": 615
    },
    {
      "epoch": 0.779746835443038,
      "grad_norm": 1.4130595922470093,
      "learning_rate": 5e-05,
      "loss": 2.082,
      "step": 616
    },
    {
      "epoch": 0.7810126582278482,
      "grad_norm": 1.4679794311523438,
      "learning_rate": 5e-05,
      "loss": 2.0835,
      "step": 617
    },
    {
      "epoch": 0.7822784810126582,
      "grad_norm": 1.4133543968200684,
      "learning_rate": 5e-05,
      "loss": 2.0707,
      "step": 618
    },
    {
      "epoch": 0.7835443037974683,
      "grad_norm": 1.4228168725967407,
      "learning_rate": 5e-05,
      "loss": 2.0454,
      "step": 619
    },
    {
      "epoch": 0.7848101265822784,
      "grad_norm": 1.4275989532470703,
      "learning_rate": 5e-05,
      "loss": 2.0885,
      "step": 620
    },
    {
      "epoch": 0.7860759493670886,
      "grad_norm": 1.3932856321334839,
      "learning_rate": 5e-05,
      "loss": 2.0091,
      "step": 621
    },
    {
      "epoch": 0.7873417721518987,
      "grad_norm": 1.3963353633880615,
      "learning_rate": 5e-05,
      "loss": 2.0779,
      "step": 622
    },
    {
      "epoch": 0.7886075949367088,
      "grad_norm": 1.4650641679763794,
      "learning_rate": 5e-05,
      "loss": 2.0371,
      "step": 623
    },
    {
      "epoch": 0.789873417721519,
      "grad_norm": 1.4336726665496826,
      "learning_rate": 5e-05,
      "loss": 2.1002,
      "step": 624
    },
    {
      "epoch": 0.7911392405063291,
      "grad_norm": 1.3627376556396484,
      "learning_rate": 5e-05,
      "loss": 2.0566,
      "step": 625
    },
    {
      "epoch": 0.7924050632911392,
      "grad_norm": 1.449538230895996,
      "learning_rate": 5e-05,
      "loss": 2.1065,
      "step": 626
    },
    {
      "epoch": 0.7936708860759494,
      "grad_norm": 1.4037309885025024,
      "learning_rate": 5e-05,
      "loss": 1.9943,
      "step": 627
    },
    {
      "epoch": 0.7949367088607595,
      "grad_norm": 1.395398497581482,
      "learning_rate": 5e-05,
      "loss": 2.0923,
      "step": 628
    },
    {
      "epoch": 0.7962025316455696,
      "grad_norm": 1.4164979457855225,
      "learning_rate": 5e-05,
      "loss": 2.0611,
      "step": 629
    },
    {
      "epoch": 0.7974683544303798,
      "grad_norm": 1.4320909976959229,
      "learning_rate": 5e-05,
      "loss": 2.0981,
      "step": 630
    },
    {
      "epoch": 0.7987341772151899,
      "grad_norm": 1.379173755645752,
      "learning_rate": 5e-05,
      "loss": 2.0174,
      "step": 631
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.4231175184249878,
      "learning_rate": 5e-05,
      "loss": 2.0483,
      "step": 632
    },
    {
      "epoch": 0.8012658227848102,
      "grad_norm": 1.453655481338501,
      "learning_rate": 5e-05,
      "loss": 2.0438,
      "step": 633
    },
    {
      "epoch": 0.8025316455696202,
      "grad_norm": 1.4043264389038086,
      "learning_rate": 5e-05,
      "loss": 1.9412,
      "step": 634
    },
    {
      "epoch": 0.8037974683544303,
      "grad_norm": 1.4130496978759766,
      "learning_rate": 5e-05,
      "loss": 1.9985,
      "step": 635
    },
    {
      "epoch": 0.8050632911392405,
      "grad_norm": 1.435680866241455,
      "learning_rate": 5e-05,
      "loss": 2.0566,
      "step": 636
    },
    {
      "epoch": 0.8063291139240506,
      "grad_norm": 1.426299810409546,
      "learning_rate": 5e-05,
      "loss": 2.064,
      "step": 637
    },
    {
      "epoch": 0.8075949367088607,
      "grad_norm": 1.4137660264968872,
      "learning_rate": 5e-05,
      "loss": 2.0735,
      "step": 638
    },
    {
      "epoch": 0.8088607594936709,
      "grad_norm": 1.4027477502822876,
      "learning_rate": 5e-05,
      "loss": 1.9569,
      "step": 639
    },
    {
      "epoch": 0.810126582278481,
      "grad_norm": 1.4439102411270142,
      "learning_rate": 5e-05,
      "loss": 2.0202,
      "step": 640
    },
    {
      "epoch": 0.8113924050632911,
      "grad_norm": 1.3810220956802368,
      "learning_rate": 5e-05,
      "loss": 1.9903,
      "step": 641
    },
    {
      "epoch": 0.8126582278481013,
      "grad_norm": 1.4263819456100464,
      "learning_rate": 5e-05,
      "loss": 2.0481,
      "step": 642
    },
    {
      "epoch": 0.8139240506329114,
      "grad_norm": 1.436011791229248,
      "learning_rate": 5e-05,
      "loss": 2.0846,
      "step": 643
    },
    {
      "epoch": 0.8151898734177215,
      "grad_norm": 1.3891955614089966,
      "learning_rate": 5e-05,
      "loss": 2.0083,
      "step": 644
    },
    {
      "epoch": 0.8164556962025317,
      "grad_norm": 1.408120036125183,
      "learning_rate": 5e-05,
      "loss": 2.0994,
      "step": 645
    },
    {
      "epoch": 0.8177215189873418,
      "grad_norm": 1.3885973691940308,
      "learning_rate": 5e-05,
      "loss": 1.9925,
      "step": 646
    },
    {
      "epoch": 0.8189873417721519,
      "grad_norm": 1.4165031909942627,
      "learning_rate": 5e-05,
      "loss": 2.0192,
      "step": 647
    },
    {
      "epoch": 0.8202531645569621,
      "grad_norm": 1.373772382736206,
      "learning_rate": 5e-05,
      "loss": 1.9958,
      "step": 648
    },
    {
      "epoch": 0.8215189873417722,
      "grad_norm": 1.4509210586547852,
      "learning_rate": 5e-05,
      "loss": 2.0272,
      "step": 649
    },
    {
      "epoch": 0.8227848101265823,
      "grad_norm": 1.4104335308074951,
      "learning_rate": 5e-05,
      "loss": 2.0091,
      "step": 650
    },
    {
      "epoch": 0.8240506329113924,
      "grad_norm": 1.3833367824554443,
      "learning_rate": 5e-05,
      "loss": 2.0111,
      "step": 651
    },
    {
      "epoch": 0.8253164556962025,
      "grad_norm": 1.3981603384017944,
      "learning_rate": 5e-05,
      "loss": 2.026,
      "step": 652
    },
    {
      "epoch": 0.8265822784810126,
      "grad_norm": 1.4256330728530884,
      "learning_rate": 5e-05,
      "loss": 2.0518,
      "step": 653
    },
    {
      "epoch": 0.8278481012658228,
      "grad_norm": 1.3749979734420776,
      "learning_rate": 5e-05,
      "loss": 1.957,
      "step": 654
    },
    {
      "epoch": 0.8291139240506329,
      "grad_norm": 1.4460393190383911,
      "learning_rate": 5e-05,
      "loss": 2.0823,
      "step": 655
    },
    {
      "epoch": 0.830379746835443,
      "grad_norm": 1.3596848249435425,
      "learning_rate": 5e-05,
      "loss": 1.941,
      "step": 656
    },
    {
      "epoch": 0.8316455696202532,
      "grad_norm": 1.3742103576660156,
      "learning_rate": 5e-05,
      "loss": 2.0025,
      "step": 657
    },
    {
      "epoch": 0.8329113924050633,
      "grad_norm": 1.3885631561279297,
      "learning_rate": 5e-05,
      "loss": 2.0097,
      "step": 658
    },
    {
      "epoch": 0.8341772151898734,
      "grad_norm": 1.379779577255249,
      "learning_rate": 5e-05,
      "loss": 2.0341,
      "step": 659
    },
    {
      "epoch": 0.8354430379746836,
      "grad_norm": 1.4125725030899048,
      "learning_rate": 5e-05,
      "loss": 2.0613,
      "step": 660
    },
    {
      "epoch": 0.8367088607594937,
      "grad_norm": 1.4591625928878784,
      "learning_rate": 5e-05,
      "loss": 2.1179,
      "step": 661
    },
    {
      "epoch": 0.8379746835443038,
      "grad_norm": 1.3775982856750488,
      "learning_rate": 5e-05,
      "loss": 1.9588,
      "step": 662
    },
    {
      "epoch": 0.839240506329114,
      "grad_norm": 1.446458101272583,
      "learning_rate": 5e-05,
      "loss": 2.067,
      "step": 663
    },
    {
      "epoch": 0.8405063291139241,
      "grad_norm": 1.4079056978225708,
      "learning_rate": 5e-05,
      "loss": 2.0067,
      "step": 664
    },
    {
      "epoch": 0.8417721518987342,
      "grad_norm": 1.3853864669799805,
      "learning_rate": 5e-05,
      "loss": 2.0571,
      "step": 665
    },
    {
      "epoch": 0.8430379746835444,
      "grad_norm": 1.4429352283477783,
      "learning_rate": 5e-05,
      "loss": 2.126,
      "step": 666
    },
    {
      "epoch": 0.8443037974683544,
      "grad_norm": 1.3977644443511963,
      "learning_rate": 5e-05,
      "loss": 1.9953,
      "step": 667
    },
    {
      "epoch": 0.8455696202531645,
      "grad_norm": 1.3904650211334229,
      "learning_rate": 5e-05,
      "loss": 2.0138,
      "step": 668
    },
    {
      "epoch": 0.8468354430379746,
      "grad_norm": 1.40793776512146,
      "learning_rate": 5e-05,
      "loss": 2.0202,
      "step": 669
    },
    {
      "epoch": 0.8481012658227848,
      "grad_norm": 1.4074376821517944,
      "learning_rate": 5e-05,
      "loss": 2.0206,
      "step": 670
    },
    {
      "epoch": 0.8493670886075949,
      "grad_norm": 1.400801658630371,
      "learning_rate": 5e-05,
      "loss": 2.0683,
      "step": 671
    },
    {
      "epoch": 0.850632911392405,
      "grad_norm": 1.4007078409194946,
      "learning_rate": 5e-05,
      "loss": 1.9848,
      "step": 672
    },
    {
      "epoch": 0.8518987341772152,
      "grad_norm": 1.4212863445281982,
      "learning_rate": 5e-05,
      "loss": 2.0022,
      "step": 673
    },
    {
      "epoch": 0.8531645569620253,
      "grad_norm": 1.4463642835617065,
      "learning_rate": 5e-05,
      "loss": 2.0521,
      "step": 674
    },
    {
      "epoch": 0.8544303797468354,
      "grad_norm": 1.4000072479248047,
      "learning_rate": 5e-05,
      "loss": 2.0116,
      "step": 675
    },
    {
      "epoch": 0.8556962025316456,
      "grad_norm": 1.4220116138458252,
      "learning_rate": 5e-05,
      "loss": 2.0122,
      "step": 676
    },
    {
      "epoch": 0.8569620253164557,
      "grad_norm": 1.384027361869812,
      "learning_rate": 5e-05,
      "loss": 1.9994,
      "step": 677
    },
    {
      "epoch": 0.8582278481012658,
      "grad_norm": 1.3578970432281494,
      "learning_rate": 5e-05,
      "loss": 1.9888,
      "step": 678
    },
    {
      "epoch": 0.859493670886076,
      "grad_norm": 1.4368003606796265,
      "learning_rate": 5e-05,
      "loss": 1.9968,
      "step": 679
    },
    {
      "epoch": 0.8607594936708861,
      "grad_norm": 1.3953121900558472,
      "learning_rate": 5e-05,
      "loss": 1.9951,
      "step": 680
    },
    {
      "epoch": 0.8620253164556962,
      "grad_norm": 1.399148941040039,
      "learning_rate": 5e-05,
      "loss": 1.9389,
      "step": 681
    },
    {
      "epoch": 0.8632911392405064,
      "grad_norm": 1.3995835781097412,
      "learning_rate": 5e-05,
      "loss": 1.9686,
      "step": 682
    },
    {
      "epoch": 0.8645569620253165,
      "grad_norm": 1.3739891052246094,
      "learning_rate": 5e-05,
      "loss": 1.9828,
      "step": 683
    },
    {
      "epoch": 0.8658227848101265,
      "grad_norm": 1.3717154264450073,
      "learning_rate": 5e-05,
      "loss": 1.9066,
      "step": 684
    },
    {
      "epoch": 0.8670886075949367,
      "grad_norm": 1.388887882232666,
      "learning_rate": 5e-05,
      "loss": 1.9528,
      "step": 685
    },
    {
      "epoch": 0.8683544303797468,
      "grad_norm": 1.4192900657653809,
      "learning_rate": 5e-05,
      "loss": 2.0077,
      "step": 686
    },
    {
      "epoch": 0.8696202531645569,
      "grad_norm": 1.3880889415740967,
      "learning_rate": 5e-05,
      "loss": 2.0102,
      "step": 687
    },
    {
      "epoch": 0.8708860759493671,
      "grad_norm": 1.3984408378601074,
      "learning_rate": 5e-05,
      "loss": 1.9904,
      "step": 688
    },
    {
      "epoch": 0.8721518987341772,
      "grad_norm": 1.416938066482544,
      "learning_rate": 5e-05,
      "loss": 1.968,
      "step": 689
    },
    {
      "epoch": 0.8734177215189873,
      "grad_norm": 1.3780152797698975,
      "learning_rate": 5e-05,
      "loss": 1.9462,
      "step": 690
    },
    {
      "epoch": 0.8746835443037975,
      "grad_norm": 1.3580971956253052,
      "learning_rate": 5e-05,
      "loss": 1.9819,
      "step": 691
    },
    {
      "epoch": 0.8759493670886076,
      "grad_norm": 1.3984436988830566,
      "learning_rate": 5e-05,
      "loss": 1.9535,
      "step": 692
    },
    {
      "epoch": 0.8772151898734177,
      "grad_norm": 1.4603995084762573,
      "learning_rate": 5e-05,
      "loss": 1.9799,
      "step": 693
    },
    {
      "epoch": 0.8784810126582279,
      "grad_norm": 1.3752453327178955,
      "learning_rate": 5e-05,
      "loss": 1.9576,
      "step": 694
    },
    {
      "epoch": 0.879746835443038,
      "grad_norm": 1.3967702388763428,
      "learning_rate": 5e-05,
      "loss": 1.9595,
      "step": 695
    },
    {
      "epoch": 0.8810126582278481,
      "grad_norm": 1.4525856971740723,
      "learning_rate": 5e-05,
      "loss": 2.0684,
      "step": 696
    },
    {
      "epoch": 0.8822784810126583,
      "grad_norm": 1.431328296661377,
      "learning_rate": 5e-05,
      "loss": 1.9709,
      "step": 697
    },
    {
      "epoch": 0.8835443037974684,
      "grad_norm": 1.3979963064193726,
      "learning_rate": 5e-05,
      "loss": 1.9881,
      "step": 698
    },
    {
      "epoch": 0.8848101265822785,
      "grad_norm": 1.399227261543274,
      "learning_rate": 5e-05,
      "loss": 1.9952,
      "step": 699
    },
    {
      "epoch": 0.8860759493670886,
      "grad_norm": 1.3912615776062012,
      "learning_rate": 5e-05,
      "loss": 1.9426,
      "step": 700
    },
    {
      "epoch": 0.8873417721518987,
      "grad_norm": 1.363404631614685,
      "learning_rate": 5e-05,
      "loss": 1.9227,
      "step": 701
    },
    {
      "epoch": 0.8886075949367088,
      "grad_norm": 1.3930644989013672,
      "learning_rate": 5e-05,
      "loss": 1.9138,
      "step": 702
    },
    {
      "epoch": 0.889873417721519,
      "grad_norm": 1.3911948204040527,
      "learning_rate": 5e-05,
      "loss": 1.9756,
      "step": 703
    },
    {
      "epoch": 0.8911392405063291,
      "grad_norm": 1.3702863454818726,
      "learning_rate": 5e-05,
      "loss": 2.0117,
      "step": 704
    },
    {
      "epoch": 0.8924050632911392,
      "grad_norm": 1.3985451459884644,
      "learning_rate": 5e-05,
      "loss": 2.0291,
      "step": 705
    },
    {
      "epoch": 0.8936708860759494,
      "grad_norm": 1.35382878780365,
      "learning_rate": 5e-05,
      "loss": 1.9451,
      "step": 706
    },
    {
      "epoch": 0.8949367088607595,
      "grad_norm": 1.3896079063415527,
      "learning_rate": 5e-05,
      "loss": 1.9932,
      "step": 707
    },
    {
      "epoch": 0.8962025316455696,
      "grad_norm": 1.349932074546814,
      "learning_rate": 5e-05,
      "loss": 2.0043,
      "step": 708
    },
    {
      "epoch": 0.8974683544303798,
      "grad_norm": 1.3639005422592163,
      "learning_rate": 5e-05,
      "loss": 1.99,
      "step": 709
    },
    {
      "epoch": 0.8987341772151899,
      "grad_norm": 1.3945585489273071,
      "learning_rate": 5e-05,
      "loss": 1.8379,
      "step": 710
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.3736250400543213,
      "learning_rate": 5e-05,
      "loss": 1.9761,
      "step": 711
    },
    {
      "epoch": 0.9012658227848102,
      "grad_norm": 1.3969357013702393,
      "learning_rate": 5e-05,
      "loss": 1.9572,
      "step": 712
    },
    {
      "epoch": 0.9025316455696203,
      "grad_norm": 1.4000694751739502,
      "learning_rate": 5e-05,
      "loss": 1.9649,
      "step": 713
    },
    {
      "epoch": 0.9037974683544304,
      "grad_norm": 1.3906464576721191,
      "learning_rate": 5e-05,
      "loss": 1.8978,
      "step": 714
    },
    {
      "epoch": 0.9050632911392406,
      "grad_norm": 1.3984487056732178,
      "learning_rate": 5e-05,
      "loss": 1.9927,
      "step": 715
    },
    {
      "epoch": 0.9063291139240506,
      "grad_norm": 1.37578284740448,
      "learning_rate": 5e-05,
      "loss": 2.0121,
      "step": 716
    },
    {
      "epoch": 0.9075949367088607,
      "grad_norm": 1.389136552810669,
      "learning_rate": 5e-05,
      "loss": 1.9643,
      "step": 717
    },
    {
      "epoch": 0.9088607594936708,
      "grad_norm": 1.3589080572128296,
      "learning_rate": 5e-05,
      "loss": 1.8897,
      "step": 718
    },
    {
      "epoch": 0.910126582278481,
      "grad_norm": 1.3848661184310913,
      "learning_rate": 5e-05,
      "loss": 1.933,
      "step": 719
    },
    {
      "epoch": 0.9113924050632911,
      "grad_norm": 1.3699736595153809,
      "learning_rate": 5e-05,
      "loss": 1.9359,
      "step": 720
    },
    {
      "epoch": 0.9126582278481012,
      "grad_norm": 1.4057741165161133,
      "learning_rate": 5e-05,
      "loss": 1.9845,
      "step": 721
    },
    {
      "epoch": 0.9139240506329114,
      "grad_norm": 1.3390082120895386,
      "learning_rate": 5e-05,
      "loss": 1.9348,
      "step": 722
    },
    {
      "epoch": 0.9151898734177215,
      "grad_norm": 1.4192547798156738,
      "learning_rate": 5e-05,
      "loss": 1.9818,
      "step": 723
    },
    {
      "epoch": 0.9164556962025316,
      "grad_norm": 1.3676297664642334,
      "learning_rate": 5e-05,
      "loss": 1.8869,
      "step": 724
    },
    {
      "epoch": 0.9177215189873418,
      "grad_norm": 1.3496284484863281,
      "learning_rate": 5e-05,
      "loss": 1.8849,
      "step": 725
    },
    {
      "epoch": 0.9189873417721519,
      "grad_norm": 1.3682215213775635,
      "learning_rate": 5e-05,
      "loss": 1.9601,
      "step": 726
    },
    {
      "epoch": 0.920253164556962,
      "grad_norm": 1.347201943397522,
      "learning_rate": 5e-05,
      "loss": 1.9182,
      "step": 727
    },
    {
      "epoch": 0.9215189873417722,
      "grad_norm": 1.3979843854904175,
      "learning_rate": 5e-05,
      "loss": 1.9588,
      "step": 728
    },
    {
      "epoch": 0.9227848101265823,
      "grad_norm": 1.3801363706588745,
      "learning_rate": 5e-05,
      "loss": 1.9187,
      "step": 729
    },
    {
      "epoch": 0.9240506329113924,
      "grad_norm": 1.3903119564056396,
      "learning_rate": 5e-05,
      "loss": 1.9633,
      "step": 730
    },
    {
      "epoch": 0.9253164556962026,
      "grad_norm": 1.346247911453247,
      "learning_rate": 5e-05,
      "loss": 1.9656,
      "step": 731
    },
    {
      "epoch": 0.9265822784810127,
      "grad_norm": 1.353007435798645,
      "learning_rate": 5e-05,
      "loss": 1.916,
      "step": 732
    },
    {
      "epoch": 0.9278481012658227,
      "grad_norm": 1.3531241416931152,
      "learning_rate": 5e-05,
      "loss": 1.9746,
      "step": 733
    },
    {
      "epoch": 0.9291139240506329,
      "grad_norm": 1.3305562734603882,
      "learning_rate": 5e-05,
      "loss": 1.8944,
      "step": 734
    },
    {
      "epoch": 0.930379746835443,
      "grad_norm": 1.4045684337615967,
      "learning_rate": 5e-05,
      "loss": 2.0666,
      "step": 735
    },
    {
      "epoch": 0.9316455696202531,
      "grad_norm": 1.3560543060302734,
      "learning_rate": 5e-05,
      "loss": 1.95,
      "step": 736
    },
    {
      "epoch": 0.9329113924050633,
      "grad_norm": 1.3465436697006226,
      "learning_rate": 5e-05,
      "loss": 1.9217,
      "step": 737
    },
    {
      "epoch": 0.9341772151898734,
      "grad_norm": 1.3680893182754517,
      "learning_rate": 5e-05,
      "loss": 1.8843,
      "step": 738
    },
    {
      "epoch": 0.9354430379746835,
      "grad_norm": 1.406158208847046,
      "learning_rate": 5e-05,
      "loss": 1.9311,
      "step": 739
    },
    {
      "epoch": 0.9367088607594937,
      "grad_norm": 1.352328896522522,
      "learning_rate": 5e-05,
      "loss": 1.9122,
      "step": 740
    },
    {
      "epoch": 0.9379746835443038,
      "grad_norm": 1.362831711769104,
      "learning_rate": 5e-05,
      "loss": 1.8223,
      "step": 741
    },
    {
      "epoch": 0.9392405063291139,
      "grad_norm": 1.3754910230636597,
      "learning_rate": 5e-05,
      "loss": 1.9837,
      "step": 742
    },
    {
      "epoch": 0.9405063291139241,
      "grad_norm": 1.407968521118164,
      "learning_rate": 5e-05,
      "loss": 1.8873,
      "step": 743
    },
    {
      "epoch": 0.9417721518987342,
      "grad_norm": 1.3542455434799194,
      "learning_rate": 5e-05,
      "loss": 1.981,
      "step": 744
    },
    {
      "epoch": 0.9430379746835443,
      "grad_norm": 1.3681926727294922,
      "learning_rate": 5e-05,
      "loss": 1.9835,
      "step": 745
    },
    {
      "epoch": 0.9443037974683545,
      "grad_norm": 1.3823208808898926,
      "learning_rate": 5e-05,
      "loss": 1.9014,
      "step": 746
    },
    {
      "epoch": 0.9455696202531646,
      "grad_norm": 1.3427788019180298,
      "learning_rate": 5e-05,
      "loss": 1.8984,
      "step": 747
    },
    {
      "epoch": 0.9468354430379747,
      "grad_norm": 1.411864161491394,
      "learning_rate": 5e-05,
      "loss": 1.9758,
      "step": 748
    },
    {
      "epoch": 0.9481012658227848,
      "grad_norm": 1.3776321411132812,
      "learning_rate": 5e-05,
      "loss": 1.9525,
      "step": 749
    },
    {
      "epoch": 0.9493670886075949,
      "grad_norm": 1.3889079093933105,
      "learning_rate": 5e-05,
      "loss": 1.9909,
      "step": 750
    },
    {
      "epoch": 0.950632911392405,
      "grad_norm": 1.3902716636657715,
      "learning_rate": 5e-05,
      "loss": 1.8998,
      "step": 751
    },
    {
      "epoch": 0.9518987341772152,
      "grad_norm": 1.3736566305160522,
      "learning_rate": 5e-05,
      "loss": 1.9699,
      "step": 752
    },
    {
      "epoch": 0.9531645569620253,
      "grad_norm": 1.3657716512680054,
      "learning_rate": 5e-05,
      "loss": 1.8345,
      "step": 753
    },
    {
      "epoch": 0.9544303797468354,
      "grad_norm": 1.3210294246673584,
      "learning_rate": 5e-05,
      "loss": 1.9074,
      "step": 754
    },
    {
      "epoch": 0.9556962025316456,
      "grad_norm": 1.3351669311523438,
      "learning_rate": 5e-05,
      "loss": 1.8546,
      "step": 755
    },
    {
      "epoch": 0.9569620253164557,
      "grad_norm": 1.3880374431610107,
      "learning_rate": 5e-05,
      "loss": 2.0258,
      "step": 756
    },
    {
      "epoch": 0.9582278481012658,
      "grad_norm": 1.3819541931152344,
      "learning_rate": 5e-05,
      "loss": 1.902,
      "step": 757
    },
    {
      "epoch": 0.959493670886076,
      "grad_norm": 1.381744146347046,
      "learning_rate": 5e-05,
      "loss": 1.9129,
      "step": 758
    },
    {
      "epoch": 0.9607594936708861,
      "grad_norm": 1.3572936058044434,
      "learning_rate": 5e-05,
      "loss": 1.8992,
      "step": 759
    },
    {
      "epoch": 0.9620253164556962,
      "grad_norm": 1.3847620487213135,
      "learning_rate": 5e-05,
      "loss": 1.9481,
      "step": 760
    },
    {
      "epoch": 0.9632911392405064,
      "grad_norm": 1.361031413078308,
      "learning_rate": 5e-05,
      "loss": 1.9447,
      "step": 761
    },
    {
      "epoch": 0.9645569620253165,
      "grad_norm": 1.3561675548553467,
      "learning_rate": 5e-05,
      "loss": 1.9546,
      "step": 762
    },
    {
      "epoch": 0.9658227848101266,
      "grad_norm": 1.4085969924926758,
      "learning_rate": 5e-05,
      "loss": 1.983,
      "step": 763
    },
    {
      "epoch": 0.9670886075949368,
      "grad_norm": 1.3846739530563354,
      "learning_rate": 5e-05,
      "loss": 1.9578,
      "step": 764
    },
    {
      "epoch": 0.9683544303797469,
      "grad_norm": 1.3789820671081543,
      "learning_rate": 5e-05,
      "loss": 1.8999,
      "step": 765
    },
    {
      "epoch": 0.9696202531645569,
      "grad_norm": 1.3848001956939697,
      "learning_rate": 5e-05,
      "loss": 1.8832,
      "step": 766
    },
    {
      "epoch": 0.970886075949367,
      "grad_norm": 1.4010387659072876,
      "learning_rate": 5e-05,
      "loss": 1.8674,
      "step": 767
    },
    {
      "epoch": 0.9721518987341772,
      "grad_norm": 1.3408548831939697,
      "learning_rate": 5e-05,
      "loss": 1.8397,
      "step": 768
    },
    {
      "epoch": 0.9734177215189873,
      "grad_norm": 1.3654009103775024,
      "learning_rate": 5e-05,
      "loss": 1.8754,
      "step": 769
    },
    {
      "epoch": 0.9746835443037974,
      "grad_norm": 1.374408483505249,
      "learning_rate": 5e-05,
      "loss": 1.9302,
      "step": 770
    },
    {
      "epoch": 0.9759493670886076,
      "grad_norm": 1.3843865394592285,
      "learning_rate": 5e-05,
      "loss": 1.986,
      "step": 771
    },
    {
      "epoch": 0.9772151898734177,
      "grad_norm": 1.3795918226242065,
      "learning_rate": 5e-05,
      "loss": 1.8924,
      "step": 772
    },
    {
      "epoch": 0.9784810126582278,
      "grad_norm": 1.418148398399353,
      "learning_rate": 5e-05,
      "loss": 1.9566,
      "step": 773
    },
    {
      "epoch": 0.979746835443038,
      "grad_norm": 1.3846826553344727,
      "learning_rate": 5e-05,
      "loss": 1.9509,
      "step": 774
    },
    {
      "epoch": 0.9810126582278481,
      "grad_norm": 1.3662642240524292,
      "learning_rate": 5e-05,
      "loss": 1.8485,
      "step": 775
    },
    {
      "epoch": 0.9822784810126582,
      "grad_norm": 1.3650239706039429,
      "learning_rate": 5e-05,
      "loss": 1.8746,
      "step": 776
    },
    {
      "epoch": 0.9835443037974684,
      "grad_norm": 1.3776949644088745,
      "learning_rate": 5e-05,
      "loss": 1.9317,
      "step": 777
    },
    {
      "epoch": 0.9848101265822785,
      "grad_norm": 1.3719788789749146,
      "learning_rate": 5e-05,
      "loss": 1.8898,
      "step": 778
    },
    {
      "epoch": 0.9860759493670886,
      "grad_norm": 1.3889260292053223,
      "learning_rate": 5e-05,
      "loss": 1.9732,
      "step": 779
    },
    {
      "epoch": 0.9873417721518988,
      "grad_norm": 1.337788462638855,
      "learning_rate": 5e-05,
      "loss": 1.8299,
      "step": 780
    },
    {
      "epoch": 0.9886075949367089,
      "grad_norm": 1.3476346731185913,
      "learning_rate": 5e-05,
      "loss": 1.8683,
      "step": 781
    },
    {
      "epoch": 0.9898734177215189,
      "grad_norm": 1.406165361404419,
      "learning_rate": 5e-05,
      "loss": 1.9784,
      "step": 782
    },
    {
      "epoch": 0.9911392405063291,
      "grad_norm": 1.4218618869781494,
      "learning_rate": 5e-05,
      "loss": 1.9872,
      "step": 783
    },
    {
      "epoch": 0.9924050632911392,
      "grad_norm": 1.3653724193572998,
      "learning_rate": 5e-05,
      "loss": 1.872,
      "step": 784
    },
    {
      "epoch": 0.9936708860759493,
      "grad_norm": 1.3774900436401367,
      "learning_rate": 5e-05,
      "loss": 1.9285,
      "step": 785
    },
    {
      "epoch": 0.9949367088607595,
      "grad_norm": 1.340975046157837,
      "learning_rate": 5e-05,
      "loss": 1.8693,
      "step": 786
    },
    {
      "epoch": 0.9962025316455696,
      "grad_norm": 1.3755275011062622,
      "learning_rate": 5e-05,
      "loss": 1.9021,
      "step": 787
    },
    {
      "epoch": 0.9974683544303797,
      "grad_norm": 1.3241281509399414,
      "learning_rate": 5e-05,
      "loss": 1.8501,
      "step": 788
    },
    {
      "epoch": 0.9987341772151899,
      "grad_norm": 1.421328067779541,
      "learning_rate": 5e-05,
      "loss": 2.0346,
      "step": 789
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.4257786273956299,
      "learning_rate": 5e-05,
      "loss": 1.8281,
      "step": 790
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.353573799133301,
      "eval_runtime": 166.0639,
      "eval_samples_per_second": 541.237,
      "eval_steps_per_second": 4.233,
      "step": 790
    },
    {
      "epoch": 1.0012658227848101,
      "grad_norm": 1.3585340976715088,
      "learning_rate": 5e-05,
      "loss": 1.8703,
      "step": 791
    },
    {
      "epoch": 1.0025316455696203,
      "grad_norm": 1.323931097984314,
      "learning_rate": 5e-05,
      "loss": 1.7472,
      "step": 792
    },
    {
      "epoch": 1.0037974683544304,
      "grad_norm": 1.3486794233322144,
      "learning_rate": 5e-05,
      "loss": 1.8367,
      "step": 793
    },
    {
      "epoch": 1.0050632911392405,
      "grad_norm": 1.370668649673462,
      "learning_rate": 5e-05,
      "loss": 1.8707,
      "step": 794
    },
    {
      "epoch": 1.0063291139240507,
      "grad_norm": 1.3249945640563965,
      "learning_rate": 5e-05,
      "loss": 1.8604,
      "step": 795
    },
    {
      "epoch": 1.0075949367088608,
      "grad_norm": 1.364938497543335,
      "learning_rate": 5e-05,
      "loss": 1.7765,
      "step": 796
    },
    {
      "epoch": 1.008860759493671,
      "grad_norm": 1.3467061519622803,
      "learning_rate": 5e-05,
      "loss": 1.8959,
      "step": 797
    },
    {
      "epoch": 1.010126582278481,
      "grad_norm": 1.324294924736023,
      "learning_rate": 5e-05,
      "loss": 1.8769,
      "step": 798
    },
    {
      "epoch": 1.0113924050632912,
      "grad_norm": 1.3607065677642822,
      "learning_rate": 5e-05,
      "loss": 1.7805,
      "step": 799
    },
    {
      "epoch": 1.0126582278481013,
      "grad_norm": 1.3412278890609741,
      "learning_rate": 5e-05,
      "loss": 1.8445,
      "step": 800
    },
    {
      "epoch": 1.0139240506329115,
      "grad_norm": 1.3032044172286987,
      "learning_rate": 5e-05,
      "loss": 1.8382,
      "step": 801
    },
    {
      "epoch": 1.0151898734177216,
      "grad_norm": 1.3422952890396118,
      "learning_rate": 5e-05,
      "loss": 1.8676,
      "step": 802
    },
    {
      "epoch": 1.0164556962025317,
      "grad_norm": 1.3671828508377075,
      "learning_rate": 5e-05,
      "loss": 1.8169,
      "step": 803
    },
    {
      "epoch": 1.0177215189873419,
      "grad_norm": 1.3567639589309692,
      "learning_rate": 5e-05,
      "loss": 1.8642,
      "step": 804
    },
    {
      "epoch": 1.018987341772152,
      "grad_norm": 1.3448978662490845,
      "learning_rate": 5e-05,
      "loss": 1.8419,
      "step": 805
    },
    {
      "epoch": 1.0202531645569621,
      "grad_norm": 1.3606154918670654,
      "learning_rate": 5e-05,
      "loss": 1.9559,
      "step": 806
    },
    {
      "epoch": 1.021518987341772,
      "grad_norm": 1.3460943698883057,
      "learning_rate": 5e-05,
      "loss": 1.806,
      "step": 807
    },
    {
      "epoch": 1.0227848101265822,
      "grad_norm": 1.3682119846343994,
      "learning_rate": 5e-05,
      "loss": 1.9108,
      "step": 808
    },
    {
      "epoch": 1.0240506329113923,
      "grad_norm": 1.3865454196929932,
      "learning_rate": 5e-05,
      "loss": 1.8606,
      "step": 809
    },
    {
      "epoch": 1.0253164556962024,
      "grad_norm": 1.3633406162261963,
      "learning_rate": 5e-05,
      "loss": 1.8071,
      "step": 810
    },
    {
      "epoch": 1.0265822784810126,
      "grad_norm": 1.338912844657898,
      "learning_rate": 5e-05,
      "loss": 1.8023,
      "step": 811
    },
    {
      "epoch": 1.0278481012658227,
      "grad_norm": 1.3683366775512695,
      "learning_rate": 5e-05,
      "loss": 1.8276,
      "step": 812
    },
    {
      "epoch": 1.0291139240506328,
      "grad_norm": 1.326361060142517,
      "learning_rate": 5e-05,
      "loss": 1.7813,
      "step": 813
    },
    {
      "epoch": 1.030379746835443,
      "grad_norm": 1.318742036819458,
      "learning_rate": 5e-05,
      "loss": 1.7405,
      "step": 814
    },
    {
      "epoch": 1.0316455696202531,
      "grad_norm": 1.3319849967956543,
      "learning_rate": 5e-05,
      "loss": 1.7446,
      "step": 815
    },
    {
      "epoch": 1.0329113924050632,
      "grad_norm": 1.3884246349334717,
      "learning_rate": 5e-05,
      "loss": 1.798,
      "step": 816
    },
    {
      "epoch": 1.0341772151898734,
      "grad_norm": 1.3593828678131104,
      "learning_rate": 5e-05,
      "loss": 1.8167,
      "step": 817
    },
    {
      "epoch": 1.0354430379746835,
      "grad_norm": 1.3241515159606934,
      "learning_rate": 5e-05,
      "loss": 1.8588,
      "step": 818
    },
    {
      "epoch": 1.0367088607594936,
      "grad_norm": 1.3258546590805054,
      "learning_rate": 5e-05,
      "loss": 1.7465,
      "step": 819
    },
    {
      "epoch": 1.0379746835443038,
      "grad_norm": 1.3282949924468994,
      "learning_rate": 5e-05,
      "loss": 1.7469,
      "step": 820
    },
    {
      "epoch": 1.039240506329114,
      "grad_norm": 1.368851900100708,
      "learning_rate": 5e-05,
      "loss": 1.8774,
      "step": 821
    },
    {
      "epoch": 1.040506329113924,
      "grad_norm": 1.3756026029586792,
      "learning_rate": 5e-05,
      "loss": 1.8901,
      "step": 822
    },
    {
      "epoch": 1.0417721518987342,
      "grad_norm": 1.3333789110183716,
      "learning_rate": 5e-05,
      "loss": 1.7814,
      "step": 823
    },
    {
      "epoch": 1.0430379746835443,
      "grad_norm": 1.3980647325515747,
      "learning_rate": 5e-05,
      "loss": 1.8787,
      "step": 824
    },
    {
      "epoch": 1.0443037974683544,
      "grad_norm": 1.286427617073059,
      "learning_rate": 5e-05,
      "loss": 1.7839,
      "step": 825
    },
    {
      "epoch": 1.0455696202531646,
      "grad_norm": 1.4024238586425781,
      "learning_rate": 5e-05,
      "loss": 1.8748,
      "step": 826
    },
    {
      "epoch": 1.0468354430379747,
      "grad_norm": 1.406389832496643,
      "learning_rate": 5e-05,
      "loss": 1.8888,
      "step": 827
    },
    {
      "epoch": 1.0481012658227848,
      "grad_norm": 1.3206689357757568,
      "learning_rate": 5e-05,
      "loss": 1.7887,
      "step": 828
    },
    {
      "epoch": 1.049367088607595,
      "grad_norm": 1.3665951490402222,
      "learning_rate": 5e-05,
      "loss": 1.8706,
      "step": 829
    },
    {
      "epoch": 1.0506329113924051,
      "grad_norm": 1.3271119594573975,
      "learning_rate": 5e-05,
      "loss": 1.7828,
      "step": 830
    },
    {
      "epoch": 1.0518987341772152,
      "grad_norm": 1.3911802768707275,
      "learning_rate": 5e-05,
      "loss": 1.8081,
      "step": 831
    },
    {
      "epoch": 1.0531645569620254,
      "grad_norm": 1.3633217811584473,
      "learning_rate": 5e-05,
      "loss": 1.8434,
      "step": 832
    },
    {
      "epoch": 1.0544303797468355,
      "grad_norm": 1.4042764902114868,
      "learning_rate": 5e-05,
      "loss": 1.8651,
      "step": 833
    },
    {
      "epoch": 1.0556962025316456,
      "grad_norm": 1.310699701309204,
      "learning_rate": 5e-05,
      "loss": 1.7456,
      "step": 834
    },
    {
      "epoch": 1.0569620253164558,
      "grad_norm": 1.3589823246002197,
      "learning_rate": 5e-05,
      "loss": 1.8256,
      "step": 835
    },
    {
      "epoch": 1.058227848101266,
      "grad_norm": 1.348281741142273,
      "learning_rate": 5e-05,
      "loss": 1.7861,
      "step": 836
    },
    {
      "epoch": 1.059493670886076,
      "grad_norm": 1.346493124961853,
      "learning_rate": 5e-05,
      "loss": 1.8493,
      "step": 837
    },
    {
      "epoch": 1.0607594936708862,
      "grad_norm": 1.3354054689407349,
      "learning_rate": 5e-05,
      "loss": 1.7726,
      "step": 838
    },
    {
      "epoch": 1.062025316455696,
      "grad_norm": 1.351939082145691,
      "learning_rate": 5e-05,
      "loss": 1.7185,
      "step": 839
    },
    {
      "epoch": 1.0632911392405062,
      "grad_norm": 1.3774861097335815,
      "learning_rate": 5e-05,
      "loss": 1.8166,
      "step": 840
    },
    {
      "epoch": 1.0645569620253164,
      "grad_norm": 1.3229471445083618,
      "learning_rate": 5e-05,
      "loss": 1.7746,
      "step": 841
    },
    {
      "epoch": 1.0658227848101265,
      "grad_norm": 1.3926873207092285,
      "learning_rate": 5e-05,
      "loss": 1.8489,
      "step": 842
    },
    {
      "epoch": 1.0670886075949366,
      "grad_norm": 1.3473527431488037,
      "learning_rate": 5e-05,
      "loss": 1.8149,
      "step": 843
    },
    {
      "epoch": 1.0683544303797468,
      "grad_norm": 1.3930073976516724,
      "learning_rate": 5e-05,
      "loss": 1.8069,
      "step": 844
    },
    {
      "epoch": 1.0696202531645569,
      "grad_norm": 1.3519577980041504,
      "learning_rate": 5e-05,
      "loss": 1.7779,
      "step": 845
    },
    {
      "epoch": 1.070886075949367,
      "grad_norm": 1.3295317888259888,
      "learning_rate": 5e-05,
      "loss": 1.8553,
      "step": 846
    },
    {
      "epoch": 1.0721518987341772,
      "grad_norm": 1.3466901779174805,
      "learning_rate": 5e-05,
      "loss": 1.7325,
      "step": 847
    },
    {
      "epoch": 1.0734177215189873,
      "grad_norm": 1.3393326997756958,
      "learning_rate": 5e-05,
      "loss": 1.7943,
      "step": 848
    },
    {
      "epoch": 1.0746835443037974,
      "grad_norm": 1.329519510269165,
      "learning_rate": 5e-05,
      "loss": 1.7446,
      "step": 849
    },
    {
      "epoch": 1.0759493670886076,
      "grad_norm": 1.3377108573913574,
      "learning_rate": 5e-05,
      "loss": 1.7878,
      "step": 850
    },
    {
      "epoch": 1.0772151898734177,
      "grad_norm": 1.3853501081466675,
      "learning_rate": 5e-05,
      "loss": 1.752,
      "step": 851
    },
    {
      "epoch": 1.0784810126582278,
      "grad_norm": 1.3354026079177856,
      "learning_rate": 5e-05,
      "loss": 1.7982,
      "step": 852
    },
    {
      "epoch": 1.079746835443038,
      "grad_norm": 1.3477768898010254,
      "learning_rate": 5e-05,
      "loss": 1.7878,
      "step": 853
    },
    {
      "epoch": 1.081012658227848,
      "grad_norm": 1.3499759435653687,
      "learning_rate": 5e-05,
      "loss": 1.7543,
      "step": 854
    },
    {
      "epoch": 1.0822784810126582,
      "grad_norm": 1.3616865873336792,
      "learning_rate": 5e-05,
      "loss": 1.7382,
      "step": 855
    },
    {
      "epoch": 1.0835443037974684,
      "grad_norm": 1.3968331813812256,
      "learning_rate": 5e-05,
      "loss": 1.8089,
      "step": 856
    },
    {
      "epoch": 1.0848101265822785,
      "grad_norm": 1.3577543497085571,
      "learning_rate": 5e-05,
      "loss": 1.8407,
      "step": 857
    },
    {
      "epoch": 1.0860759493670886,
      "grad_norm": 1.2909101247787476,
      "learning_rate": 5e-05,
      "loss": 1.7192,
      "step": 858
    },
    {
      "epoch": 1.0873417721518988,
      "grad_norm": 1.393060564994812,
      "learning_rate": 5e-05,
      "loss": 1.8367,
      "step": 859
    },
    {
      "epoch": 1.0886075949367089,
      "grad_norm": 1.3910101652145386,
      "learning_rate": 5e-05,
      "loss": 1.8214,
      "step": 860
    },
    {
      "epoch": 1.089873417721519,
      "grad_norm": 1.3591264486312866,
      "learning_rate": 5e-05,
      "loss": 1.7603,
      "step": 861
    },
    {
      "epoch": 1.0911392405063292,
      "grad_norm": 1.336848258972168,
      "learning_rate": 5e-05,
      "loss": 1.7885,
      "step": 862
    },
    {
      "epoch": 1.0924050632911393,
      "grad_norm": 1.3728079795837402,
      "learning_rate": 5e-05,
      "loss": 1.8237,
      "step": 863
    },
    {
      "epoch": 1.0936708860759494,
      "grad_norm": 1.3794866800308228,
      "learning_rate": 5e-05,
      "loss": 1.8286,
      "step": 864
    },
    {
      "epoch": 1.0949367088607596,
      "grad_norm": 1.3482234477996826,
      "learning_rate": 5e-05,
      "loss": 1.735,
      "step": 865
    },
    {
      "epoch": 1.0962025316455697,
      "grad_norm": 1.3688644170761108,
      "learning_rate": 5e-05,
      "loss": 1.7685,
      "step": 866
    },
    {
      "epoch": 1.0974683544303798,
      "grad_norm": 1.3720911741256714,
      "learning_rate": 5e-05,
      "loss": 1.8074,
      "step": 867
    },
    {
      "epoch": 1.09873417721519,
      "grad_norm": 1.3576034307479858,
      "learning_rate": 5e-05,
      "loss": 1.7524,
      "step": 868
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.3872473239898682,
      "learning_rate": 5e-05,
      "loss": 1.8068,
      "step": 869
    },
    {
      "epoch": 1.1012658227848102,
      "grad_norm": 1.373010516166687,
      "learning_rate": 5e-05,
      "loss": 1.8056,
      "step": 870
    },
    {
      "epoch": 1.1025316455696204,
      "grad_norm": 1.3547412157058716,
      "learning_rate": 5e-05,
      "loss": 1.7941,
      "step": 871
    },
    {
      "epoch": 1.1037974683544305,
      "grad_norm": 1.3614739179611206,
      "learning_rate": 5e-05,
      "loss": 1.7406,
      "step": 872
    },
    {
      "epoch": 1.1050632911392406,
      "grad_norm": 1.3489563465118408,
      "learning_rate": 5e-05,
      "loss": 1.8357,
      "step": 873
    },
    {
      "epoch": 1.1063291139240505,
      "grad_norm": 1.350712537765503,
      "learning_rate": 5e-05,
      "loss": 1.7631,
      "step": 874
    },
    {
      "epoch": 1.1075949367088607,
      "grad_norm": 1.4184753894805908,
      "learning_rate": 5e-05,
      "loss": 1.8275,
      "step": 875
    },
    {
      "epoch": 1.1088607594936708,
      "grad_norm": 1.3366668224334717,
      "learning_rate": 5e-05,
      "loss": 1.7219,
      "step": 876
    },
    {
      "epoch": 1.110126582278481,
      "grad_norm": 1.3820492029190063,
      "learning_rate": 5e-05,
      "loss": 1.8346,
      "step": 877
    },
    {
      "epoch": 1.111392405063291,
      "grad_norm": 1.3640793561935425,
      "learning_rate": 5e-05,
      "loss": 1.7196,
      "step": 878
    },
    {
      "epoch": 1.1126582278481012,
      "grad_norm": 1.352753758430481,
      "learning_rate": 5e-05,
      "loss": 1.7355,
      "step": 879
    },
    {
      "epoch": 1.1139240506329113,
      "grad_norm": 1.3521840572357178,
      "learning_rate": 5e-05,
      "loss": 1.8425,
      "step": 880
    },
    {
      "epoch": 1.1151898734177215,
      "grad_norm": 1.3461233377456665,
      "learning_rate": 5e-05,
      "loss": 1.8035,
      "step": 881
    },
    {
      "epoch": 1.1164556962025316,
      "grad_norm": 1.367586374282837,
      "learning_rate": 5e-05,
      "loss": 1.7869,
      "step": 882
    },
    {
      "epoch": 1.1177215189873417,
      "grad_norm": 1.3820875883102417,
      "learning_rate": 5e-05,
      "loss": 1.8318,
      "step": 883
    },
    {
      "epoch": 1.1189873417721519,
      "grad_norm": 1.3473467826843262,
      "learning_rate": 5e-05,
      "loss": 1.7684,
      "step": 884
    },
    {
      "epoch": 1.120253164556962,
      "grad_norm": 1.3740527629852295,
      "learning_rate": 5e-05,
      "loss": 1.8506,
      "step": 885
    },
    {
      "epoch": 1.1215189873417721,
      "grad_norm": 1.3643357753753662,
      "learning_rate": 5e-05,
      "loss": 1.7463,
      "step": 886
    },
    {
      "epoch": 1.1227848101265823,
      "grad_norm": 1.337195873260498,
      "learning_rate": 5e-05,
      "loss": 1.7162,
      "step": 887
    },
    {
      "epoch": 1.1240506329113924,
      "grad_norm": 1.3312753438949585,
      "learning_rate": 5e-05,
      "loss": 1.7622,
      "step": 888
    },
    {
      "epoch": 1.1253164556962025,
      "grad_norm": 1.3481066226959229,
      "learning_rate": 5e-05,
      "loss": 1.7877,
      "step": 889
    },
    {
      "epoch": 1.1265822784810127,
      "grad_norm": 1.3309922218322754,
      "learning_rate": 5e-05,
      "loss": 1.7289,
      "step": 890
    },
    {
      "epoch": 1.1278481012658228,
      "grad_norm": 1.3618919849395752,
      "learning_rate": 5e-05,
      "loss": 1.8693,
      "step": 891
    },
    {
      "epoch": 1.129113924050633,
      "grad_norm": 1.3355263471603394,
      "learning_rate": 5e-05,
      "loss": 1.7453,
      "step": 892
    },
    {
      "epoch": 1.130379746835443,
      "grad_norm": 1.340467929840088,
      "learning_rate": 5e-05,
      "loss": 1.8493,
      "step": 893
    },
    {
      "epoch": 1.1316455696202532,
      "grad_norm": 1.3206112384796143,
      "learning_rate": 5e-05,
      "loss": 1.7133,
      "step": 894
    },
    {
      "epoch": 1.1329113924050633,
      "grad_norm": 1.3302932977676392,
      "learning_rate": 5e-05,
      "loss": 1.7403,
      "step": 895
    },
    {
      "epoch": 1.1341772151898735,
      "grad_norm": 1.35009765625,
      "learning_rate": 5e-05,
      "loss": 1.8514,
      "step": 896
    },
    {
      "epoch": 1.1354430379746836,
      "grad_norm": 1.349134922027588,
      "learning_rate": 5e-05,
      "loss": 1.7914,
      "step": 897
    },
    {
      "epoch": 1.1367088607594937,
      "grad_norm": 1.3605844974517822,
      "learning_rate": 5e-05,
      "loss": 1.7716,
      "step": 898
    },
    {
      "epoch": 1.1379746835443039,
      "grad_norm": 1.3975955247879028,
      "learning_rate": 5e-05,
      "loss": 1.777,
      "step": 899
    },
    {
      "epoch": 1.139240506329114,
      "grad_norm": 1.3325574398040771,
      "learning_rate": 5e-05,
      "loss": 1.7541,
      "step": 900
    },
    {
      "epoch": 1.1405063291139241,
      "grad_norm": 1.3354041576385498,
      "learning_rate": 5e-05,
      "loss": 1.7941,
      "step": 901
    },
    {
      "epoch": 1.1417721518987343,
      "grad_norm": 1.2836072444915771,
      "learning_rate": 5e-05,
      "loss": 1.7421,
      "step": 902
    },
    {
      "epoch": 1.1430379746835444,
      "grad_norm": 1.3948254585266113,
      "learning_rate": 5e-05,
      "loss": 1.8015,
      "step": 903
    },
    {
      "epoch": 1.1443037974683543,
      "grad_norm": 1.330004334449768,
      "learning_rate": 5e-05,
      "loss": 1.746,
      "step": 904
    },
    {
      "epoch": 1.1455696202531644,
      "grad_norm": 1.381177306175232,
      "learning_rate": 5e-05,
      "loss": 1.7904,
      "step": 905
    },
    {
      "epoch": 1.1468354430379746,
      "grad_norm": 1.39674711227417,
      "learning_rate": 5e-05,
      "loss": 1.7332,
      "step": 906
    },
    {
      "epoch": 1.1481012658227847,
      "grad_norm": 1.351178765296936,
      "learning_rate": 5e-05,
      "loss": 1.7539,
      "step": 907
    },
    {
      "epoch": 1.1493670886075948,
      "grad_norm": 1.319545865058899,
      "learning_rate": 5e-05,
      "loss": 1.786,
      "step": 908
    },
    {
      "epoch": 1.150632911392405,
      "grad_norm": 1.3367058038711548,
      "learning_rate": 5e-05,
      "loss": 1.7916,
      "step": 909
    },
    {
      "epoch": 1.1518987341772151,
      "grad_norm": 1.3477623462677002,
      "learning_rate": 5e-05,
      "loss": 1.7501,
      "step": 910
    },
    {
      "epoch": 1.1531645569620252,
      "grad_norm": 1.36769700050354,
      "learning_rate": 5e-05,
      "loss": 1.8481,
      "step": 911
    },
    {
      "epoch": 1.1544303797468354,
      "grad_norm": 1.3540802001953125,
      "learning_rate": 5e-05,
      "loss": 1.8037,
      "step": 912
    },
    {
      "epoch": 1.1556962025316455,
      "grad_norm": 1.3742395639419556,
      "learning_rate": 5e-05,
      "loss": 1.7639,
      "step": 913
    },
    {
      "epoch": 1.1569620253164556,
      "grad_norm": 1.364018440246582,
      "learning_rate": 5e-05,
      "loss": 1.756,
      "step": 914
    },
    {
      "epoch": 1.1582278481012658,
      "grad_norm": 1.3986883163452148,
      "learning_rate": 5e-05,
      "loss": 1.7603,
      "step": 915
    },
    {
      "epoch": 1.159493670886076,
      "grad_norm": 1.328176736831665,
      "learning_rate": 5e-05,
      "loss": 1.7513,
      "step": 916
    },
    {
      "epoch": 1.160759493670886,
      "grad_norm": 1.3602831363677979,
      "learning_rate": 5e-05,
      "loss": 1.7472,
      "step": 917
    },
    {
      "epoch": 1.1620253164556962,
      "grad_norm": 1.3242919445037842,
      "learning_rate": 5e-05,
      "loss": 1.8213,
      "step": 918
    },
    {
      "epoch": 1.1632911392405063,
      "grad_norm": 1.3355038166046143,
      "learning_rate": 5e-05,
      "loss": 1.7437,
      "step": 919
    },
    {
      "epoch": 1.1645569620253164,
      "grad_norm": 1.3434330224990845,
      "learning_rate": 5e-05,
      "loss": 1.7286,
      "step": 920
    },
    {
      "epoch": 1.1658227848101266,
      "grad_norm": 1.306283712387085,
      "learning_rate": 5e-05,
      "loss": 1.743,
      "step": 921
    },
    {
      "epoch": 1.1670886075949367,
      "grad_norm": 1.3561509847640991,
      "learning_rate": 5e-05,
      "loss": 1.8167,
      "step": 922
    },
    {
      "epoch": 1.1683544303797468,
      "grad_norm": 1.3653388023376465,
      "learning_rate": 5e-05,
      "loss": 1.7127,
      "step": 923
    },
    {
      "epoch": 1.169620253164557,
      "grad_norm": 1.313485860824585,
      "learning_rate": 5e-05,
      "loss": 1.6924,
      "step": 924
    },
    {
      "epoch": 1.1708860759493671,
      "grad_norm": 1.3256689310073853,
      "learning_rate": 5e-05,
      "loss": 1.7185,
      "step": 925
    },
    {
      "epoch": 1.1721518987341772,
      "grad_norm": 1.3362836837768555,
      "learning_rate": 5e-05,
      "loss": 1.7437,
      "step": 926
    },
    {
      "epoch": 1.1734177215189874,
      "grad_norm": 1.3875278234481812,
      "learning_rate": 5e-05,
      "loss": 1.823,
      "step": 927
    },
    {
      "epoch": 1.1746835443037975,
      "grad_norm": 1.3256195783615112,
      "learning_rate": 5e-05,
      "loss": 1.7754,
      "step": 928
    },
    {
      "epoch": 1.1759493670886076,
      "grad_norm": 1.3911945819854736,
      "learning_rate": 5e-05,
      "loss": 1.7244,
      "step": 929
    },
    {
      "epoch": 1.1772151898734178,
      "grad_norm": 1.36954927444458,
      "learning_rate": 5e-05,
      "loss": 1.7876,
      "step": 930
    },
    {
      "epoch": 1.178481012658228,
      "grad_norm": 1.3570754528045654,
      "learning_rate": 5e-05,
      "loss": 1.7147,
      "step": 931
    },
    {
      "epoch": 1.179746835443038,
      "grad_norm": 1.3829079866409302,
      "learning_rate": 5e-05,
      "loss": 1.7713,
      "step": 932
    },
    {
      "epoch": 1.1810126582278482,
      "grad_norm": 1.3109383583068848,
      "learning_rate": 5e-05,
      "loss": 1.7105,
      "step": 933
    },
    {
      "epoch": 1.1822784810126583,
      "grad_norm": 1.3725587129592896,
      "learning_rate": 5e-05,
      "loss": 1.8613,
      "step": 934
    },
    {
      "epoch": 1.1835443037974684,
      "grad_norm": 1.332308292388916,
      "learning_rate": 5e-05,
      "loss": 1.7106,
      "step": 935
    },
    {
      "epoch": 1.1848101265822786,
      "grad_norm": 1.3088361024856567,
      "learning_rate": 5e-05,
      "loss": 1.7343,
      "step": 936
    },
    {
      "epoch": 1.1860759493670887,
      "grad_norm": 1.317410945892334,
      "learning_rate": 5e-05,
      "loss": 1.727,
      "step": 937
    },
    {
      "epoch": 1.1873417721518988,
      "grad_norm": 1.3489629030227661,
      "learning_rate": 5e-05,
      "loss": 1.7617,
      "step": 938
    },
    {
      "epoch": 1.188607594936709,
      "grad_norm": 1.311461329460144,
      "learning_rate": 5e-05,
      "loss": 1.754,
      "step": 939
    },
    {
      "epoch": 1.189873417721519,
      "grad_norm": 1.3610788583755493,
      "learning_rate": 5e-05,
      "loss": 1.7789,
      "step": 940
    },
    {
      "epoch": 1.191139240506329,
      "grad_norm": 1.3942360877990723,
      "learning_rate": 5e-05,
      "loss": 1.7662,
      "step": 941
    },
    {
      "epoch": 1.1924050632911392,
      "grad_norm": 1.3504388332366943,
      "learning_rate": 5e-05,
      "loss": 1.757,
      "step": 942
    },
    {
      "epoch": 1.1936708860759493,
      "grad_norm": 1.3747504949569702,
      "learning_rate": 5e-05,
      "loss": 1.8466,
      "step": 943
    },
    {
      "epoch": 1.1949367088607594,
      "grad_norm": 1.3459491729736328,
      "learning_rate": 5e-05,
      "loss": 1.7405,
      "step": 944
    },
    {
      "epoch": 1.1962025316455696,
      "grad_norm": 1.3170664310455322,
      "learning_rate": 5e-05,
      "loss": 1.7884,
      "step": 945
    },
    {
      "epoch": 1.1974683544303797,
      "grad_norm": 1.349047064781189,
      "learning_rate": 5e-05,
      "loss": 1.7871,
      "step": 946
    },
    {
      "epoch": 1.1987341772151898,
      "grad_norm": 1.3618327379226685,
      "learning_rate": 5e-05,
      "loss": 1.7849,
      "step": 947
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.3768792152404785,
      "learning_rate": 5e-05,
      "loss": 1.8335,
      "step": 948
    },
    {
      "epoch": 1.20126582278481,
      "grad_norm": 1.3359951972961426,
      "learning_rate": 5e-05,
      "loss": 1.636,
      "step": 949
    },
    {
      "epoch": 1.2025316455696202,
      "grad_norm": 1.3568308353424072,
      "learning_rate": 5e-05,
      "loss": 1.769,
      "step": 950
    },
    {
      "epoch": 1.2037974683544304,
      "grad_norm": 1.378909945487976,
      "learning_rate": 5e-05,
      "loss": 1.7401,
      "step": 951
    },
    {
      "epoch": 1.2050632911392405,
      "grad_norm": 1.3820152282714844,
      "learning_rate": 5e-05,
      "loss": 1.7128,
      "step": 952
    },
    {
      "epoch": 1.2063291139240506,
      "grad_norm": 1.336944818496704,
      "learning_rate": 5e-05,
      "loss": 1.7401,
      "step": 953
    },
    {
      "epoch": 1.2075949367088608,
      "grad_norm": 1.3271386623382568,
      "learning_rate": 5e-05,
      "loss": 1.7609,
      "step": 954
    },
    {
      "epoch": 1.2088607594936709,
      "grad_norm": 1.3570449352264404,
      "learning_rate": 5e-05,
      "loss": 1.7688,
      "step": 955
    },
    {
      "epoch": 1.210126582278481,
      "grad_norm": 1.3308666944503784,
      "learning_rate": 5e-05,
      "loss": 1.7311,
      "step": 956
    },
    {
      "epoch": 1.2113924050632912,
      "grad_norm": 1.3488762378692627,
      "learning_rate": 5e-05,
      "loss": 1.7906,
      "step": 957
    },
    {
      "epoch": 1.2126582278481013,
      "grad_norm": 1.3893991708755493,
      "learning_rate": 5e-05,
      "loss": 1.7694,
      "step": 958
    },
    {
      "epoch": 1.2139240506329114,
      "grad_norm": 1.3264069557189941,
      "learning_rate": 5e-05,
      "loss": 1.7329,
      "step": 959
    },
    {
      "epoch": 1.2151898734177216,
      "grad_norm": 1.3903754949569702,
      "learning_rate": 5e-05,
      "loss": 1.6718,
      "step": 960
    },
    {
      "epoch": 1.2164556962025317,
      "grad_norm": 1.3516771793365479,
      "learning_rate": 5e-05,
      "loss": 1.7172,
      "step": 961
    },
    {
      "epoch": 1.2177215189873418,
      "grad_norm": 1.350702166557312,
      "learning_rate": 5e-05,
      "loss": 1.7576,
      "step": 962
    },
    {
      "epoch": 1.218987341772152,
      "grad_norm": 1.350111961364746,
      "learning_rate": 5e-05,
      "loss": 1.7705,
      "step": 963
    },
    {
      "epoch": 1.220253164556962,
      "grad_norm": 1.3484890460968018,
      "learning_rate": 5e-05,
      "loss": 1.7416,
      "step": 964
    },
    {
      "epoch": 1.2215189873417722,
      "grad_norm": 1.3508474826812744,
      "learning_rate": 5e-05,
      "loss": 1.7757,
      "step": 965
    },
    {
      "epoch": 1.2227848101265824,
      "grad_norm": 1.3048181533813477,
      "learning_rate": 5e-05,
      "loss": 1.7151,
      "step": 966
    },
    {
      "epoch": 1.2240506329113925,
      "grad_norm": 1.3198037147521973,
      "learning_rate": 5e-05,
      "loss": 1.7739,
      "step": 967
    },
    {
      "epoch": 1.2253164556962026,
      "grad_norm": 1.3089475631713867,
      "learning_rate": 5e-05,
      "loss": 1.7255,
      "step": 968
    },
    {
      "epoch": 1.2265822784810125,
      "grad_norm": 1.365545630455017,
      "learning_rate": 5e-05,
      "loss": 1.7523,
      "step": 969
    },
    {
      "epoch": 1.2278481012658227,
      "grad_norm": 1.350067138671875,
      "learning_rate": 5e-05,
      "loss": 1.8186,
      "step": 970
    },
    {
      "epoch": 1.2291139240506328,
      "grad_norm": 1.3241437673568726,
      "learning_rate": 5e-05,
      "loss": 1.6967,
      "step": 971
    },
    {
      "epoch": 1.230379746835443,
      "grad_norm": 1.3222696781158447,
      "learning_rate": 5e-05,
      "loss": 1.743,
      "step": 972
    },
    {
      "epoch": 1.231645569620253,
      "grad_norm": 1.362966537475586,
      "learning_rate": 5e-05,
      "loss": 1.7713,
      "step": 973
    },
    {
      "epoch": 1.2329113924050632,
      "grad_norm": 1.3470643758773804,
      "learning_rate": 5e-05,
      "loss": 1.7211,
      "step": 974
    },
    {
      "epoch": 1.2341772151898733,
      "grad_norm": 1.3228870630264282,
      "learning_rate": 5e-05,
      "loss": 1.6623,
      "step": 975
    },
    {
      "epoch": 1.2354430379746835,
      "grad_norm": 1.3183858394622803,
      "learning_rate": 5e-05,
      "loss": 1.6701,
      "step": 976
    },
    {
      "epoch": 1.2367088607594936,
      "grad_norm": 1.3434473276138306,
      "learning_rate": 5e-05,
      "loss": 1.7956,
      "step": 977
    },
    {
      "epoch": 1.2379746835443037,
      "grad_norm": 1.381131649017334,
      "learning_rate": 5e-05,
      "loss": 1.775,
      "step": 978
    },
    {
      "epoch": 1.2392405063291139,
      "grad_norm": 1.3665223121643066,
      "learning_rate": 5e-05,
      "loss": 1.775,
      "step": 979
    },
    {
      "epoch": 1.240506329113924,
      "grad_norm": 1.311287760734558,
      "learning_rate": 5e-05,
      "loss": 1.66,
      "step": 980
    },
    {
      "epoch": 1.2417721518987341,
      "grad_norm": 1.3424105644226074,
      "learning_rate": 5e-05,
      "loss": 1.7228,
      "step": 981
    },
    {
      "epoch": 1.2430379746835443,
      "grad_norm": 1.3615837097167969,
      "learning_rate": 5e-05,
      "loss": 1.7084,
      "step": 982
    },
    {
      "epoch": 1.2443037974683544,
      "grad_norm": 1.33347749710083,
      "learning_rate": 5e-05,
      "loss": 1.7327,
      "step": 983
    },
    {
      "epoch": 1.2455696202531645,
      "grad_norm": 1.357450008392334,
      "learning_rate": 5e-05,
      "loss": 1.7501,
      "step": 984
    },
    {
      "epoch": 1.2468354430379747,
      "grad_norm": 1.36825692653656,
      "learning_rate": 5e-05,
      "loss": 1.7276,
      "step": 985
    },
    {
      "epoch": 1.2481012658227848,
      "grad_norm": 1.3194929361343384,
      "learning_rate": 5e-05,
      "loss": 1.7089,
      "step": 986
    },
    {
      "epoch": 1.249367088607595,
      "grad_norm": 1.3845494985580444,
      "learning_rate": 5e-05,
      "loss": 1.7872,
      "step": 987
    },
    {
      "epoch": 1.250632911392405,
      "grad_norm": 1.3975796699523926,
      "learning_rate": 5e-05,
      "loss": 1.7171,
      "step": 988
    },
    {
      "epoch": 1.2518987341772152,
      "grad_norm": 1.388768196105957,
      "learning_rate": 5e-05,
      "loss": 1.7413,
      "step": 989
    },
    {
      "epoch": 1.2531645569620253,
      "grad_norm": 1.3375730514526367,
      "learning_rate": 5e-05,
      "loss": 1.6713,
      "step": 990
    },
    {
      "epoch": 1.2544303797468355,
      "grad_norm": 1.4429258108139038,
      "learning_rate": 5e-05,
      "loss": 1.7521,
      "step": 991
    },
    {
      "epoch": 1.2556962025316456,
      "grad_norm": 1.3666967153549194,
      "learning_rate": 5e-05,
      "loss": 1.7335,
      "step": 992
    },
    {
      "epoch": 1.2569620253164557,
      "grad_norm": 1.3475273847579956,
      "learning_rate": 5e-05,
      "loss": 1.7244,
      "step": 993
    },
    {
      "epoch": 1.2582278481012659,
      "grad_norm": 1.3571032285690308,
      "learning_rate": 5e-05,
      "loss": 1.7184,
      "step": 994
    },
    {
      "epoch": 1.259493670886076,
      "grad_norm": 1.3957574367523193,
      "learning_rate": 5e-05,
      "loss": 1.7206,
      "step": 995
    },
    {
      "epoch": 1.2607594936708861,
      "grad_norm": 1.3276536464691162,
      "learning_rate": 5e-05,
      "loss": 1.6959,
      "step": 996
    },
    {
      "epoch": 1.2620253164556963,
      "grad_norm": 1.3493421077728271,
      "learning_rate": 5e-05,
      "loss": 1.7471,
      "step": 997
    },
    {
      "epoch": 1.2632911392405064,
      "grad_norm": 1.321594476699829,
      "learning_rate": 5e-05,
      "loss": 1.713,
      "step": 998
    },
    {
      "epoch": 1.2645569620253165,
      "grad_norm": 1.37893545627594,
      "learning_rate": 5e-05,
      "loss": 1.7436,
      "step": 999
    },
    {
      "epoch": 1.2658227848101267,
      "grad_norm": 1.3763949871063232,
      "learning_rate": 5e-05,
      "loss": 1.7198,
      "step": 1000
    },
    {
      "epoch": 1.2670886075949368,
      "grad_norm": 1.346066951751709,
      "learning_rate": 5e-05,
      "loss": 1.6924,
      "step": 1001
    },
    {
      "epoch": 1.268354430379747,
      "grad_norm": 1.329648733139038,
      "learning_rate": 5e-05,
      "loss": 1.6794,
      "step": 1002
    },
    {
      "epoch": 1.269620253164557,
      "grad_norm": 1.3496026992797852,
      "learning_rate": 5e-05,
      "loss": 1.7091,
      "step": 1003
    },
    {
      "epoch": 1.2708860759493672,
      "grad_norm": 1.3704484701156616,
      "learning_rate": 5e-05,
      "loss": 1.7814,
      "step": 1004
    },
    {
      "epoch": 1.2721518987341773,
      "grad_norm": 1.3132617473602295,
      "learning_rate": 5e-05,
      "loss": 1.7333,
      "step": 1005
    },
    {
      "epoch": 1.2734177215189875,
      "grad_norm": 1.362127661705017,
      "learning_rate": 5e-05,
      "loss": 1.7224,
      "step": 1006
    },
    {
      "epoch": 1.2746835443037976,
      "grad_norm": 1.3191379308700562,
      "learning_rate": 5e-05,
      "loss": 1.6755,
      "step": 1007
    },
    {
      "epoch": 1.2759493670886077,
      "grad_norm": 1.3414678573608398,
      "learning_rate": 5e-05,
      "loss": 1.716,
      "step": 1008
    },
    {
      "epoch": 1.2772151898734176,
      "grad_norm": 1.353897213935852,
      "learning_rate": 5e-05,
      "loss": 1.7126,
      "step": 1009
    },
    {
      "epoch": 1.2784810126582278,
      "grad_norm": 1.3216981887817383,
      "learning_rate": 5e-05,
      "loss": 1.7199,
      "step": 1010
    },
    {
      "epoch": 1.279746835443038,
      "grad_norm": 1.3390648365020752,
      "learning_rate": 5e-05,
      "loss": 1.7288,
      "step": 1011
    },
    {
      "epoch": 1.281012658227848,
      "grad_norm": 1.3871715068817139,
      "learning_rate": 5e-05,
      "loss": 1.7192,
      "step": 1012
    },
    {
      "epoch": 1.2822784810126582,
      "grad_norm": 1.3877084255218506,
      "learning_rate": 5e-05,
      "loss": 1.8141,
      "step": 1013
    },
    {
      "epoch": 1.2835443037974683,
      "grad_norm": 1.3759971857070923,
      "learning_rate": 5e-05,
      "loss": 1.7318,
      "step": 1014
    },
    {
      "epoch": 1.2848101265822784,
      "grad_norm": 1.3362683057785034,
      "learning_rate": 5e-05,
      "loss": 1.74,
      "step": 1015
    },
    {
      "epoch": 1.2860759493670886,
      "grad_norm": 1.3354710340499878,
      "learning_rate": 5e-05,
      "loss": 1.7064,
      "step": 1016
    },
    {
      "epoch": 1.2873417721518987,
      "grad_norm": 1.3268119096755981,
      "learning_rate": 5e-05,
      "loss": 1.6526,
      "step": 1017
    },
    {
      "epoch": 1.2886075949367088,
      "grad_norm": 1.3669061660766602,
      "learning_rate": 5e-05,
      "loss": 1.6721,
      "step": 1018
    },
    {
      "epoch": 1.289873417721519,
      "grad_norm": 1.3170963525772095,
      "learning_rate": 5e-05,
      "loss": 1.6116,
      "step": 1019
    },
    {
      "epoch": 1.2911392405063291,
      "grad_norm": 1.352674961090088,
      "learning_rate": 5e-05,
      "loss": 1.7034,
      "step": 1020
    },
    {
      "epoch": 1.2924050632911392,
      "grad_norm": 1.3672890663146973,
      "learning_rate": 5e-05,
      "loss": 1.7024,
      "step": 1021
    },
    {
      "epoch": 1.2936708860759494,
      "grad_norm": 1.3625702857971191,
      "learning_rate": 5e-05,
      "loss": 1.6679,
      "step": 1022
    },
    {
      "epoch": 1.2949367088607595,
      "grad_norm": 1.3540359735488892,
      "learning_rate": 5e-05,
      "loss": 1.735,
      "step": 1023
    },
    {
      "epoch": 1.2962025316455696,
      "grad_norm": 1.3156886100769043,
      "learning_rate": 5e-05,
      "loss": 1.7421,
      "step": 1024
    },
    {
      "epoch": 1.2974683544303798,
      "grad_norm": 1.3201323747634888,
      "learning_rate": 5e-05,
      "loss": 1.6997,
      "step": 1025
    },
    {
      "epoch": 1.29873417721519,
      "grad_norm": 1.3795197010040283,
      "learning_rate": 5e-05,
      "loss": 1.7675,
      "step": 1026
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.3416868448257446,
      "learning_rate": 5e-05,
      "loss": 1.6675,
      "step": 1027
    },
    {
      "epoch": 1.3012658227848102,
      "grad_norm": 1.346312165260315,
      "learning_rate": 5e-05,
      "loss": 1.6664,
      "step": 1028
    },
    {
      "epoch": 1.3025316455696203,
      "grad_norm": 1.3337321281433105,
      "learning_rate": 5e-05,
      "loss": 1.7705,
      "step": 1029
    },
    {
      "epoch": 1.3037974683544304,
      "grad_norm": 1.3723429441452026,
      "learning_rate": 5e-05,
      "loss": 1.6585,
      "step": 1030
    },
    {
      "epoch": 1.3050632911392406,
      "grad_norm": 1.3546675443649292,
      "learning_rate": 5e-05,
      "loss": 1.7386,
      "step": 1031
    },
    {
      "epoch": 1.3063291139240507,
      "grad_norm": 1.3433507680892944,
      "learning_rate": 5e-05,
      "loss": 1.7035,
      "step": 1032
    },
    {
      "epoch": 1.3075949367088606,
      "grad_norm": 1.3204652070999146,
      "learning_rate": 5e-05,
      "loss": 1.6118,
      "step": 1033
    },
    {
      "epoch": 1.3088607594936708,
      "grad_norm": 1.408068299293518,
      "learning_rate": 5e-05,
      "loss": 1.7449,
      "step": 1034
    },
    {
      "epoch": 1.310126582278481,
      "grad_norm": 1.3231077194213867,
      "learning_rate": 5e-05,
      "loss": 1.7443,
      "step": 1035
    },
    {
      "epoch": 1.311392405063291,
      "grad_norm": 1.349136233329773,
      "learning_rate": 5e-05,
      "loss": 1.7178,
      "step": 1036
    },
    {
      "epoch": 1.3126582278481012,
      "grad_norm": 1.3626407384872437,
      "learning_rate": 5e-05,
      "loss": 1.7014,
      "step": 1037
    },
    {
      "epoch": 1.3139240506329113,
      "grad_norm": 1.3179320096969604,
      "learning_rate": 5e-05,
      "loss": 1.6832,
      "step": 1038
    },
    {
      "epoch": 1.3151898734177214,
      "grad_norm": 1.3338931798934937,
      "learning_rate": 5e-05,
      "loss": 1.6399,
      "step": 1039
    },
    {
      "epoch": 1.3164556962025316,
      "grad_norm": 1.3177298307418823,
      "learning_rate": 5e-05,
      "loss": 1.6715,
      "step": 1040
    },
    {
      "epoch": 1.3177215189873417,
      "grad_norm": 1.351262092590332,
      "learning_rate": 5e-05,
      "loss": 1.6624,
      "step": 1041
    },
    {
      "epoch": 1.3189873417721518,
      "grad_norm": 1.3566296100616455,
      "learning_rate": 5e-05,
      "loss": 1.7092,
      "step": 1042
    },
    {
      "epoch": 1.320253164556962,
      "grad_norm": 1.3604234457015991,
      "learning_rate": 5e-05,
      "loss": 1.7713,
      "step": 1043
    },
    {
      "epoch": 1.321518987341772,
      "grad_norm": 1.3803272247314453,
      "learning_rate": 5e-05,
      "loss": 1.7415,
      "step": 1044
    },
    {
      "epoch": 1.3227848101265822,
      "grad_norm": 1.3506114482879639,
      "learning_rate": 5e-05,
      "loss": 1.6488,
      "step": 1045
    },
    {
      "epoch": 1.3240506329113924,
      "grad_norm": 1.2732869386672974,
      "learning_rate": 5e-05,
      "loss": 1.6502,
      "step": 1046
    },
    {
      "epoch": 1.3253164556962025,
      "grad_norm": 1.3395816087722778,
      "learning_rate": 5e-05,
      "loss": 1.6649,
      "step": 1047
    },
    {
      "epoch": 1.3265822784810126,
      "grad_norm": 1.3228821754455566,
      "learning_rate": 5e-05,
      "loss": 1.673,
      "step": 1048
    },
    {
      "epoch": 1.3278481012658228,
      "grad_norm": 1.3741300106048584,
      "learning_rate": 5e-05,
      "loss": 1.6839,
      "step": 1049
    },
    {
      "epoch": 1.3291139240506329,
      "grad_norm": 1.383795976638794,
      "learning_rate": 5e-05,
      "loss": 1.7107,
      "step": 1050
    },
    {
      "epoch": 1.330379746835443,
      "grad_norm": 1.333261251449585,
      "learning_rate": 5e-05,
      "loss": 1.6886,
      "step": 1051
    },
    {
      "epoch": 1.3316455696202532,
      "grad_norm": 1.3182340860366821,
      "learning_rate": 5e-05,
      "loss": 1.6578,
      "step": 1052
    },
    {
      "epoch": 1.3329113924050633,
      "grad_norm": 1.379482626914978,
      "learning_rate": 5e-05,
      "loss": 1.616,
      "step": 1053
    },
    {
      "epoch": 1.3341772151898734,
      "grad_norm": 1.3545893430709839,
      "learning_rate": 5e-05,
      "loss": 1.6103,
      "step": 1054
    },
    {
      "epoch": 1.3354430379746836,
      "grad_norm": 1.324273705482483,
      "learning_rate": 5e-05,
      "loss": 1.7014,
      "step": 1055
    },
    {
      "epoch": 1.3367088607594937,
      "grad_norm": 1.3024096488952637,
      "learning_rate": 5e-05,
      "loss": 1.6456,
      "step": 1056
    },
    {
      "epoch": 1.3379746835443038,
      "grad_norm": 1.3553102016448975,
      "learning_rate": 5e-05,
      "loss": 1.7369,
      "step": 1057
    },
    {
      "epoch": 1.339240506329114,
      "grad_norm": 1.30955171585083,
      "learning_rate": 5e-05,
      "loss": 1.6656,
      "step": 1058
    },
    {
      "epoch": 1.340506329113924,
      "grad_norm": 1.3511826992034912,
      "learning_rate": 5e-05,
      "loss": 1.7107,
      "step": 1059
    },
    {
      "epoch": 1.3417721518987342,
      "grad_norm": 1.3011997938156128,
      "learning_rate": 5e-05,
      "loss": 1.6316,
      "step": 1060
    },
    {
      "epoch": 1.3430379746835444,
      "grad_norm": 1.352569818496704,
      "learning_rate": 5e-05,
      "loss": 1.5859,
      "step": 1061
    },
    {
      "epoch": 1.3443037974683545,
      "grad_norm": 1.3371225595474243,
      "learning_rate": 5e-05,
      "loss": 1.6757,
      "step": 1062
    },
    {
      "epoch": 1.3455696202531646,
      "grad_norm": 1.388960361480713,
      "learning_rate": 5e-05,
      "loss": 1.6729,
      "step": 1063
    },
    {
      "epoch": 1.3468354430379748,
      "grad_norm": 1.3210846185684204,
      "learning_rate": 5e-05,
      "loss": 1.7173,
      "step": 1064
    },
    {
      "epoch": 1.3481012658227849,
      "grad_norm": 1.3749265670776367,
      "learning_rate": 5e-05,
      "loss": 1.7106,
      "step": 1065
    },
    {
      "epoch": 1.349367088607595,
      "grad_norm": 1.3586256504058838,
      "learning_rate": 5e-05,
      "loss": 1.7375,
      "step": 1066
    },
    {
      "epoch": 1.3506329113924052,
      "grad_norm": 1.335085153579712,
      "learning_rate": 5e-05,
      "loss": 1.6128,
      "step": 1067
    },
    {
      "epoch": 1.3518987341772153,
      "grad_norm": 1.3503034114837646,
      "learning_rate": 5e-05,
      "loss": 1.666,
      "step": 1068
    },
    {
      "epoch": 1.3531645569620254,
      "grad_norm": 1.419719934463501,
      "learning_rate": 5e-05,
      "loss": 1.6905,
      "step": 1069
    },
    {
      "epoch": 1.3544303797468356,
      "grad_norm": 1.317280650138855,
      "learning_rate": 5e-05,
      "loss": 1.6996,
      "step": 1070
    },
    {
      "epoch": 1.3556962025316457,
      "grad_norm": 1.284589409828186,
      "learning_rate": 5e-05,
      "loss": 1.6289,
      "step": 1071
    },
    {
      "epoch": 1.3569620253164558,
      "grad_norm": 1.3096184730529785,
      "learning_rate": 5e-05,
      "loss": 1.6289,
      "step": 1072
    },
    {
      "epoch": 1.358227848101266,
      "grad_norm": 1.3576773405075073,
      "learning_rate": 5e-05,
      "loss": 1.7209,
      "step": 1073
    },
    {
      "epoch": 1.3594936708860759,
      "grad_norm": 1.3542838096618652,
      "learning_rate": 5e-05,
      "loss": 1.6718,
      "step": 1074
    },
    {
      "epoch": 1.360759493670886,
      "grad_norm": 1.3015813827514648,
      "learning_rate": 5e-05,
      "loss": 1.646,
      "step": 1075
    },
    {
      "epoch": 1.3620253164556961,
      "grad_norm": 1.3380885124206543,
      "learning_rate": 5e-05,
      "loss": 1.6892,
      "step": 1076
    },
    {
      "epoch": 1.3632911392405063,
      "grad_norm": 1.3598932027816772,
      "learning_rate": 5e-05,
      "loss": 1.6839,
      "step": 1077
    },
    {
      "epoch": 1.3645569620253164,
      "grad_norm": 1.359582781791687,
      "learning_rate": 5e-05,
      "loss": 1.7314,
      "step": 1078
    },
    {
      "epoch": 1.3658227848101265,
      "grad_norm": 1.3168185949325562,
      "learning_rate": 5e-05,
      "loss": 1.714,
      "step": 1079
    },
    {
      "epoch": 1.3670886075949367,
      "grad_norm": 1.3338955640792847,
      "learning_rate": 5e-05,
      "loss": 1.6768,
      "step": 1080
    },
    {
      "epoch": 1.3683544303797468,
      "grad_norm": 1.3167431354522705,
      "learning_rate": 5e-05,
      "loss": 1.63,
      "step": 1081
    },
    {
      "epoch": 1.369620253164557,
      "grad_norm": 1.3845665454864502,
      "learning_rate": 5e-05,
      "loss": 1.68,
      "step": 1082
    },
    {
      "epoch": 1.370886075949367,
      "grad_norm": 1.3116576671600342,
      "learning_rate": 5e-05,
      "loss": 1.6039,
      "step": 1083
    },
    {
      "epoch": 1.3721518987341772,
      "grad_norm": 1.3429559469223022,
      "learning_rate": 5e-05,
      "loss": 1.7338,
      "step": 1084
    },
    {
      "epoch": 1.3734177215189873,
      "grad_norm": 1.3316686153411865,
      "learning_rate": 5e-05,
      "loss": 1.6199,
      "step": 1085
    },
    {
      "epoch": 1.3746835443037975,
      "grad_norm": 1.2839133739471436,
      "learning_rate": 5e-05,
      "loss": 1.603,
      "step": 1086
    },
    {
      "epoch": 1.3759493670886076,
      "grad_norm": 1.36078941822052,
      "learning_rate": 5e-05,
      "loss": 1.6615,
      "step": 1087
    },
    {
      "epoch": 1.3772151898734177,
      "grad_norm": 1.3923335075378418,
      "learning_rate": 5e-05,
      "loss": 1.6402,
      "step": 1088
    },
    {
      "epoch": 1.3784810126582279,
      "grad_norm": 1.3086191415786743,
      "learning_rate": 5e-05,
      "loss": 1.6425,
      "step": 1089
    },
    {
      "epoch": 1.379746835443038,
      "grad_norm": 1.3306822776794434,
      "learning_rate": 5e-05,
      "loss": 1.6725,
      "step": 1090
    },
    {
      "epoch": 1.3810126582278481,
      "grad_norm": 1.394723653793335,
      "learning_rate": 5e-05,
      "loss": 1.7413,
      "step": 1091
    },
    {
      "epoch": 1.3822784810126583,
      "grad_norm": 1.3503518104553223,
      "learning_rate": 5e-05,
      "loss": 1.75,
      "step": 1092
    },
    {
      "epoch": 1.3835443037974684,
      "grad_norm": 1.3653287887573242,
      "learning_rate": 5e-05,
      "loss": 1.7046,
      "step": 1093
    },
    {
      "epoch": 1.3848101265822785,
      "grad_norm": 1.3283443450927734,
      "learning_rate": 5e-05,
      "loss": 1.5834,
      "step": 1094
    },
    {
      "epoch": 1.3860759493670887,
      "grad_norm": 1.3511027097702026,
      "learning_rate": 5e-05,
      "loss": 1.5975,
      "step": 1095
    },
    {
      "epoch": 1.3873417721518988,
      "grad_norm": 1.3294918537139893,
      "learning_rate": 5e-05,
      "loss": 1.6789,
      "step": 1096
    },
    {
      "epoch": 1.388607594936709,
      "grad_norm": 1.307938814163208,
      "learning_rate": 5e-05,
      "loss": 1.5704,
      "step": 1097
    },
    {
      "epoch": 1.389873417721519,
      "grad_norm": 1.3900026082992554,
      "learning_rate": 5e-05,
      "loss": 1.6831,
      "step": 1098
    },
    {
      "epoch": 1.391139240506329,
      "grad_norm": 1.37188720703125,
      "learning_rate": 5e-05,
      "loss": 1.6677,
      "step": 1099
    },
    {
      "epoch": 1.3924050632911391,
      "grad_norm": 1.3145768642425537,
      "learning_rate": 5e-05,
      "loss": 1.6294,
      "step": 1100
    },
    {
      "epoch": 1.3936708860759492,
      "grad_norm": 1.3503526449203491,
      "learning_rate": 5e-05,
      "loss": 1.6359,
      "step": 1101
    },
    {
      "epoch": 1.3949367088607594,
      "grad_norm": 1.352562665939331,
      "learning_rate": 5e-05,
      "loss": 1.6351,
      "step": 1102
    },
    {
      "epoch": 1.3962025316455695,
      "grad_norm": 1.3185676336288452,
      "learning_rate": 5e-05,
      "loss": 1.6336,
      "step": 1103
    },
    {
      "epoch": 1.3974683544303796,
      "grad_norm": 1.3235746622085571,
      "learning_rate": 5e-05,
      "loss": 1.6276,
      "step": 1104
    },
    {
      "epoch": 1.3987341772151898,
      "grad_norm": 1.364121675491333,
      "learning_rate": 5e-05,
      "loss": 1.6765,
      "step": 1105
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.323278784751892,
      "learning_rate": 5e-05,
      "loss": 1.6226,
      "step": 1106
    },
    {
      "epoch": 1.40126582278481,
      "grad_norm": 1.3597993850708008,
      "learning_rate": 5e-05,
      "loss": 1.7325,
      "step": 1107
    },
    {
      "epoch": 1.4025316455696202,
      "grad_norm": 1.2958192825317383,
      "learning_rate": 5e-05,
      "loss": 1.5942,
      "step": 1108
    },
    {
      "epoch": 1.4037974683544303,
      "grad_norm": 1.3725674152374268,
      "learning_rate": 5e-05,
      "loss": 1.7363,
      "step": 1109
    },
    {
      "epoch": 1.4050632911392404,
      "grad_norm": 1.340774655342102,
      "learning_rate": 5e-05,
      "loss": 1.6633,
      "step": 1110
    },
    {
      "epoch": 1.4063291139240506,
      "grad_norm": 1.3184224367141724,
      "learning_rate": 5e-05,
      "loss": 1.6308,
      "step": 1111
    },
    {
      "epoch": 1.4075949367088607,
      "grad_norm": 1.3033510446548462,
      "learning_rate": 5e-05,
      "loss": 1.6458,
      "step": 1112
    },
    {
      "epoch": 1.4088607594936708,
      "grad_norm": 1.355780005455017,
      "learning_rate": 5e-05,
      "loss": 1.611,
      "step": 1113
    },
    {
      "epoch": 1.410126582278481,
      "grad_norm": 1.2977030277252197,
      "learning_rate": 5e-05,
      "loss": 1.6477,
      "step": 1114
    },
    {
      "epoch": 1.4113924050632911,
      "grad_norm": 1.3112925291061401,
      "learning_rate": 5e-05,
      "loss": 1.6309,
      "step": 1115
    },
    {
      "epoch": 1.4126582278481012,
      "grad_norm": 1.3448398113250732,
      "learning_rate": 5e-05,
      "loss": 1.6622,
      "step": 1116
    },
    {
      "epoch": 1.4139240506329114,
      "grad_norm": 1.2814841270446777,
      "learning_rate": 5e-05,
      "loss": 1.5708,
      "step": 1117
    },
    {
      "epoch": 1.4151898734177215,
      "grad_norm": 1.3653923273086548,
      "learning_rate": 5e-05,
      "loss": 1.6569,
      "step": 1118
    },
    {
      "epoch": 1.4164556962025316,
      "grad_norm": 1.3726603984832764,
      "learning_rate": 5e-05,
      "loss": 1.6499,
      "step": 1119
    },
    {
      "epoch": 1.4177215189873418,
      "grad_norm": 1.324986457824707,
      "learning_rate": 5e-05,
      "loss": 1.5885,
      "step": 1120
    },
    {
      "epoch": 1.418987341772152,
      "grad_norm": 1.3437106609344482,
      "learning_rate": 5e-05,
      "loss": 1.6386,
      "step": 1121
    },
    {
      "epoch": 1.420253164556962,
      "grad_norm": 1.33476722240448,
      "learning_rate": 5e-05,
      "loss": 1.6377,
      "step": 1122
    },
    {
      "epoch": 1.4215189873417722,
      "grad_norm": 1.331981897354126,
      "learning_rate": 5e-05,
      "loss": 1.5453,
      "step": 1123
    },
    {
      "epoch": 1.4227848101265823,
      "grad_norm": 1.271408200263977,
      "learning_rate": 5e-05,
      "loss": 1.5099,
      "step": 1124
    },
    {
      "epoch": 1.4240506329113924,
      "grad_norm": 1.3967353105545044,
      "learning_rate": 5e-05,
      "loss": 1.6061,
      "step": 1125
    },
    {
      "epoch": 1.4253164556962026,
      "grad_norm": 1.3166782855987549,
      "learning_rate": 5e-05,
      "loss": 1.6422,
      "step": 1126
    },
    {
      "epoch": 1.4265822784810127,
      "grad_norm": 1.3682458400726318,
      "learning_rate": 5e-05,
      "loss": 1.6766,
      "step": 1127
    },
    {
      "epoch": 1.4278481012658228,
      "grad_norm": 1.2948094606399536,
      "learning_rate": 5e-05,
      "loss": 1.6282,
      "step": 1128
    },
    {
      "epoch": 1.429113924050633,
      "grad_norm": 1.3223334550857544,
      "learning_rate": 5e-05,
      "loss": 1.5562,
      "step": 1129
    },
    {
      "epoch": 1.4303797468354431,
      "grad_norm": 1.3213441371917725,
      "learning_rate": 5e-05,
      "loss": 1.6276,
      "step": 1130
    },
    {
      "epoch": 1.4316455696202532,
      "grad_norm": 1.3007327318191528,
      "learning_rate": 5e-05,
      "loss": 1.6107,
      "step": 1131
    },
    {
      "epoch": 1.4329113924050634,
      "grad_norm": 1.2796374559402466,
      "learning_rate": 5e-05,
      "loss": 1.5621,
      "step": 1132
    },
    {
      "epoch": 1.4341772151898735,
      "grad_norm": 1.3362139463424683,
      "learning_rate": 5e-05,
      "loss": 1.5932,
      "step": 1133
    },
    {
      "epoch": 1.4354430379746836,
      "grad_norm": 1.3211640119552612,
      "learning_rate": 5e-05,
      "loss": 1.6146,
      "step": 1134
    },
    {
      "epoch": 1.4367088607594938,
      "grad_norm": 1.3091039657592773,
      "learning_rate": 5e-05,
      "loss": 1.5941,
      "step": 1135
    },
    {
      "epoch": 1.437974683544304,
      "grad_norm": 1.3484936952590942,
      "learning_rate": 5e-05,
      "loss": 1.7164,
      "step": 1136
    },
    {
      "epoch": 1.439240506329114,
      "grad_norm": 1.2923575639724731,
      "learning_rate": 5e-05,
      "loss": 1.6357,
      "step": 1137
    },
    {
      "epoch": 1.4405063291139242,
      "grad_norm": 1.281092643737793,
      "learning_rate": 5e-05,
      "loss": 1.5378,
      "step": 1138
    },
    {
      "epoch": 1.4417721518987343,
      "grad_norm": 1.3214528560638428,
      "learning_rate": 5e-05,
      "loss": 1.6106,
      "step": 1139
    },
    {
      "epoch": 1.4430379746835442,
      "grad_norm": 1.317607045173645,
      "learning_rate": 5e-05,
      "loss": 1.576,
      "step": 1140
    },
    {
      "epoch": 1.4443037974683544,
      "grad_norm": 1.3364109992980957,
      "learning_rate": 5e-05,
      "loss": 1.7073,
      "step": 1141
    },
    {
      "epoch": 1.4455696202531645,
      "grad_norm": 1.3248705863952637,
      "learning_rate": 5e-05,
      "loss": 1.5939,
      "step": 1142
    },
    {
      "epoch": 1.4468354430379746,
      "grad_norm": 1.3565603494644165,
      "learning_rate": 5e-05,
      "loss": 1.61,
      "step": 1143
    },
    {
      "epoch": 1.4481012658227848,
      "grad_norm": 1.3488459587097168,
      "learning_rate": 5e-05,
      "loss": 1.6963,
      "step": 1144
    },
    {
      "epoch": 1.4493670886075949,
      "grad_norm": 1.346116304397583,
      "learning_rate": 5e-05,
      "loss": 1.6583,
      "step": 1145
    },
    {
      "epoch": 1.450632911392405,
      "grad_norm": 1.345579743385315,
      "learning_rate": 5e-05,
      "loss": 1.6152,
      "step": 1146
    },
    {
      "epoch": 1.4518987341772152,
      "grad_norm": 1.2984200716018677,
      "learning_rate": 5e-05,
      "loss": 1.5777,
      "step": 1147
    },
    {
      "epoch": 1.4531645569620253,
      "grad_norm": 1.2921544313430786,
      "learning_rate": 5e-05,
      "loss": 1.6246,
      "step": 1148
    },
    {
      "epoch": 1.4544303797468354,
      "grad_norm": 1.3095788955688477,
      "learning_rate": 5e-05,
      "loss": 1.5885,
      "step": 1149
    },
    {
      "epoch": 1.4556962025316456,
      "grad_norm": 1.340261697769165,
      "learning_rate": 5e-05,
      "loss": 1.6027,
      "step": 1150
    },
    {
      "epoch": 1.4569620253164557,
      "grad_norm": 1.3278741836547852,
      "learning_rate": 5e-05,
      "loss": 1.6461,
      "step": 1151
    },
    {
      "epoch": 1.4582278481012658,
      "grad_norm": 1.3758195638656616,
      "learning_rate": 5e-05,
      "loss": 1.6973,
      "step": 1152
    },
    {
      "epoch": 1.459493670886076,
      "grad_norm": 1.3141241073608398,
      "learning_rate": 5e-05,
      "loss": 1.6428,
      "step": 1153
    },
    {
      "epoch": 1.460759493670886,
      "grad_norm": 1.3328109979629517,
      "learning_rate": 5e-05,
      "loss": 1.6184,
      "step": 1154
    },
    {
      "epoch": 1.4620253164556962,
      "grad_norm": 1.3238497972488403,
      "learning_rate": 5e-05,
      "loss": 1.6295,
      "step": 1155
    },
    {
      "epoch": 1.4632911392405064,
      "grad_norm": 1.3356497287750244,
      "learning_rate": 5e-05,
      "loss": 1.5457,
      "step": 1156
    },
    {
      "epoch": 1.4645569620253165,
      "grad_norm": 1.3287229537963867,
      "learning_rate": 5e-05,
      "loss": 1.7031,
      "step": 1157
    },
    {
      "epoch": 1.4658227848101266,
      "grad_norm": 1.324723482131958,
      "learning_rate": 5e-05,
      "loss": 1.6795,
      "step": 1158
    },
    {
      "epoch": 1.4670886075949368,
      "grad_norm": 1.3378719091415405,
      "learning_rate": 5e-05,
      "loss": 1.6983,
      "step": 1159
    },
    {
      "epoch": 1.4683544303797469,
      "grad_norm": 1.3160326480865479,
      "learning_rate": 5e-05,
      "loss": 1.6593,
      "step": 1160
    },
    {
      "epoch": 1.469620253164557,
      "grad_norm": 1.336673617362976,
      "learning_rate": 5e-05,
      "loss": 1.6697,
      "step": 1161
    },
    {
      "epoch": 1.4708860759493672,
      "grad_norm": 1.3185197114944458,
      "learning_rate": 5e-05,
      "loss": 1.6252,
      "step": 1162
    },
    {
      "epoch": 1.4721518987341773,
      "grad_norm": 1.317154884338379,
      "learning_rate": 5e-05,
      "loss": 1.6247,
      "step": 1163
    },
    {
      "epoch": 1.4734177215189874,
      "grad_norm": 1.3065383434295654,
      "learning_rate": 5e-05,
      "loss": 1.6275,
      "step": 1164
    },
    {
      "epoch": 1.4746835443037973,
      "grad_norm": 1.2699203491210938,
      "learning_rate": 5e-05,
      "loss": 1.5548,
      "step": 1165
    },
    {
      "epoch": 1.4759493670886075,
      "grad_norm": 1.3659355640411377,
      "learning_rate": 5e-05,
      "loss": 1.6407,
      "step": 1166
    },
    {
      "epoch": 1.4772151898734176,
      "grad_norm": 1.3433523178100586,
      "learning_rate": 5e-05,
      "loss": 1.6313,
      "step": 1167
    },
    {
      "epoch": 1.4784810126582277,
      "grad_norm": 1.3565645217895508,
      "learning_rate": 5e-05,
      "loss": 1.5645,
      "step": 1168
    },
    {
      "epoch": 1.4797468354430379,
      "grad_norm": 1.3096204996109009,
      "learning_rate": 5e-05,
      "loss": 1.5979,
      "step": 1169
    },
    {
      "epoch": 1.481012658227848,
      "grad_norm": 1.3289873600006104,
      "learning_rate": 5e-05,
      "loss": 1.6401,
      "step": 1170
    },
    {
      "epoch": 1.4822784810126581,
      "grad_norm": 1.3502544164657593,
      "learning_rate": 5e-05,
      "loss": 1.6344,
      "step": 1171
    },
    {
      "epoch": 1.4835443037974683,
      "grad_norm": 1.326680302619934,
      "learning_rate": 5e-05,
      "loss": 1.603,
      "step": 1172
    },
    {
      "epoch": 1.4848101265822784,
      "grad_norm": 1.3198660612106323,
      "learning_rate": 5e-05,
      "loss": 1.602,
      "step": 1173
    },
    {
      "epoch": 1.4860759493670885,
      "grad_norm": 1.3083945512771606,
      "learning_rate": 5e-05,
      "loss": 1.5807,
      "step": 1174
    },
    {
      "epoch": 1.4873417721518987,
      "grad_norm": 1.3295691013336182,
      "learning_rate": 5e-05,
      "loss": 1.5787,
      "step": 1175
    },
    {
      "epoch": 1.4886075949367088,
      "grad_norm": 1.3105815649032593,
      "learning_rate": 5e-05,
      "loss": 1.6471,
      "step": 1176
    },
    {
      "epoch": 1.489873417721519,
      "grad_norm": 1.3221768140792847,
      "learning_rate": 5e-05,
      "loss": 1.5552,
      "step": 1177
    },
    {
      "epoch": 1.491139240506329,
      "grad_norm": 1.3431650400161743,
      "learning_rate": 5e-05,
      "loss": 1.6172,
      "step": 1178
    },
    {
      "epoch": 1.4924050632911392,
      "grad_norm": 1.3477425575256348,
      "learning_rate": 5e-05,
      "loss": 1.6665,
      "step": 1179
    },
    {
      "epoch": 1.4936708860759493,
      "grad_norm": 1.3064218759536743,
      "learning_rate": 5e-05,
      "loss": 1.6268,
      "step": 1180
    },
    {
      "epoch": 1.4949367088607595,
      "grad_norm": 1.3041921854019165,
      "learning_rate": 5e-05,
      "loss": 1.556,
      "step": 1181
    },
    {
      "epoch": 1.4962025316455696,
      "grad_norm": 1.3980087041854858,
      "learning_rate": 5e-05,
      "loss": 1.6151,
      "step": 1182
    },
    {
      "epoch": 1.4974683544303797,
      "grad_norm": 1.2987474203109741,
      "learning_rate": 5e-05,
      "loss": 1.6008,
      "step": 1183
    },
    {
      "epoch": 1.4987341772151899,
      "grad_norm": 1.2818524837493896,
      "learning_rate": 5e-05,
      "loss": 1.5268,
      "step": 1184
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.3387622833251953,
      "learning_rate": 5e-05,
      "loss": 1.607,
      "step": 1185
    },
    {
      "epoch": 1.5,
      "eval_loss": 2.226088523864746,
      "eval_runtime": 1068.3534,
      "eval_samples_per_second": 84.129,
      "eval_steps_per_second": 0.658,
      "step": 1185
    },
    {
      "epoch": 1.5012658227848101,
      "grad_norm": 1.3913637399673462,
      "learning_rate": 5e-05,
      "loss": 1.7404,
      "step": 1186
    },
    {
      "epoch": 1.5025316455696203,
      "grad_norm": 1.3215397596359253,
      "learning_rate": 5e-05,
      "loss": 1.6637,
      "step": 1187
    },
    {
      "epoch": 1.5037974683544304,
      "grad_norm": 1.326996922492981,
      "learning_rate": 5e-05,
      "loss": 1.5509,
      "step": 1188
    },
    {
      "epoch": 1.5050632911392405,
      "grad_norm": 1.332370400428772,
      "learning_rate": 5e-05,
      "loss": 1.6263,
      "step": 1189
    },
    {
      "epoch": 1.5063291139240507,
      "grad_norm": 1.2993590831756592,
      "learning_rate": 5e-05,
      "loss": 1.6004,
      "step": 1190
    },
    {
      "epoch": 1.5075949367088608,
      "grad_norm": 1.345234751701355,
      "learning_rate": 5e-05,
      "loss": 1.6415,
      "step": 1191
    },
    {
      "epoch": 1.508860759493671,
      "grad_norm": 1.3077303171157837,
      "learning_rate": 5e-05,
      "loss": 1.573,
      "step": 1192
    },
    {
      "epoch": 1.510126582278481,
      "grad_norm": 1.3043725490570068,
      "learning_rate": 5e-05,
      "loss": 1.5711,
      "step": 1193
    },
    {
      "epoch": 1.5113924050632912,
      "grad_norm": 1.3170690536499023,
      "learning_rate": 5e-05,
      "loss": 1.6167,
      "step": 1194
    },
    {
      "epoch": 1.5126582278481013,
      "grad_norm": 1.3253036737442017,
      "learning_rate": 5e-05,
      "loss": 1.6402,
      "step": 1195
    },
    {
      "epoch": 1.5139240506329115,
      "grad_norm": 1.3063721656799316,
      "learning_rate": 5e-05,
      "loss": 1.6137,
      "step": 1196
    },
    {
      "epoch": 1.5151898734177216,
      "grad_norm": 1.353804588317871,
      "learning_rate": 5e-05,
      "loss": 1.6428,
      "step": 1197
    },
    {
      "epoch": 1.5164556962025317,
      "grad_norm": 1.3074010610580444,
      "learning_rate": 5e-05,
      "loss": 1.6216,
      "step": 1198
    },
    {
      "epoch": 1.5177215189873419,
      "grad_norm": 1.3402782678604126,
      "learning_rate": 5e-05,
      "loss": 1.5662,
      "step": 1199
    },
    {
      "epoch": 1.518987341772152,
      "grad_norm": 1.3589167594909668,
      "learning_rate": 5e-05,
      "loss": 1.6153,
      "step": 1200
    },
    {
      "epoch": 1.5202531645569621,
      "grad_norm": 1.3420228958129883,
      "learning_rate": 5e-05,
      "loss": 1.5613,
      "step": 1201
    },
    {
      "epoch": 1.5215189873417723,
      "grad_norm": 1.3082249164581299,
      "learning_rate": 5e-05,
      "loss": 1.5721,
      "step": 1202
    },
    {
      "epoch": 1.5227848101265824,
      "grad_norm": 1.3449490070343018,
      "learning_rate": 5e-05,
      "loss": 1.576,
      "step": 1203
    },
    {
      "epoch": 1.5240506329113925,
      "grad_norm": 1.297575831413269,
      "learning_rate": 5e-05,
      "loss": 1.5766,
      "step": 1204
    },
    {
      "epoch": 1.5253164556962027,
      "grad_norm": 1.345091700553894,
      "learning_rate": 5e-05,
      "loss": 1.6858,
      "step": 1205
    },
    {
      "epoch": 1.5265822784810128,
      "grad_norm": 1.3718163967132568,
      "learning_rate": 5e-05,
      "loss": 1.6712,
      "step": 1206
    },
    {
      "epoch": 1.527848101265823,
      "grad_norm": 1.2866358757019043,
      "learning_rate": 5e-05,
      "loss": 1.4943,
      "step": 1207
    },
    {
      "epoch": 1.529113924050633,
      "grad_norm": 1.3684877157211304,
      "learning_rate": 5e-05,
      "loss": 1.6258,
      "step": 1208
    },
    {
      "epoch": 1.5303797468354432,
      "grad_norm": 1.3116236925125122,
      "learning_rate": 5e-05,
      "loss": 1.5429,
      "step": 1209
    },
    {
      "epoch": 1.5316455696202531,
      "grad_norm": 1.3966176509857178,
      "learning_rate": 5e-05,
      "loss": 1.6646,
      "step": 1210
    },
    {
      "epoch": 1.5329113924050632,
      "grad_norm": 1.333619236946106,
      "learning_rate": 5e-05,
      "loss": 1.6692,
      "step": 1211
    },
    {
      "epoch": 1.5341772151898734,
      "grad_norm": 1.3202146291732788,
      "learning_rate": 5e-05,
      "loss": 1.5904,
      "step": 1212
    },
    {
      "epoch": 1.5354430379746835,
      "grad_norm": 1.3055484294891357,
      "learning_rate": 5e-05,
      "loss": 1.6114,
      "step": 1213
    },
    {
      "epoch": 1.5367088607594936,
      "grad_norm": 1.3235632181167603,
      "learning_rate": 5e-05,
      "loss": 1.6093,
      "step": 1214
    },
    {
      "epoch": 1.5379746835443038,
      "grad_norm": 1.3447827100753784,
      "learning_rate": 5e-05,
      "loss": 1.5394,
      "step": 1215
    },
    {
      "epoch": 1.539240506329114,
      "grad_norm": 1.35183584690094,
      "learning_rate": 5e-05,
      "loss": 1.6231,
      "step": 1216
    },
    {
      "epoch": 1.540506329113924,
      "grad_norm": 1.327208399772644,
      "learning_rate": 5e-05,
      "loss": 1.5662,
      "step": 1217
    },
    {
      "epoch": 1.5417721518987342,
      "grad_norm": 1.3330953121185303,
      "learning_rate": 5e-05,
      "loss": 1.5771,
      "step": 1218
    },
    {
      "epoch": 1.5430379746835443,
      "grad_norm": 1.3256393671035767,
      "learning_rate": 5e-05,
      "loss": 1.6361,
      "step": 1219
    },
    {
      "epoch": 1.5443037974683544,
      "grad_norm": 1.2899571657180786,
      "learning_rate": 5e-05,
      "loss": 1.4817,
      "step": 1220
    },
    {
      "epoch": 1.5455696202531646,
      "grad_norm": 1.3185282945632935,
      "learning_rate": 5e-05,
      "loss": 1.6345,
      "step": 1221
    },
    {
      "epoch": 1.5468354430379747,
      "grad_norm": 1.2981743812561035,
      "learning_rate": 5e-05,
      "loss": 1.5366,
      "step": 1222
    },
    {
      "epoch": 1.5481012658227848,
      "grad_norm": 1.3130804300308228,
      "learning_rate": 5e-05,
      "loss": 1.5285,
      "step": 1223
    },
    {
      "epoch": 1.549367088607595,
      "grad_norm": 1.3324520587921143,
      "learning_rate": 5e-05,
      "loss": 1.5764,
      "step": 1224
    },
    {
      "epoch": 1.5506329113924051,
      "grad_norm": 1.3612816333770752,
      "learning_rate": 5e-05,
      "loss": 1.6541,
      "step": 1225
    },
    {
      "epoch": 1.5518987341772152,
      "grad_norm": 1.3730522394180298,
      "learning_rate": 5e-05,
      "loss": 1.65,
      "step": 1226
    },
    {
      "epoch": 1.5531645569620252,
      "grad_norm": 1.3459535837173462,
      "learning_rate": 5e-05,
      "loss": 1.573,
      "step": 1227
    },
    {
      "epoch": 1.5544303797468353,
      "grad_norm": 1.2988407611846924,
      "learning_rate": 5e-05,
      "loss": 1.5423,
      "step": 1228
    },
    {
      "epoch": 1.5556962025316454,
      "grad_norm": 1.3461202383041382,
      "learning_rate": 5e-05,
      "loss": 1.5613,
      "step": 1229
    },
    {
      "epoch": 1.5569620253164556,
      "grad_norm": 1.2938716411590576,
      "learning_rate": 5e-05,
      "loss": 1.502,
      "step": 1230
    },
    {
      "epoch": 1.5582278481012657,
      "grad_norm": 1.317613959312439,
      "learning_rate": 5e-05,
      "loss": 1.5659,
      "step": 1231
    },
    {
      "epoch": 1.5594936708860758,
      "grad_norm": 1.3076965808868408,
      "learning_rate": 5e-05,
      "loss": 1.5883,
      "step": 1232
    },
    {
      "epoch": 1.560759493670886,
      "grad_norm": 1.2974886894226074,
      "learning_rate": 5e-05,
      "loss": 1.5866,
      "step": 1233
    },
    {
      "epoch": 1.562025316455696,
      "grad_norm": 1.3383285999298096,
      "learning_rate": 5e-05,
      "loss": 1.5742,
      "step": 1234
    },
    {
      "epoch": 1.5632911392405062,
      "grad_norm": 1.3121387958526611,
      "learning_rate": 5e-05,
      "loss": 1.619,
      "step": 1235
    },
    {
      "epoch": 1.5645569620253164,
      "grad_norm": 1.3226139545440674,
      "learning_rate": 5e-05,
      "loss": 1.6584,
      "step": 1236
    },
    {
      "epoch": 1.5658227848101265,
      "grad_norm": 1.3234941959381104,
      "learning_rate": 5e-05,
      "loss": 1.5709,
      "step": 1237
    },
    {
      "epoch": 1.5670886075949366,
      "grad_norm": 1.366855502128601,
      "learning_rate": 5e-05,
      "loss": 1.6856,
      "step": 1238
    },
    {
      "epoch": 1.5683544303797468,
      "grad_norm": 1.358414888381958,
      "learning_rate": 5e-05,
      "loss": 1.6462,
      "step": 1239
    },
    {
      "epoch": 1.5696202531645569,
      "grad_norm": 1.3172322511672974,
      "learning_rate": 5e-05,
      "loss": 1.5807,
      "step": 1240
    },
    {
      "epoch": 1.570886075949367,
      "grad_norm": 1.3137362003326416,
      "learning_rate": 5e-05,
      "loss": 1.5899,
      "step": 1241
    },
    {
      "epoch": 1.5721518987341772,
      "grad_norm": 1.3230607509613037,
      "learning_rate": 5e-05,
      "loss": 1.5804,
      "step": 1242
    },
    {
      "epoch": 1.5734177215189873,
      "grad_norm": 1.3118221759796143,
      "learning_rate": 5e-05,
      "loss": 1.5796,
      "step": 1243
    },
    {
      "epoch": 1.5746835443037974,
      "grad_norm": 1.3123701810836792,
      "learning_rate": 5e-05,
      "loss": 1.5834,
      "step": 1244
    },
    {
      "epoch": 1.5759493670886076,
      "grad_norm": 1.35354745388031,
      "learning_rate": 5e-05,
      "loss": 1.4602,
      "step": 1245
    },
    {
      "epoch": 1.5772151898734177,
      "grad_norm": 1.3286617994308472,
      "learning_rate": 5e-05,
      "loss": 1.6532,
      "step": 1246
    },
    {
      "epoch": 1.5784810126582278,
      "grad_norm": 1.3207851648330688,
      "learning_rate": 5e-05,
      "loss": 1.5649,
      "step": 1247
    },
    {
      "epoch": 1.579746835443038,
      "grad_norm": 1.3220791816711426,
      "learning_rate": 5e-05,
      "loss": 1.521,
      "step": 1248
    },
    {
      "epoch": 1.581012658227848,
      "grad_norm": 1.332767367362976,
      "learning_rate": 5e-05,
      "loss": 1.5559,
      "step": 1249
    },
    {
      "epoch": 1.5822784810126582,
      "grad_norm": 1.3937153816223145,
      "learning_rate": 5e-05,
      "loss": 1.6292,
      "step": 1250
    },
    {
      "epoch": 1.5835443037974684,
      "grad_norm": 1.29386568069458,
      "learning_rate": 5e-05,
      "loss": 1.544,
      "step": 1251
    },
    {
      "epoch": 1.5848101265822785,
      "grad_norm": 1.3284544944763184,
      "learning_rate": 5e-05,
      "loss": 1.556,
      "step": 1252
    },
    {
      "epoch": 1.5860759493670886,
      "grad_norm": 1.3293979167938232,
      "learning_rate": 5e-05,
      "loss": 1.6017,
      "step": 1253
    },
    {
      "epoch": 1.5873417721518988,
      "grad_norm": 1.317305564880371,
      "learning_rate": 5e-05,
      "loss": 1.5391,
      "step": 1254
    },
    {
      "epoch": 1.5886075949367089,
      "grad_norm": 1.3226934671401978,
      "learning_rate": 5e-05,
      "loss": 1.5773,
      "step": 1255
    },
    {
      "epoch": 1.589873417721519,
      "grad_norm": 1.3085840940475464,
      "learning_rate": 5e-05,
      "loss": 1.5943,
      "step": 1256
    },
    {
      "epoch": 1.5911392405063292,
      "grad_norm": 1.394387125968933,
      "learning_rate": 5e-05,
      "loss": 1.5852,
      "step": 1257
    },
    {
      "epoch": 1.5924050632911393,
      "grad_norm": 1.3237829208374023,
      "learning_rate": 5e-05,
      "loss": 1.5916,
      "step": 1258
    },
    {
      "epoch": 1.5936708860759494,
      "grad_norm": 1.333778738975525,
      "learning_rate": 5e-05,
      "loss": 1.6145,
      "step": 1259
    },
    {
      "epoch": 1.5949367088607596,
      "grad_norm": 1.3028995990753174,
      "learning_rate": 5e-05,
      "loss": 1.6095,
      "step": 1260
    },
    {
      "epoch": 1.5962025316455697,
      "grad_norm": 1.3848531246185303,
      "learning_rate": 5e-05,
      "loss": 1.6251,
      "step": 1261
    },
    {
      "epoch": 1.5974683544303798,
      "grad_norm": 1.3544574975967407,
      "learning_rate": 5e-05,
      "loss": 1.5212,
      "step": 1262
    },
    {
      "epoch": 1.59873417721519,
      "grad_norm": 1.3915607929229736,
      "learning_rate": 5e-05,
      "loss": 1.5887,
      "step": 1263
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.3100594282150269,
      "learning_rate": 5e-05,
      "loss": 1.5589,
      "step": 1264
    },
    {
      "epoch": 1.6012658227848102,
      "grad_norm": 1.3211003541946411,
      "learning_rate": 5e-05,
      "loss": 1.5718,
      "step": 1265
    },
    {
      "epoch": 1.6025316455696204,
      "grad_norm": 1.3426564931869507,
      "learning_rate": 5e-05,
      "loss": 1.576,
      "step": 1266
    },
    {
      "epoch": 1.6037974683544305,
      "grad_norm": 1.33974289894104,
      "learning_rate": 5e-05,
      "loss": 1.6171,
      "step": 1267
    },
    {
      "epoch": 1.6050632911392406,
      "grad_norm": 1.2990226745605469,
      "learning_rate": 5e-05,
      "loss": 1.5059,
      "step": 1268
    },
    {
      "epoch": 1.6063291139240508,
      "grad_norm": 1.3341286182403564,
      "learning_rate": 5e-05,
      "loss": 1.531,
      "step": 1269
    },
    {
      "epoch": 1.6075949367088609,
      "grad_norm": 1.3666795492172241,
      "learning_rate": 5e-05,
      "loss": 1.5773,
      "step": 1270
    },
    {
      "epoch": 1.608860759493671,
      "grad_norm": 1.2901262044906616,
      "learning_rate": 5e-05,
      "loss": 1.5161,
      "step": 1271
    },
    {
      "epoch": 1.6101265822784812,
      "grad_norm": 1.3309693336486816,
      "learning_rate": 5e-05,
      "loss": 1.5606,
      "step": 1272
    },
    {
      "epoch": 1.6113924050632913,
      "grad_norm": 1.2957483530044556,
      "learning_rate": 5e-05,
      "loss": 1.451,
      "step": 1273
    },
    {
      "epoch": 1.6126582278481014,
      "grad_norm": 1.2960695028305054,
      "learning_rate": 5e-05,
      "loss": 1.5309,
      "step": 1274
    },
    {
      "epoch": 1.6139240506329116,
      "grad_norm": 1.3619003295898438,
      "learning_rate": 5e-05,
      "loss": 1.6395,
      "step": 1275
    },
    {
      "epoch": 1.6151898734177215,
      "grad_norm": 1.3259178400039673,
      "learning_rate": 5e-05,
      "loss": 1.6013,
      "step": 1276
    },
    {
      "epoch": 1.6164556962025316,
      "grad_norm": 1.3194500207901,
      "learning_rate": 5e-05,
      "loss": 1.6208,
      "step": 1277
    },
    {
      "epoch": 1.6177215189873417,
      "grad_norm": 1.3422346115112305,
      "learning_rate": 5e-05,
      "loss": 1.625,
      "step": 1278
    },
    {
      "epoch": 1.6189873417721519,
      "grad_norm": 1.286225438117981,
      "learning_rate": 5e-05,
      "loss": 1.5424,
      "step": 1279
    },
    {
      "epoch": 1.620253164556962,
      "grad_norm": 1.336119532585144,
      "learning_rate": 5e-05,
      "loss": 1.573,
      "step": 1280
    },
    {
      "epoch": 1.6215189873417721,
      "grad_norm": 1.3345584869384766,
      "learning_rate": 5e-05,
      "loss": 1.5939,
      "step": 1281
    },
    {
      "epoch": 1.6227848101265823,
      "grad_norm": 1.3039270639419556,
      "learning_rate": 5e-05,
      "loss": 1.5378,
      "step": 1282
    },
    {
      "epoch": 1.6240506329113924,
      "grad_norm": 1.4027642011642456,
      "learning_rate": 5e-05,
      "loss": 1.6008,
      "step": 1283
    },
    {
      "epoch": 1.6253164556962025,
      "grad_norm": 1.331738829612732,
      "learning_rate": 5e-05,
      "loss": 1.644,
      "step": 1284
    },
    {
      "epoch": 1.6265822784810127,
      "grad_norm": 1.3058488368988037,
      "learning_rate": 5e-05,
      "loss": 1.5288,
      "step": 1285
    },
    {
      "epoch": 1.6278481012658228,
      "grad_norm": 1.359887719154358,
      "learning_rate": 5e-05,
      "loss": 1.5676,
      "step": 1286
    },
    {
      "epoch": 1.629113924050633,
      "grad_norm": 1.3244619369506836,
      "learning_rate": 5e-05,
      "loss": 1.5311,
      "step": 1287
    },
    {
      "epoch": 1.630379746835443,
      "grad_norm": 1.3898547887802124,
      "learning_rate": 5e-05,
      "loss": 1.6783,
      "step": 1288
    },
    {
      "epoch": 1.6316455696202532,
      "grad_norm": 1.3291969299316406,
      "learning_rate": 5e-05,
      "loss": 1.5325,
      "step": 1289
    },
    {
      "epoch": 1.6329113924050633,
      "grad_norm": 1.2968201637268066,
      "learning_rate": 5e-05,
      "loss": 1.5856,
      "step": 1290
    },
    {
      "epoch": 1.6341772151898735,
      "grad_norm": 1.3304741382598877,
      "learning_rate": 5e-05,
      "loss": 1.6436,
      "step": 1291
    },
    {
      "epoch": 1.6354430379746834,
      "grad_norm": 1.336143970489502,
      "learning_rate": 5e-05,
      "loss": 1.5194,
      "step": 1292
    },
    {
      "epoch": 1.6367088607594935,
      "grad_norm": 1.297149658203125,
      "learning_rate": 5e-05,
      "loss": 1.5626,
      "step": 1293
    },
    {
      "epoch": 1.6379746835443036,
      "grad_norm": 1.3124483823776245,
      "learning_rate": 5e-05,
      "loss": 1.521,
      "step": 1294
    },
    {
      "epoch": 1.6392405063291138,
      "grad_norm": 1.3373297452926636,
      "learning_rate": 5e-05,
      "loss": 1.5728,
      "step": 1295
    },
    {
      "epoch": 1.640506329113924,
      "grad_norm": 1.3137253522872925,
      "learning_rate": 5e-05,
      "loss": 1.6002,
      "step": 1296
    },
    {
      "epoch": 1.641772151898734,
      "grad_norm": 1.3185410499572754,
      "learning_rate": 5e-05,
      "loss": 1.6074,
      "step": 1297
    },
    {
      "epoch": 1.6430379746835442,
      "grad_norm": 1.2896287441253662,
      "learning_rate": 5e-05,
      "loss": 1.4732,
      "step": 1298
    },
    {
      "epoch": 1.6443037974683543,
      "grad_norm": 1.293404459953308,
      "learning_rate": 5e-05,
      "loss": 1.5228,
      "step": 1299
    },
    {
      "epoch": 1.6455696202531644,
      "grad_norm": 1.3130123615264893,
      "learning_rate": 5e-05,
      "loss": 1.4893,
      "step": 1300
    },
    {
      "epoch": 1.6468354430379746,
      "grad_norm": 1.3494689464569092,
      "learning_rate": 5e-05,
      "loss": 1.4654,
      "step": 1301
    },
    {
      "epoch": 1.6481012658227847,
      "grad_norm": 1.3149701356887817,
      "learning_rate": 5e-05,
      "loss": 1.56,
      "step": 1302
    },
    {
      "epoch": 1.6493670886075948,
      "grad_norm": 1.341708779335022,
      "learning_rate": 5e-05,
      "loss": 1.5385,
      "step": 1303
    },
    {
      "epoch": 1.650632911392405,
      "grad_norm": 1.2917708158493042,
      "learning_rate": 5e-05,
      "loss": 1.5739,
      "step": 1304
    },
    {
      "epoch": 1.6518987341772151,
      "grad_norm": 1.2890510559082031,
      "learning_rate": 5e-05,
      "loss": 1.5384,
      "step": 1305
    },
    {
      "epoch": 1.6531645569620252,
      "grad_norm": 1.3530840873718262,
      "learning_rate": 5e-05,
      "loss": 1.5597,
      "step": 1306
    },
    {
      "epoch": 1.6544303797468354,
      "grad_norm": 1.3240630626678467,
      "learning_rate": 5e-05,
      "loss": 1.6051,
      "step": 1307
    },
    {
      "epoch": 1.6556962025316455,
      "grad_norm": 1.363995909690857,
      "learning_rate": 5e-05,
      "loss": 1.6151,
      "step": 1308
    },
    {
      "epoch": 1.6569620253164556,
      "grad_norm": 1.3227040767669678,
      "learning_rate": 5e-05,
      "loss": 1.555,
      "step": 1309
    },
    {
      "epoch": 1.6582278481012658,
      "grad_norm": 1.408970594406128,
      "learning_rate": 5e-05,
      "loss": 1.5073,
      "step": 1310
    },
    {
      "epoch": 1.659493670886076,
      "grad_norm": 1.3225055932998657,
      "learning_rate": 5e-05,
      "loss": 1.5509,
      "step": 1311
    },
    {
      "epoch": 1.660759493670886,
      "grad_norm": 1.2867830991744995,
      "learning_rate": 5e-05,
      "loss": 1.5417,
      "step": 1312
    },
    {
      "epoch": 1.6620253164556962,
      "grad_norm": 1.3303041458129883,
      "learning_rate": 5e-05,
      "loss": 1.6099,
      "step": 1313
    },
    {
      "epoch": 1.6632911392405063,
      "grad_norm": 1.319163203239441,
      "learning_rate": 5e-05,
      "loss": 1.5683,
      "step": 1314
    },
    {
      "epoch": 1.6645569620253164,
      "grad_norm": 1.3420586585998535,
      "learning_rate": 5e-05,
      "loss": 1.5517,
      "step": 1315
    },
    {
      "epoch": 1.6658227848101266,
      "grad_norm": 1.297349214553833,
      "learning_rate": 5e-05,
      "loss": 1.508,
      "step": 1316
    },
    {
      "epoch": 1.6670886075949367,
      "grad_norm": 1.349725365638733,
      "learning_rate": 5e-05,
      "loss": 1.5984,
      "step": 1317
    },
    {
      "epoch": 1.6683544303797468,
      "grad_norm": 1.3288445472717285,
      "learning_rate": 5e-05,
      "loss": 1.5269,
      "step": 1318
    },
    {
      "epoch": 1.669620253164557,
      "grad_norm": 1.3661185503005981,
      "learning_rate": 5e-05,
      "loss": 1.5672,
      "step": 1319
    },
    {
      "epoch": 1.6708860759493671,
      "grad_norm": 1.3694329261779785,
      "learning_rate": 5e-05,
      "loss": 1.5745,
      "step": 1320
    },
    {
      "epoch": 1.6721518987341772,
      "grad_norm": 1.3579370975494385,
      "learning_rate": 5e-05,
      "loss": 1.4769,
      "step": 1321
    },
    {
      "epoch": 1.6734177215189874,
      "grad_norm": 1.3064758777618408,
      "learning_rate": 5e-05,
      "loss": 1.5231,
      "step": 1322
    },
    {
      "epoch": 1.6746835443037975,
      "grad_norm": 1.348970651626587,
      "learning_rate": 5e-05,
      "loss": 1.569,
      "step": 1323
    },
    {
      "epoch": 1.6759493670886076,
      "grad_norm": 1.334113597869873,
      "learning_rate": 5e-05,
      "loss": 1.5336,
      "step": 1324
    },
    {
      "epoch": 1.6772151898734178,
      "grad_norm": 1.3183106184005737,
      "learning_rate": 5e-05,
      "loss": 1.5531,
      "step": 1325
    },
    {
      "epoch": 1.678481012658228,
      "grad_norm": 1.3549518585205078,
      "learning_rate": 5e-05,
      "loss": 1.4834,
      "step": 1326
    },
    {
      "epoch": 1.679746835443038,
      "grad_norm": 1.348029613494873,
      "learning_rate": 5e-05,
      "loss": 1.5721,
      "step": 1327
    },
    {
      "epoch": 1.6810126582278482,
      "grad_norm": 1.360108733177185,
      "learning_rate": 5e-05,
      "loss": 1.5237,
      "step": 1328
    },
    {
      "epoch": 1.6822784810126583,
      "grad_norm": 1.343801736831665,
      "learning_rate": 5e-05,
      "loss": 1.6051,
      "step": 1329
    },
    {
      "epoch": 1.6835443037974684,
      "grad_norm": 1.321012258529663,
      "learning_rate": 5e-05,
      "loss": 1.6186,
      "step": 1330
    },
    {
      "epoch": 1.6848101265822786,
      "grad_norm": 1.4195255041122437,
      "learning_rate": 5e-05,
      "loss": 1.5533,
      "step": 1331
    },
    {
      "epoch": 1.6860759493670887,
      "grad_norm": 1.3106553554534912,
      "learning_rate": 5e-05,
      "loss": 1.5696,
      "step": 1332
    },
    {
      "epoch": 1.6873417721518988,
      "grad_norm": 1.2934846878051758,
      "learning_rate": 5e-05,
      "loss": 1.5232,
      "step": 1333
    },
    {
      "epoch": 1.688607594936709,
      "grad_norm": 1.3421401977539062,
      "learning_rate": 5e-05,
      "loss": 1.5405,
      "step": 1334
    },
    {
      "epoch": 1.689873417721519,
      "grad_norm": 1.3217717409133911,
      "learning_rate": 5e-05,
      "loss": 1.5007,
      "step": 1335
    },
    {
      "epoch": 1.6911392405063292,
      "grad_norm": 1.3320715427398682,
      "learning_rate": 5e-05,
      "loss": 1.5787,
      "step": 1336
    },
    {
      "epoch": 1.6924050632911394,
      "grad_norm": 1.3415477275848389,
      "learning_rate": 5e-05,
      "loss": 1.5829,
      "step": 1337
    },
    {
      "epoch": 1.6936708860759495,
      "grad_norm": 1.2974112033843994,
      "learning_rate": 5e-05,
      "loss": 1.5366,
      "step": 1338
    },
    {
      "epoch": 1.6949367088607596,
      "grad_norm": 1.3636828660964966,
      "learning_rate": 5e-05,
      "loss": 1.5696,
      "step": 1339
    },
    {
      "epoch": 1.6962025316455698,
      "grad_norm": 1.3315891027450562,
      "learning_rate": 5e-05,
      "loss": 1.5435,
      "step": 1340
    },
    {
      "epoch": 1.69746835443038,
      "grad_norm": 1.3379267454147339,
      "learning_rate": 5e-05,
      "loss": 1.5407,
      "step": 1341
    },
    {
      "epoch": 1.6987341772151898,
      "grad_norm": 1.3385581970214844,
      "learning_rate": 5e-05,
      "loss": 1.5876,
      "step": 1342
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.3459968566894531,
      "learning_rate": 5e-05,
      "loss": 1.5535,
      "step": 1343
    },
    {
      "epoch": 1.70126582278481,
      "grad_norm": 1.3440662622451782,
      "learning_rate": 5e-05,
      "loss": 1.5009,
      "step": 1344
    },
    {
      "epoch": 1.7025316455696202,
      "grad_norm": 1.2818412780761719,
      "learning_rate": 5e-05,
      "loss": 1.5449,
      "step": 1345
    },
    {
      "epoch": 1.7037974683544304,
      "grad_norm": 1.350624680519104,
      "learning_rate": 5e-05,
      "loss": 1.5101,
      "step": 1346
    },
    {
      "epoch": 1.7050632911392405,
      "grad_norm": 1.3557605743408203,
      "learning_rate": 5e-05,
      "loss": 1.5098,
      "step": 1347
    },
    {
      "epoch": 1.7063291139240506,
      "grad_norm": 1.381259799003601,
      "learning_rate": 5e-05,
      "loss": 1.5732,
      "step": 1348
    },
    {
      "epoch": 1.7075949367088608,
      "grad_norm": 1.3296678066253662,
      "learning_rate": 5e-05,
      "loss": 1.5038,
      "step": 1349
    },
    {
      "epoch": 1.7088607594936709,
      "grad_norm": 1.3481605052947998,
      "learning_rate": 5e-05,
      "loss": 1.5832,
      "step": 1350
    },
    {
      "epoch": 1.710126582278481,
      "grad_norm": 1.318978190422058,
      "learning_rate": 5e-05,
      "loss": 1.5566,
      "step": 1351
    },
    {
      "epoch": 1.7113924050632912,
      "grad_norm": 1.3570493459701538,
      "learning_rate": 5e-05,
      "loss": 1.5262,
      "step": 1352
    },
    {
      "epoch": 1.7126582278481013,
      "grad_norm": 1.3701037168502808,
      "learning_rate": 5e-05,
      "loss": 1.5442,
      "step": 1353
    },
    {
      "epoch": 1.7139240506329114,
      "grad_norm": 1.314876675605774,
      "learning_rate": 5e-05,
      "loss": 1.5491,
      "step": 1354
    },
    {
      "epoch": 1.7151898734177216,
      "grad_norm": 1.3506008386611938,
      "learning_rate": 5e-05,
      "loss": 1.53,
      "step": 1355
    },
    {
      "epoch": 1.7164556962025317,
      "grad_norm": 1.374222993850708,
      "learning_rate": 5e-05,
      "loss": 1.5564,
      "step": 1356
    },
    {
      "epoch": 1.7177215189873418,
      "grad_norm": 1.3346097469329834,
      "learning_rate": 5e-05,
      "loss": 1.5112,
      "step": 1357
    },
    {
      "epoch": 1.7189873417721517,
      "grad_norm": 1.2852287292480469,
      "learning_rate": 5e-05,
      "loss": 1.4998,
      "step": 1358
    },
    {
      "epoch": 1.7202531645569619,
      "grad_norm": 1.363492488861084,
      "learning_rate": 5e-05,
      "loss": 1.6413,
      "step": 1359
    },
    {
      "epoch": 1.721518987341772,
      "grad_norm": 1.336820363998413,
      "learning_rate": 5e-05,
      "loss": 1.5273,
      "step": 1360
    },
    {
      "epoch": 1.7227848101265821,
      "grad_norm": 1.3457545042037964,
      "learning_rate": 5e-05,
      "loss": 1.581,
      "step": 1361
    },
    {
      "epoch": 1.7240506329113923,
      "grad_norm": 1.3173651695251465,
      "learning_rate": 5e-05,
      "loss": 1.5261,
      "step": 1362
    },
    {
      "epoch": 1.7253164556962024,
      "grad_norm": 1.2965059280395508,
      "learning_rate": 5e-05,
      "loss": 1.5435,
      "step": 1363
    },
    {
      "epoch": 1.7265822784810125,
      "grad_norm": 1.3444137573242188,
      "learning_rate": 5e-05,
      "loss": 1.5341,
      "step": 1364
    },
    {
      "epoch": 1.7278481012658227,
      "grad_norm": 1.2849286794662476,
      "learning_rate": 5e-05,
      "loss": 1.4604,
      "step": 1365
    },
    {
      "epoch": 1.7291139240506328,
      "grad_norm": 1.3117252588272095,
      "learning_rate": 5e-05,
      "loss": 1.504,
      "step": 1366
    },
    {
      "epoch": 1.730379746835443,
      "grad_norm": 1.284071683883667,
      "learning_rate": 5e-05,
      "loss": 1.4663,
      "step": 1367
    },
    {
      "epoch": 1.731645569620253,
      "grad_norm": 1.314010500907898,
      "learning_rate": 5e-05,
      "loss": 1.4852,
      "step": 1368
    },
    {
      "epoch": 1.7329113924050632,
      "grad_norm": 1.2932525873184204,
      "learning_rate": 5e-05,
      "loss": 1.5199,
      "step": 1369
    },
    {
      "epoch": 1.7341772151898733,
      "grad_norm": 1.2960970401763916,
      "learning_rate": 5e-05,
      "loss": 1.4688,
      "step": 1370
    },
    {
      "epoch": 1.7354430379746835,
      "grad_norm": 1.3250606060028076,
      "learning_rate": 5e-05,
      "loss": 1.5699,
      "step": 1371
    },
    {
      "epoch": 1.7367088607594936,
      "grad_norm": 1.3220419883728027,
      "learning_rate": 5e-05,
      "loss": 1.5235,
      "step": 1372
    },
    {
      "epoch": 1.7379746835443037,
      "grad_norm": 1.2969719171524048,
      "learning_rate": 5e-05,
      "loss": 1.4927,
      "step": 1373
    },
    {
      "epoch": 1.7392405063291139,
      "grad_norm": 1.3017241954803467,
      "learning_rate": 5e-05,
      "loss": 1.4848,
      "step": 1374
    },
    {
      "epoch": 1.740506329113924,
      "grad_norm": 1.2814054489135742,
      "learning_rate": 5e-05,
      "loss": 1.4236,
      "step": 1375
    },
    {
      "epoch": 1.7417721518987341,
      "grad_norm": 1.3115233182907104,
      "learning_rate": 5e-05,
      "loss": 1.5403,
      "step": 1376
    },
    {
      "epoch": 1.7430379746835443,
      "grad_norm": 1.320846676826477,
      "learning_rate": 5e-05,
      "loss": 1.5294,
      "step": 1377
    },
    {
      "epoch": 1.7443037974683544,
      "grad_norm": 1.300004243850708,
      "learning_rate": 5e-05,
      "loss": 1.5015,
      "step": 1378
    },
    {
      "epoch": 1.7455696202531645,
      "grad_norm": 1.3478244543075562,
      "learning_rate": 5e-05,
      "loss": 1.5137,
      "step": 1379
    },
    {
      "epoch": 1.7468354430379747,
      "grad_norm": 1.3655099868774414,
      "learning_rate": 5e-05,
      "loss": 1.5906,
      "step": 1380
    },
    {
      "epoch": 1.7481012658227848,
      "grad_norm": 1.2859933376312256,
      "learning_rate": 5e-05,
      "loss": 1.5949,
      "step": 1381
    },
    {
      "epoch": 1.749367088607595,
      "grad_norm": 1.3017157316207886,
      "learning_rate": 5e-05,
      "loss": 1.5377,
      "step": 1382
    },
    {
      "epoch": 1.750632911392405,
      "grad_norm": 1.3224440813064575,
      "learning_rate": 5e-05,
      "loss": 1.5673,
      "step": 1383
    },
    {
      "epoch": 1.7518987341772152,
      "grad_norm": 1.330348253250122,
      "learning_rate": 5e-05,
      "loss": 1.4857,
      "step": 1384
    },
    {
      "epoch": 1.7531645569620253,
      "grad_norm": 1.3344025611877441,
      "learning_rate": 5e-05,
      "loss": 1.5166,
      "step": 1385
    },
    {
      "epoch": 1.7544303797468355,
      "grad_norm": 1.3132754564285278,
      "learning_rate": 5e-05,
      "loss": 1.5029,
      "step": 1386
    },
    {
      "epoch": 1.7556962025316456,
      "grad_norm": 1.3129360675811768,
      "learning_rate": 5e-05,
      "loss": 1.4834,
      "step": 1387
    },
    {
      "epoch": 1.7569620253164557,
      "grad_norm": 1.3889423608779907,
      "learning_rate": 5e-05,
      "loss": 1.4801,
      "step": 1388
    },
    {
      "epoch": 1.7582278481012659,
      "grad_norm": 1.313030481338501,
      "learning_rate": 5e-05,
      "loss": 1.4338,
      "step": 1389
    },
    {
      "epoch": 1.759493670886076,
      "grad_norm": 1.3259822130203247,
      "learning_rate": 5e-05,
      "loss": 1.4968,
      "step": 1390
    },
    {
      "epoch": 1.7607594936708861,
      "grad_norm": 1.3701318502426147,
      "learning_rate": 5e-05,
      "loss": 1.5686,
      "step": 1391
    },
    {
      "epoch": 1.7620253164556963,
      "grad_norm": 1.3139469623565674,
      "learning_rate": 5e-05,
      "loss": 1.5032,
      "step": 1392
    },
    {
      "epoch": 1.7632911392405064,
      "grad_norm": 1.3763664960861206,
      "learning_rate": 5e-05,
      "loss": 1.5838,
      "step": 1393
    },
    {
      "epoch": 1.7645569620253165,
      "grad_norm": 1.3652926683425903,
      "learning_rate": 5e-05,
      "loss": 1.5831,
      "step": 1394
    },
    {
      "epoch": 1.7658227848101267,
      "grad_norm": 1.312060832977295,
      "learning_rate": 5e-05,
      "loss": 1.4526,
      "step": 1395
    },
    {
      "epoch": 1.7670886075949368,
      "grad_norm": 1.320114254951477,
      "learning_rate": 5e-05,
      "loss": 1.4717,
      "step": 1396
    },
    {
      "epoch": 1.768354430379747,
      "grad_norm": 1.2740412950515747,
      "learning_rate": 5e-05,
      "loss": 1.4466,
      "step": 1397
    },
    {
      "epoch": 1.769620253164557,
      "grad_norm": 1.2683414220809937,
      "learning_rate": 5e-05,
      "loss": 1.446,
      "step": 1398
    },
    {
      "epoch": 1.7708860759493672,
      "grad_norm": 1.3184019327163696,
      "learning_rate": 5e-05,
      "loss": 1.4975,
      "step": 1399
    },
    {
      "epoch": 1.7721518987341773,
      "grad_norm": 1.334678292274475,
      "learning_rate": 5e-05,
      "loss": 1.5083,
      "step": 1400
    },
    {
      "epoch": 1.7734177215189875,
      "grad_norm": 1.3420473337173462,
      "learning_rate": 5e-05,
      "loss": 1.47,
      "step": 1401
    },
    {
      "epoch": 1.7746835443037976,
      "grad_norm": 1.3212779760360718,
      "learning_rate": 5e-05,
      "loss": 1.5469,
      "step": 1402
    },
    {
      "epoch": 1.7759493670886077,
      "grad_norm": 1.3663287162780762,
      "learning_rate": 5e-05,
      "loss": 1.6186,
      "step": 1403
    },
    {
      "epoch": 1.7772151898734179,
      "grad_norm": 1.3244974613189697,
      "learning_rate": 5e-05,
      "loss": 1.4978,
      "step": 1404
    },
    {
      "epoch": 1.778481012658228,
      "grad_norm": 1.3239885568618774,
      "learning_rate": 5e-05,
      "loss": 1.4998,
      "step": 1405
    },
    {
      "epoch": 1.7797468354430381,
      "grad_norm": 1.3295713663101196,
      "learning_rate": 5e-05,
      "loss": 1.469,
      "step": 1406
    },
    {
      "epoch": 1.7810126582278483,
      "grad_norm": 1.3320459127426147,
      "learning_rate": 5e-05,
      "loss": 1.4301,
      "step": 1407
    },
    {
      "epoch": 1.7822784810126582,
      "grad_norm": 1.3424358367919922,
      "learning_rate": 5e-05,
      "loss": 1.5237,
      "step": 1408
    },
    {
      "epoch": 1.7835443037974683,
      "grad_norm": 1.3493974208831787,
      "learning_rate": 5e-05,
      "loss": 1.5797,
      "step": 1409
    },
    {
      "epoch": 1.7848101265822784,
      "grad_norm": 1.3161247968673706,
      "learning_rate": 5e-05,
      "loss": 1.5588,
      "step": 1410
    },
    {
      "epoch": 1.7860759493670886,
      "grad_norm": 1.2811673879623413,
      "learning_rate": 5e-05,
      "loss": 1.4781,
      "step": 1411
    },
    {
      "epoch": 1.7873417721518987,
      "grad_norm": 1.3830467462539673,
      "learning_rate": 5e-05,
      "loss": 1.4933,
      "step": 1412
    },
    {
      "epoch": 1.7886075949367088,
      "grad_norm": 1.321177363395691,
      "learning_rate": 5e-05,
      "loss": 1.4985,
      "step": 1413
    },
    {
      "epoch": 1.789873417721519,
      "grad_norm": 1.3067494630813599,
      "learning_rate": 5e-05,
      "loss": 1.5351,
      "step": 1414
    },
    {
      "epoch": 1.7911392405063291,
      "grad_norm": 1.3148066997528076,
      "learning_rate": 5e-05,
      "loss": 1.447,
      "step": 1415
    },
    {
      "epoch": 1.7924050632911392,
      "grad_norm": 1.3033945560455322,
      "learning_rate": 5e-05,
      "loss": 1.4636,
      "step": 1416
    },
    {
      "epoch": 1.7936708860759494,
      "grad_norm": 1.334314227104187,
      "learning_rate": 5e-05,
      "loss": 1.5416,
      "step": 1417
    },
    {
      "epoch": 1.7949367088607595,
      "grad_norm": 1.3419044017791748,
      "learning_rate": 5e-05,
      "loss": 1.5116,
      "step": 1418
    },
    {
      "epoch": 1.7962025316455696,
      "grad_norm": 1.3436264991760254,
      "learning_rate": 5e-05,
      "loss": 1.5608,
      "step": 1419
    },
    {
      "epoch": 1.7974683544303798,
      "grad_norm": 1.2950623035430908,
      "learning_rate": 5e-05,
      "loss": 1.4812,
      "step": 1420
    },
    {
      "epoch": 1.79873417721519,
      "grad_norm": 1.3450647592544556,
      "learning_rate": 5e-05,
      "loss": 1.5358,
      "step": 1421
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.3413639068603516,
      "learning_rate": 5e-05,
      "loss": 1.5338,
      "step": 1422
    },
    {
      "epoch": 1.8012658227848102,
      "grad_norm": 1.3359315395355225,
      "learning_rate": 5e-05,
      "loss": 1.4849,
      "step": 1423
    },
    {
      "epoch": 1.80253164556962,
      "grad_norm": 1.3251858949661255,
      "learning_rate": 5e-05,
      "loss": 1.4636,
      "step": 1424
    },
    {
      "epoch": 1.8037974683544302,
      "grad_norm": 1.2913521528244019,
      "learning_rate": 5e-05,
      "loss": 1.4706,
      "step": 1425
    },
    {
      "epoch": 1.8050632911392404,
      "grad_norm": 1.3455848693847656,
      "learning_rate": 5e-05,
      "loss": 1.478,
      "step": 1426
    },
    {
      "epoch": 1.8063291139240505,
      "grad_norm": 1.2952871322631836,
      "learning_rate": 5e-05,
      "loss": 1.4578,
      "step": 1427
    },
    {
      "epoch": 1.8075949367088606,
      "grad_norm": 1.3531650304794312,
      "learning_rate": 5e-05,
      "loss": 1.548,
      "step": 1428
    },
    {
      "epoch": 1.8088607594936708,
      "grad_norm": 1.3451193571090698,
      "learning_rate": 5e-05,
      "loss": 1.548,
      "step": 1429
    },
    {
      "epoch": 1.810126582278481,
      "grad_norm": 1.2893853187561035,
      "learning_rate": 5e-05,
      "loss": 1.4901,
      "step": 1430
    },
    {
      "epoch": 1.811392405063291,
      "grad_norm": 1.313612461090088,
      "learning_rate": 5e-05,
      "loss": 1.5422,
      "step": 1431
    },
    {
      "epoch": 1.8126582278481012,
      "grad_norm": 1.3198919296264648,
      "learning_rate": 5e-05,
      "loss": 1.4991,
      "step": 1432
    },
    {
      "epoch": 1.8139240506329113,
      "grad_norm": 1.2978249788284302,
      "learning_rate": 5e-05,
      "loss": 1.3985,
      "step": 1433
    },
    {
      "epoch": 1.8151898734177214,
      "grad_norm": 1.3011478185653687,
      "learning_rate": 5e-05,
      "loss": 1.4404,
      "step": 1434
    },
    {
      "epoch": 1.8164556962025316,
      "grad_norm": 1.2982789278030396,
      "learning_rate": 5e-05,
      "loss": 1.4485,
      "step": 1435
    },
    {
      "epoch": 1.8177215189873417,
      "grad_norm": 1.3477262258529663,
      "learning_rate": 5e-05,
      "loss": 1.5187,
      "step": 1436
    },
    {
      "epoch": 1.8189873417721518,
      "grad_norm": 1.3070162534713745,
      "learning_rate": 5e-05,
      "loss": 1.4346,
      "step": 1437
    },
    {
      "epoch": 1.820253164556962,
      "grad_norm": 1.3147634267807007,
      "learning_rate": 5e-05,
      "loss": 1.4853,
      "step": 1438
    },
    {
      "epoch": 1.821518987341772,
      "grad_norm": 1.3860602378845215,
      "learning_rate": 5e-05,
      "loss": 1.5922,
      "step": 1439
    },
    {
      "epoch": 1.8227848101265822,
      "grad_norm": 1.3039363622665405,
      "learning_rate": 5e-05,
      "loss": 1.4911,
      "step": 1440
    },
    {
      "epoch": 1.8240506329113924,
      "grad_norm": 1.2857670783996582,
      "learning_rate": 5e-05,
      "loss": 1.4406,
      "step": 1441
    },
    {
      "epoch": 1.8253164556962025,
      "grad_norm": 1.3499974012374878,
      "learning_rate": 5e-05,
      "loss": 1.5409,
      "step": 1442
    },
    {
      "epoch": 1.8265822784810126,
      "grad_norm": 1.312778115272522,
      "learning_rate": 5e-05,
      "loss": 1.4788,
      "step": 1443
    },
    {
      "epoch": 1.8278481012658228,
      "grad_norm": 1.33233642578125,
      "learning_rate": 5e-05,
      "loss": 1.4774,
      "step": 1444
    },
    {
      "epoch": 1.8291139240506329,
      "grad_norm": 1.314834713935852,
      "learning_rate": 5e-05,
      "loss": 1.5404,
      "step": 1445
    },
    {
      "epoch": 1.830379746835443,
      "grad_norm": 1.351841688156128,
      "learning_rate": 5e-05,
      "loss": 1.5057,
      "step": 1446
    },
    {
      "epoch": 1.8316455696202532,
      "grad_norm": 1.3633112907409668,
      "learning_rate": 5e-05,
      "loss": 1.5818,
      "step": 1447
    },
    {
      "epoch": 1.8329113924050633,
      "grad_norm": 1.3091570138931274,
      "learning_rate": 5e-05,
      "loss": 1.4875,
      "step": 1448
    },
    {
      "epoch": 1.8341772151898734,
      "grad_norm": 1.3525421619415283,
      "learning_rate": 5e-05,
      "loss": 1.5005,
      "step": 1449
    },
    {
      "epoch": 1.8354430379746836,
      "grad_norm": 1.292344570159912,
      "learning_rate": 5e-05,
      "loss": 1.4631,
      "step": 1450
    },
    {
      "epoch": 1.8367088607594937,
      "grad_norm": 1.3774949312210083,
      "learning_rate": 5e-05,
      "loss": 1.4748,
      "step": 1451
    },
    {
      "epoch": 1.8379746835443038,
      "grad_norm": 1.317517876625061,
      "learning_rate": 5e-05,
      "loss": 1.5213,
      "step": 1452
    },
    {
      "epoch": 1.839240506329114,
      "grad_norm": 1.2903445959091187,
      "learning_rate": 5e-05,
      "loss": 1.5338,
      "step": 1453
    },
    {
      "epoch": 1.840506329113924,
      "grad_norm": 1.3181471824645996,
      "learning_rate": 5e-05,
      "loss": 1.4235,
      "step": 1454
    },
    {
      "epoch": 1.8417721518987342,
      "grad_norm": 1.3432506322860718,
      "learning_rate": 5e-05,
      "loss": 1.5314,
      "step": 1455
    },
    {
      "epoch": 1.8430379746835444,
      "grad_norm": 1.3573353290557861,
      "learning_rate": 5e-05,
      "loss": 1.4905,
      "step": 1456
    },
    {
      "epoch": 1.8443037974683545,
      "grad_norm": 1.3316348791122437,
      "learning_rate": 5e-05,
      "loss": 1.4995,
      "step": 1457
    },
    {
      "epoch": 1.8455696202531646,
      "grad_norm": 1.3110032081604004,
      "learning_rate": 5e-05,
      "loss": 1.5275,
      "step": 1458
    },
    {
      "epoch": 1.8468354430379748,
      "grad_norm": 1.2762043476104736,
      "learning_rate": 5e-05,
      "loss": 1.4101,
      "step": 1459
    },
    {
      "epoch": 1.8481012658227849,
      "grad_norm": 1.302536129951477,
      "learning_rate": 5e-05,
      "loss": 1.5253,
      "step": 1460
    },
    {
      "epoch": 1.849367088607595,
      "grad_norm": 1.3005794286727905,
      "learning_rate": 5e-05,
      "loss": 1.4279,
      "step": 1461
    },
    {
      "epoch": 1.8506329113924052,
      "grad_norm": 1.2894881963729858,
      "learning_rate": 5e-05,
      "loss": 1.4843,
      "step": 1462
    },
    {
      "epoch": 1.8518987341772153,
      "grad_norm": 1.2998579740524292,
      "learning_rate": 5e-05,
      "loss": 1.4335,
      "step": 1463
    },
    {
      "epoch": 1.8531645569620254,
      "grad_norm": 1.2954919338226318,
      "learning_rate": 5e-05,
      "loss": 1.4312,
      "step": 1464
    },
    {
      "epoch": 1.8544303797468356,
      "grad_norm": 1.2535310983657837,
      "learning_rate": 5e-05,
      "loss": 1.4447,
      "step": 1465
    },
    {
      "epoch": 1.8556962025316457,
      "grad_norm": 1.3476420640945435,
      "learning_rate": 5e-05,
      "loss": 1.539,
      "step": 1466
    },
    {
      "epoch": 1.8569620253164558,
      "grad_norm": 1.3253705501556396,
      "learning_rate": 5e-05,
      "loss": 1.5091,
      "step": 1467
    },
    {
      "epoch": 1.858227848101266,
      "grad_norm": 1.3133829832077026,
      "learning_rate": 5e-05,
      "loss": 1.533,
      "step": 1468
    },
    {
      "epoch": 1.859493670886076,
      "grad_norm": 1.2843761444091797,
      "learning_rate": 5e-05,
      "loss": 1.4762,
      "step": 1469
    },
    {
      "epoch": 1.8607594936708862,
      "grad_norm": 1.3146390914916992,
      "learning_rate": 5e-05,
      "loss": 1.4763,
      "step": 1470
    },
    {
      "epoch": 1.8620253164556964,
      "grad_norm": 1.3030922412872314,
      "learning_rate": 5e-05,
      "loss": 1.4115,
      "step": 1471
    },
    {
      "epoch": 1.8632911392405065,
      "grad_norm": 1.2841509580612183,
      "learning_rate": 5e-05,
      "loss": 1.4133,
      "step": 1472
    },
    {
      "epoch": 1.8645569620253166,
      "grad_norm": 1.2962274551391602,
      "learning_rate": 5e-05,
      "loss": 1.5145,
      "step": 1473
    },
    {
      "epoch": 1.8658227848101265,
      "grad_norm": 1.3329801559448242,
      "learning_rate": 5e-05,
      "loss": 1.4254,
      "step": 1474
    },
    {
      "epoch": 1.8670886075949367,
      "grad_norm": 1.313953161239624,
      "learning_rate": 5e-05,
      "loss": 1.4648,
      "step": 1475
    },
    {
      "epoch": 1.8683544303797468,
      "grad_norm": 1.3164671659469604,
      "learning_rate": 5e-05,
      "loss": 1.4792,
      "step": 1476
    },
    {
      "epoch": 1.869620253164557,
      "grad_norm": 1.3144915103912354,
      "learning_rate": 5e-05,
      "loss": 1.4959,
      "step": 1477
    },
    {
      "epoch": 1.870886075949367,
      "grad_norm": 1.3484175205230713,
      "learning_rate": 5e-05,
      "loss": 1.5188,
      "step": 1478
    },
    {
      "epoch": 1.8721518987341772,
      "grad_norm": 1.2827513217926025,
      "learning_rate": 5e-05,
      "loss": 1.4098,
      "step": 1479
    },
    {
      "epoch": 1.8734177215189873,
      "grad_norm": 1.303314208984375,
      "learning_rate": 5e-05,
      "loss": 1.4374,
      "step": 1480
    },
    {
      "epoch": 1.8746835443037975,
      "grad_norm": 1.3743311166763306,
      "learning_rate": 5e-05,
      "loss": 1.5241,
      "step": 1481
    },
    {
      "epoch": 1.8759493670886076,
      "grad_norm": 1.3329157829284668,
      "learning_rate": 5e-05,
      "loss": 1.4762,
      "step": 1482
    },
    {
      "epoch": 1.8772151898734177,
      "grad_norm": 1.353574514389038,
      "learning_rate": 5e-05,
      "loss": 1.5144,
      "step": 1483
    },
    {
      "epoch": 1.8784810126582279,
      "grad_norm": 1.3269435167312622,
      "learning_rate": 5e-05,
      "loss": 1.4395,
      "step": 1484
    },
    {
      "epoch": 1.879746835443038,
      "grad_norm": 1.346451997756958,
      "learning_rate": 5e-05,
      "loss": 1.4528,
      "step": 1485
    },
    {
      "epoch": 1.8810126582278481,
      "grad_norm": 1.3076423406600952,
      "learning_rate": 5e-05,
      "loss": 1.4849,
      "step": 1486
    },
    {
      "epoch": 1.8822784810126583,
      "grad_norm": 1.305317759513855,
      "learning_rate": 5e-05,
      "loss": 1.4078,
      "step": 1487
    },
    {
      "epoch": 1.8835443037974684,
      "grad_norm": 1.3229236602783203,
      "learning_rate": 5e-05,
      "loss": 1.5215,
      "step": 1488
    },
    {
      "epoch": 1.8848101265822785,
      "grad_norm": 1.2935765981674194,
      "learning_rate": 5e-05,
      "loss": 1.406,
      "step": 1489
    },
    {
      "epoch": 1.8860759493670884,
      "grad_norm": 1.3068737983703613,
      "learning_rate": 5e-05,
      "loss": 1.4971,
      "step": 1490
    },
    {
      "epoch": 1.8873417721518986,
      "grad_norm": 1.2873610258102417,
      "learning_rate": 5e-05,
      "loss": 1.4602,
      "step": 1491
    },
    {
      "epoch": 1.8886075949367087,
      "grad_norm": 1.3190094232559204,
      "learning_rate": 5e-05,
      "loss": 1.4732,
      "step": 1492
    },
    {
      "epoch": 1.8898734177215188,
      "grad_norm": 1.3559913635253906,
      "learning_rate": 5e-05,
      "loss": 1.456,
      "step": 1493
    },
    {
      "epoch": 1.891139240506329,
      "grad_norm": 1.3323049545288086,
      "learning_rate": 5e-05,
      "loss": 1.4405,
      "step": 1494
    },
    {
      "epoch": 1.8924050632911391,
      "grad_norm": 1.306809663772583,
      "learning_rate": 5e-05,
      "loss": 1.4868,
      "step": 1495
    },
    {
      "epoch": 1.8936708860759492,
      "grad_norm": 1.3180060386657715,
      "learning_rate": 5e-05,
      "loss": 1.4561,
      "step": 1496
    },
    {
      "epoch": 1.8949367088607594,
      "grad_norm": 1.363189935684204,
      "learning_rate": 5e-05,
      "loss": 1.5055,
      "step": 1497
    },
    {
      "epoch": 1.8962025316455695,
      "grad_norm": 1.3363447189331055,
      "learning_rate": 5e-05,
      "loss": 1.5137,
      "step": 1498
    },
    {
      "epoch": 1.8974683544303796,
      "grad_norm": 1.3613518476486206,
      "learning_rate": 5e-05,
      "loss": 1.5225,
      "step": 1499
    },
    {
      "epoch": 1.8987341772151898,
      "grad_norm": 1.3430380821228027,
      "learning_rate": 5e-05,
      "loss": 1.4702,
      "step": 1500
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.3367762565612793,
      "learning_rate": 5e-05,
      "loss": 1.4794,
      "step": 1501
    },
    {
      "epoch": 1.90126582278481,
      "grad_norm": 1.3667360544204712,
      "learning_rate": 5e-05,
      "loss": 1.4941,
      "step": 1502
    },
    {
      "epoch": 1.9025316455696202,
      "grad_norm": 1.3369097709655762,
      "learning_rate": 5e-05,
      "loss": 1.4416,
      "step": 1503
    },
    {
      "epoch": 1.9037974683544303,
      "grad_norm": 1.330741286277771,
      "learning_rate": 5e-05,
      "loss": 1.4921,
      "step": 1504
    },
    {
      "epoch": 1.9050632911392404,
      "grad_norm": 1.3425697088241577,
      "learning_rate": 5e-05,
      "loss": 1.5088,
      "step": 1505
    },
    {
      "epoch": 1.9063291139240506,
      "grad_norm": 1.3631106615066528,
      "learning_rate": 5e-05,
      "loss": 1.4907,
      "step": 1506
    },
    {
      "epoch": 1.9075949367088607,
      "grad_norm": 1.322445273399353,
      "learning_rate": 5e-05,
      "loss": 1.362,
      "step": 1507
    },
    {
      "epoch": 1.9088607594936708,
      "grad_norm": 1.3243833780288696,
      "learning_rate": 5e-05,
      "loss": 1.5402,
      "step": 1508
    },
    {
      "epoch": 1.910126582278481,
      "grad_norm": 1.3765997886657715,
      "learning_rate": 5e-05,
      "loss": 1.4698,
      "step": 1509
    },
    {
      "epoch": 1.9113924050632911,
      "grad_norm": 1.3258566856384277,
      "learning_rate": 5e-05,
      "loss": 1.5007,
      "step": 1510
    },
    {
      "epoch": 1.9126582278481012,
      "grad_norm": 1.3285462856292725,
      "learning_rate": 5e-05,
      "loss": 1.4595,
      "step": 1511
    },
    {
      "epoch": 1.9139240506329114,
      "grad_norm": 1.3554273843765259,
      "learning_rate": 5e-05,
      "loss": 1.4844,
      "step": 1512
    },
    {
      "epoch": 1.9151898734177215,
      "grad_norm": 1.2993031740188599,
      "learning_rate": 5e-05,
      "loss": 1.4026,
      "step": 1513
    },
    {
      "epoch": 1.9164556962025316,
      "grad_norm": 1.3200089931488037,
      "learning_rate": 5e-05,
      "loss": 1.5202,
      "step": 1514
    },
    {
      "epoch": 1.9177215189873418,
      "grad_norm": 1.3062561750411987,
      "learning_rate": 5e-05,
      "loss": 1.4719,
      "step": 1515
    },
    {
      "epoch": 1.918987341772152,
      "grad_norm": 1.3408117294311523,
      "learning_rate": 5e-05,
      "loss": 1.4935,
      "step": 1516
    },
    {
      "epoch": 1.920253164556962,
      "grad_norm": 1.340083360671997,
      "learning_rate": 5e-05,
      "loss": 1.4423,
      "step": 1517
    },
    {
      "epoch": 1.9215189873417722,
      "grad_norm": 1.2816967964172363,
      "learning_rate": 5e-05,
      "loss": 1.3782,
      "step": 1518
    },
    {
      "epoch": 1.9227848101265823,
      "grad_norm": 1.3020950555801392,
      "learning_rate": 5e-05,
      "loss": 1.4454,
      "step": 1519
    },
    {
      "epoch": 1.9240506329113924,
      "grad_norm": 1.3194458484649658,
      "learning_rate": 5e-05,
      "loss": 1.4617,
      "step": 1520
    },
    {
      "epoch": 1.9253164556962026,
      "grad_norm": 1.3314348459243774,
      "learning_rate": 5e-05,
      "loss": 1.4984,
      "step": 1521
    },
    {
      "epoch": 1.9265822784810127,
      "grad_norm": 1.3287277221679688,
      "learning_rate": 5e-05,
      "loss": 1.4771,
      "step": 1522
    },
    {
      "epoch": 1.9278481012658228,
      "grad_norm": 1.3033702373504639,
      "learning_rate": 5e-05,
      "loss": 1.4047,
      "step": 1523
    },
    {
      "epoch": 1.929113924050633,
      "grad_norm": 1.3156404495239258,
      "learning_rate": 5e-05,
      "loss": 1.4768,
      "step": 1524
    },
    {
      "epoch": 1.9303797468354431,
      "grad_norm": 1.340369701385498,
      "learning_rate": 5e-05,
      "loss": 1.4824,
      "step": 1525
    },
    {
      "epoch": 1.9316455696202532,
      "grad_norm": 1.3341480493545532,
      "learning_rate": 5e-05,
      "loss": 1.5248,
      "step": 1526
    },
    {
      "epoch": 1.9329113924050634,
      "grad_norm": 1.2872978448867798,
      "learning_rate": 5e-05,
      "loss": 1.4681,
      "step": 1527
    },
    {
      "epoch": 1.9341772151898735,
      "grad_norm": 1.3293050527572632,
      "learning_rate": 5e-05,
      "loss": 1.4513,
      "step": 1528
    },
    {
      "epoch": 1.9354430379746836,
      "grad_norm": 1.2618480920791626,
      "learning_rate": 5e-05,
      "loss": 1.4079,
      "step": 1529
    },
    {
      "epoch": 1.9367088607594938,
      "grad_norm": 1.3105077743530273,
      "learning_rate": 5e-05,
      "loss": 1.4472,
      "step": 1530
    },
    {
      "epoch": 1.937974683544304,
      "grad_norm": 1.2824187278747559,
      "learning_rate": 5e-05,
      "loss": 1.4504,
      "step": 1531
    },
    {
      "epoch": 1.939240506329114,
      "grad_norm": 1.3429498672485352,
      "learning_rate": 5e-05,
      "loss": 1.4017,
      "step": 1532
    },
    {
      "epoch": 1.9405063291139242,
      "grad_norm": 1.332999348640442,
      "learning_rate": 5e-05,
      "loss": 1.508,
      "step": 1533
    },
    {
      "epoch": 1.9417721518987343,
      "grad_norm": 1.3441494703292847,
      "learning_rate": 5e-05,
      "loss": 1.4533,
      "step": 1534
    },
    {
      "epoch": 1.9430379746835444,
      "grad_norm": 1.3235501050949097,
      "learning_rate": 5e-05,
      "loss": 1.4047,
      "step": 1535
    },
    {
      "epoch": 1.9443037974683546,
      "grad_norm": 1.35102117061615,
      "learning_rate": 5e-05,
      "loss": 1.4814,
      "step": 1536
    },
    {
      "epoch": 1.9455696202531647,
      "grad_norm": 1.3387256860733032,
      "learning_rate": 5e-05,
      "loss": 1.5004,
      "step": 1537
    },
    {
      "epoch": 1.9468354430379748,
      "grad_norm": 1.2553701400756836,
      "learning_rate": 5e-05,
      "loss": 1.3773,
      "step": 1538
    },
    {
      "epoch": 1.9481012658227848,
      "grad_norm": 1.3757245540618896,
      "learning_rate": 5e-05,
      "loss": 1.4631,
      "step": 1539
    },
    {
      "epoch": 1.9493670886075949,
      "grad_norm": 1.343019723892212,
      "learning_rate": 5e-05,
      "loss": 1.4332,
      "step": 1540
    },
    {
      "epoch": 1.950632911392405,
      "grad_norm": 1.3195191621780396,
      "learning_rate": 5e-05,
      "loss": 1.4362,
      "step": 1541
    },
    {
      "epoch": 1.9518987341772152,
      "grad_norm": 1.359379768371582,
      "learning_rate": 5e-05,
      "loss": 1.5118,
      "step": 1542
    },
    {
      "epoch": 1.9531645569620253,
      "grad_norm": 1.3408128023147583,
      "learning_rate": 5e-05,
      "loss": 1.5508,
      "step": 1543
    },
    {
      "epoch": 1.9544303797468354,
      "grad_norm": 1.3185832500457764,
      "learning_rate": 5e-05,
      "loss": 1.5013,
      "step": 1544
    },
    {
      "epoch": 1.9556962025316456,
      "grad_norm": 1.3572890758514404,
      "learning_rate": 5e-05,
      "loss": 1.4967,
      "step": 1545
    },
    {
      "epoch": 1.9569620253164557,
      "grad_norm": 1.327102541923523,
      "learning_rate": 5e-05,
      "loss": 1.4652,
      "step": 1546
    },
    {
      "epoch": 1.9582278481012658,
      "grad_norm": 1.289896011352539,
      "learning_rate": 5e-05,
      "loss": 1.376,
      "step": 1547
    },
    {
      "epoch": 1.959493670886076,
      "grad_norm": 1.301171064376831,
      "learning_rate": 5e-05,
      "loss": 1.4637,
      "step": 1548
    },
    {
      "epoch": 1.960759493670886,
      "grad_norm": 1.3127769231796265,
      "learning_rate": 5e-05,
      "loss": 1.5179,
      "step": 1549
    },
    {
      "epoch": 1.9620253164556962,
      "grad_norm": 1.3013403415679932,
      "learning_rate": 5e-05,
      "loss": 1.4596,
      "step": 1550
    },
    {
      "epoch": 1.9632911392405064,
      "grad_norm": 1.3278416395187378,
      "learning_rate": 5e-05,
      "loss": 1.4426,
      "step": 1551
    },
    {
      "epoch": 1.9645569620253165,
      "grad_norm": 1.308768630027771,
      "learning_rate": 5e-05,
      "loss": 1.4746,
      "step": 1552
    },
    {
      "epoch": 1.9658227848101266,
      "grad_norm": 1.3282127380371094,
      "learning_rate": 5e-05,
      "loss": 1.4624,
      "step": 1553
    },
    {
      "epoch": 1.9670886075949368,
      "grad_norm": 1.3066444396972656,
      "learning_rate": 5e-05,
      "loss": 1.407,
      "step": 1554
    },
    {
      "epoch": 1.9683544303797469,
      "grad_norm": 1.3695111274719238,
      "learning_rate": 5e-05,
      "loss": 1.4917,
      "step": 1555
    },
    {
      "epoch": 1.9696202531645568,
      "grad_norm": 1.3018447160720825,
      "learning_rate": 5e-05,
      "loss": 1.3974,
      "step": 1556
    },
    {
      "epoch": 1.970886075949367,
      "grad_norm": 1.3111621141433716,
      "learning_rate": 5e-05,
      "loss": 1.4495,
      "step": 1557
    },
    {
      "epoch": 1.972151898734177,
      "grad_norm": 1.318389654159546,
      "learning_rate": 5e-05,
      "loss": 1.4401,
      "step": 1558
    },
    {
      "epoch": 1.9734177215189872,
      "grad_norm": 1.373140811920166,
      "learning_rate": 5e-05,
      "loss": 1.4587,
      "step": 1559
    },
    {
      "epoch": 1.9746835443037973,
      "grad_norm": 1.3185912370681763,
      "learning_rate": 5e-05,
      "loss": 1.4481,
      "step": 1560
    },
    {
      "epoch": 1.9759493670886075,
      "grad_norm": 1.299606204032898,
      "learning_rate": 5e-05,
      "loss": 1.3754,
      "step": 1561
    },
    {
      "epoch": 1.9772151898734176,
      "grad_norm": 1.3330947160720825,
      "learning_rate": 5e-05,
      "loss": 1.4538,
      "step": 1562
    },
    {
      "epoch": 1.9784810126582277,
      "grad_norm": 1.386576533317566,
      "learning_rate": 5e-05,
      "loss": 1.5118,
      "step": 1563
    },
    {
      "epoch": 1.9797468354430379,
      "grad_norm": 1.3140298128128052,
      "learning_rate": 5e-05,
      "loss": 1.4546,
      "step": 1564
    },
    {
      "epoch": 1.981012658227848,
      "grad_norm": 1.3687975406646729,
      "learning_rate": 5e-05,
      "loss": 1.4549,
      "step": 1565
    },
    {
      "epoch": 1.9822784810126581,
      "grad_norm": 1.3458952903747559,
      "learning_rate": 5e-05,
      "loss": 1.4877,
      "step": 1566
    },
    {
      "epoch": 1.9835443037974683,
      "grad_norm": 1.3551661968231201,
      "learning_rate": 5e-05,
      "loss": 1.5098,
      "step": 1567
    },
    {
      "epoch": 1.9848101265822784,
      "grad_norm": 1.318223476409912,
      "learning_rate": 5e-05,
      "loss": 1.3758,
      "step": 1568
    },
    {
      "epoch": 1.9860759493670885,
      "grad_norm": 1.3182810544967651,
      "learning_rate": 5e-05,
      "loss": 1.452,
      "step": 1569
    },
    {
      "epoch": 1.9873417721518987,
      "grad_norm": 1.3031506538391113,
      "learning_rate": 5e-05,
      "loss": 1.4678,
      "step": 1570
    },
    {
      "epoch": 1.9886075949367088,
      "grad_norm": 1.3579497337341309,
      "learning_rate": 5e-05,
      "loss": 1.4443,
      "step": 1571
    },
    {
      "epoch": 1.989873417721519,
      "grad_norm": 1.357120394706726,
      "learning_rate": 5e-05,
      "loss": 1.4382,
      "step": 1572
    },
    {
      "epoch": 1.991139240506329,
      "grad_norm": 1.349879503250122,
      "learning_rate": 5e-05,
      "loss": 1.5046,
      "step": 1573
    },
    {
      "epoch": 1.9924050632911392,
      "grad_norm": 1.2956135272979736,
      "learning_rate": 5e-05,
      "loss": 1.4274,
      "step": 1574
    },
    {
      "epoch": 1.9936708860759493,
      "grad_norm": 1.292876958847046,
      "learning_rate": 5e-05,
      "loss": 1.3822,
      "step": 1575
    },
    {
      "epoch": 1.9949367088607595,
      "grad_norm": 1.324901819229126,
      "learning_rate": 5e-05,
      "loss": 1.4602,
      "step": 1576
    },
    {
      "epoch": 1.9962025316455696,
      "grad_norm": 1.3453493118286133,
      "learning_rate": 5e-05,
      "loss": 1.452,
      "step": 1577
    },
    {
      "epoch": 1.9974683544303797,
      "grad_norm": 1.3049752712249756,
      "learning_rate": 5e-05,
      "loss": 1.4423,
      "step": 1578
    },
    {
      "epoch": 1.9987341772151899,
      "grad_norm": 1.2934478521347046,
      "learning_rate": 5e-05,
      "loss": 1.4044,
      "step": 1579
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3432738780975342,
      "learning_rate": 5e-05,
      "loss": 1.4997,
      "step": 1580
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.1249492168426514,
      "eval_runtime": 169.9904,
      "eval_samples_per_second": 528.736,
      "eval_steps_per_second": 4.136,
      "step": 1580
    },
    {
      "epoch": 2.00126582278481,
      "grad_norm": 1.3528424501419067,
      "learning_rate": 5e-05,
      "loss": 1.4383,
      "step": 1581
    },
    {
      "epoch": 2.0025316455696203,
      "grad_norm": 1.3101627826690674,
      "learning_rate": 5e-05,
      "loss": 1.3641,
      "step": 1582
    },
    {
      "epoch": 2.0037974683544304,
      "grad_norm": 1.2704110145568848,
      "learning_rate": 5e-05,
      "loss": 1.3607,
      "step": 1583
    },
    {
      "epoch": 2.0050632911392405,
      "grad_norm": 1.2788890600204468,
      "learning_rate": 5e-05,
      "loss": 1.3642,
      "step": 1584
    },
    {
      "epoch": 2.0063291139240507,
      "grad_norm": 1.3532778024673462,
      "learning_rate": 5e-05,
      "loss": 1.3874,
      "step": 1585
    },
    {
      "epoch": 2.007594936708861,
      "grad_norm": 1.264281988143921,
      "learning_rate": 5e-05,
      "loss": 1.4035,
      "step": 1586
    },
    {
      "epoch": 2.008860759493671,
      "grad_norm": 1.3036401271820068,
      "learning_rate": 5e-05,
      "loss": 1.4141,
      "step": 1587
    },
    {
      "epoch": 2.010126582278481,
      "grad_norm": 1.2715405225753784,
      "learning_rate": 5e-05,
      "loss": 1.3383,
      "step": 1588
    },
    {
      "epoch": 2.011392405063291,
      "grad_norm": 1.286362648010254,
      "learning_rate": 5e-05,
      "loss": 1.3726,
      "step": 1589
    },
    {
      "epoch": 2.0126582278481013,
      "grad_norm": 1.3274738788604736,
      "learning_rate": 5e-05,
      "loss": 1.4768,
      "step": 1590
    },
    {
      "epoch": 2.0139240506329115,
      "grad_norm": 1.3078950643539429,
      "learning_rate": 5e-05,
      "loss": 1.3908,
      "step": 1591
    },
    {
      "epoch": 2.0151898734177216,
      "grad_norm": 1.3347079753875732,
      "learning_rate": 5e-05,
      "loss": 1.3982,
      "step": 1592
    },
    {
      "epoch": 2.0164556962025317,
      "grad_norm": 1.3215082883834839,
      "learning_rate": 5e-05,
      "loss": 1.4203,
      "step": 1593
    },
    {
      "epoch": 2.017721518987342,
      "grad_norm": 1.2831308841705322,
      "learning_rate": 5e-05,
      "loss": 1.3553,
      "step": 1594
    },
    {
      "epoch": 2.018987341772152,
      "grad_norm": 1.277883529663086,
      "learning_rate": 5e-05,
      "loss": 1.3317,
      "step": 1595
    },
    {
      "epoch": 2.020253164556962,
      "grad_norm": 1.2923732995986938,
      "learning_rate": 5e-05,
      "loss": 1.3959,
      "step": 1596
    },
    {
      "epoch": 2.0215189873417723,
      "grad_norm": 1.2763901948928833,
      "learning_rate": 5e-05,
      "loss": 1.4049,
      "step": 1597
    },
    {
      "epoch": 2.0227848101265824,
      "grad_norm": 1.2884782552719116,
      "learning_rate": 5e-05,
      "loss": 1.4171,
      "step": 1598
    },
    {
      "epoch": 2.0240506329113925,
      "grad_norm": 1.3551971912384033,
      "learning_rate": 5e-05,
      "loss": 1.4698,
      "step": 1599
    },
    {
      "epoch": 2.0253164556962027,
      "grad_norm": 1.28239107131958,
      "learning_rate": 5e-05,
      "loss": 1.4051,
      "step": 1600
    },
    {
      "epoch": 2.026582278481013,
      "grad_norm": 1.2932714223861694,
      "learning_rate": 5e-05,
      "loss": 1.3142,
      "step": 1601
    },
    {
      "epoch": 2.027848101265823,
      "grad_norm": 1.2898081541061401,
      "learning_rate": 5e-05,
      "loss": 1.2948,
      "step": 1602
    },
    {
      "epoch": 2.029113924050633,
      "grad_norm": 1.341485857963562,
      "learning_rate": 5e-05,
      "loss": 1.4519,
      "step": 1603
    },
    {
      "epoch": 2.030379746835443,
      "grad_norm": 1.3030004501342773,
      "learning_rate": 5e-05,
      "loss": 1.3572,
      "step": 1604
    },
    {
      "epoch": 2.0316455696202533,
      "grad_norm": 1.2854650020599365,
      "learning_rate": 5e-05,
      "loss": 1.3827,
      "step": 1605
    },
    {
      "epoch": 2.0329113924050635,
      "grad_norm": 1.2870032787322998,
      "learning_rate": 5e-05,
      "loss": 1.3823,
      "step": 1606
    },
    {
      "epoch": 2.0341772151898736,
      "grad_norm": 1.323096513748169,
      "learning_rate": 5e-05,
      "loss": 1.4083,
      "step": 1607
    },
    {
      "epoch": 2.0354430379746837,
      "grad_norm": 1.2642320394515991,
      "learning_rate": 5e-05,
      "loss": 1.3098,
      "step": 1608
    },
    {
      "epoch": 2.036708860759494,
      "grad_norm": 1.3748767375946045,
      "learning_rate": 5e-05,
      "loss": 1.4856,
      "step": 1609
    },
    {
      "epoch": 2.037974683544304,
      "grad_norm": 1.284622311592102,
      "learning_rate": 5e-05,
      "loss": 1.4191,
      "step": 1610
    },
    {
      "epoch": 2.039240506329114,
      "grad_norm": 1.3314926624298096,
      "learning_rate": 5e-05,
      "loss": 1.3576,
      "step": 1611
    },
    {
      "epoch": 2.0405063291139243,
      "grad_norm": 1.2755920886993408,
      "learning_rate": 5e-05,
      "loss": 1.3623,
      "step": 1612
    },
    {
      "epoch": 2.0417721518987344,
      "grad_norm": 1.3093363046646118,
      "learning_rate": 5e-05,
      "loss": 1.3288,
      "step": 1613
    },
    {
      "epoch": 2.043037974683544,
      "grad_norm": 1.3405767679214478,
      "learning_rate": 5e-05,
      "loss": 1.3797,
      "step": 1614
    },
    {
      "epoch": 2.0443037974683542,
      "grad_norm": 1.3189860582351685,
      "learning_rate": 5e-05,
      "loss": 1.3205,
      "step": 1615
    },
    {
      "epoch": 2.0455696202531644,
      "grad_norm": 1.2937145233154297,
      "learning_rate": 5e-05,
      "loss": 1.3725,
      "step": 1616
    },
    {
      "epoch": 2.0468354430379745,
      "grad_norm": 1.3249868154525757,
      "learning_rate": 5e-05,
      "loss": 1.3514,
      "step": 1617
    },
    {
      "epoch": 2.0481012658227846,
      "grad_norm": 1.246056318283081,
      "learning_rate": 5e-05,
      "loss": 1.3181,
      "step": 1618
    },
    {
      "epoch": 2.0493670886075948,
      "grad_norm": 1.3187055587768555,
      "learning_rate": 5e-05,
      "loss": 1.3983,
      "step": 1619
    },
    {
      "epoch": 2.050632911392405,
      "grad_norm": 1.3175369501113892,
      "learning_rate": 5e-05,
      "loss": 1.329,
      "step": 1620
    },
    {
      "epoch": 2.051898734177215,
      "grad_norm": 1.3038839101791382,
      "learning_rate": 5e-05,
      "loss": 1.3542,
      "step": 1621
    },
    {
      "epoch": 2.053164556962025,
      "grad_norm": 1.2750681638717651,
      "learning_rate": 5e-05,
      "loss": 1.3335,
      "step": 1622
    },
    {
      "epoch": 2.0544303797468353,
      "grad_norm": 1.297536849975586,
      "learning_rate": 5e-05,
      "loss": 1.4002,
      "step": 1623
    },
    {
      "epoch": 2.0556962025316454,
      "grad_norm": 1.309561848640442,
      "learning_rate": 5e-05,
      "loss": 1.3755,
      "step": 1624
    },
    {
      "epoch": 2.0569620253164556,
      "grad_norm": 1.302922248840332,
      "learning_rate": 5e-05,
      "loss": 1.3395,
      "step": 1625
    },
    {
      "epoch": 2.0582278481012657,
      "grad_norm": 1.3102285861968994,
      "learning_rate": 5e-05,
      "loss": 1.3467,
      "step": 1626
    },
    {
      "epoch": 2.059493670886076,
      "grad_norm": 1.3271558284759521,
      "learning_rate": 5e-05,
      "loss": 1.384,
      "step": 1627
    },
    {
      "epoch": 2.060759493670886,
      "grad_norm": 1.3056493997573853,
      "learning_rate": 5e-05,
      "loss": 1.3624,
      "step": 1628
    },
    {
      "epoch": 2.062025316455696,
      "grad_norm": 1.3007076978683472,
      "learning_rate": 5e-05,
      "loss": 1.418,
      "step": 1629
    },
    {
      "epoch": 2.0632911392405062,
      "grad_norm": 1.304966688156128,
      "learning_rate": 5e-05,
      "loss": 1.3367,
      "step": 1630
    },
    {
      "epoch": 2.0645569620253164,
      "grad_norm": 1.3147509098052979,
      "learning_rate": 5e-05,
      "loss": 1.3356,
      "step": 1631
    },
    {
      "epoch": 2.0658227848101265,
      "grad_norm": 1.307016134262085,
      "learning_rate": 5e-05,
      "loss": 1.3721,
      "step": 1632
    },
    {
      "epoch": 2.0670886075949366,
      "grad_norm": 1.3428212404251099,
      "learning_rate": 5e-05,
      "loss": 1.3943,
      "step": 1633
    },
    {
      "epoch": 2.0683544303797468,
      "grad_norm": 1.3321659564971924,
      "learning_rate": 5e-05,
      "loss": 1.3895,
      "step": 1634
    },
    {
      "epoch": 2.069620253164557,
      "grad_norm": 1.3022063970565796,
      "learning_rate": 5e-05,
      "loss": 1.4265,
      "step": 1635
    },
    {
      "epoch": 2.070886075949367,
      "grad_norm": 1.3298227787017822,
      "learning_rate": 5e-05,
      "loss": 1.3862,
      "step": 1636
    },
    {
      "epoch": 2.072151898734177,
      "grad_norm": 1.2901171445846558,
      "learning_rate": 5e-05,
      "loss": 1.3691,
      "step": 1637
    },
    {
      "epoch": 2.0734177215189873,
      "grad_norm": 1.3133846521377563,
      "learning_rate": 5e-05,
      "loss": 1.4417,
      "step": 1638
    },
    {
      "epoch": 2.0746835443037974,
      "grad_norm": 1.2919496297836304,
      "learning_rate": 5e-05,
      "loss": 1.4084,
      "step": 1639
    },
    {
      "epoch": 2.0759493670886076,
      "grad_norm": 1.307389259338379,
      "learning_rate": 5e-05,
      "loss": 1.3947,
      "step": 1640
    },
    {
      "epoch": 2.0772151898734177,
      "grad_norm": 1.2818070650100708,
      "learning_rate": 5e-05,
      "loss": 1.3067,
      "step": 1641
    },
    {
      "epoch": 2.078481012658228,
      "grad_norm": 1.2815176248550415,
      "learning_rate": 5e-05,
      "loss": 1.3781,
      "step": 1642
    },
    {
      "epoch": 2.079746835443038,
      "grad_norm": 1.2747606039047241,
      "learning_rate": 5e-05,
      "loss": 1.3775,
      "step": 1643
    },
    {
      "epoch": 2.081012658227848,
      "grad_norm": 1.3400850296020508,
      "learning_rate": 5e-05,
      "loss": 1.3639,
      "step": 1644
    },
    {
      "epoch": 2.0822784810126582,
      "grad_norm": 1.2733547687530518,
      "learning_rate": 5e-05,
      "loss": 1.3828,
      "step": 1645
    },
    {
      "epoch": 2.0835443037974684,
      "grad_norm": 1.299561619758606,
      "learning_rate": 5e-05,
      "loss": 1.3923,
      "step": 1646
    },
    {
      "epoch": 2.0848101265822785,
      "grad_norm": 1.3358075618743896,
      "learning_rate": 5e-05,
      "loss": 1.3836,
      "step": 1647
    },
    {
      "epoch": 2.0860759493670886,
      "grad_norm": 1.3423634767532349,
      "learning_rate": 5e-05,
      "loss": 1.322,
      "step": 1648
    },
    {
      "epoch": 2.0873417721518988,
      "grad_norm": 1.2926576137542725,
      "learning_rate": 5e-05,
      "loss": 1.3136,
      "step": 1649
    },
    {
      "epoch": 2.088607594936709,
      "grad_norm": 1.3592774868011475,
      "learning_rate": 5e-05,
      "loss": 1.4197,
      "step": 1650
    },
    {
      "epoch": 2.089873417721519,
      "grad_norm": 1.3367667198181152,
      "learning_rate": 5e-05,
      "loss": 1.4026,
      "step": 1651
    },
    {
      "epoch": 2.091139240506329,
      "grad_norm": 1.282658576965332,
      "learning_rate": 5e-05,
      "loss": 1.3308,
      "step": 1652
    },
    {
      "epoch": 2.0924050632911393,
      "grad_norm": 1.3003934621810913,
      "learning_rate": 5e-05,
      "loss": 1.3326,
      "step": 1653
    },
    {
      "epoch": 2.0936708860759494,
      "grad_norm": 1.32015061378479,
      "learning_rate": 5e-05,
      "loss": 1.4191,
      "step": 1654
    },
    {
      "epoch": 2.0949367088607596,
      "grad_norm": 1.3246573209762573,
      "learning_rate": 5e-05,
      "loss": 1.3839,
      "step": 1655
    },
    {
      "epoch": 2.0962025316455697,
      "grad_norm": 1.3175936937332153,
      "learning_rate": 5e-05,
      "loss": 1.3968,
      "step": 1656
    },
    {
      "epoch": 2.09746835443038,
      "grad_norm": 1.3312119245529175,
      "learning_rate": 5e-05,
      "loss": 1.3657,
      "step": 1657
    },
    {
      "epoch": 2.09873417721519,
      "grad_norm": 1.3001749515533447,
      "learning_rate": 5e-05,
      "loss": 1.3559,
      "step": 1658
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.3061963319778442,
      "learning_rate": 5e-05,
      "loss": 1.3945,
      "step": 1659
    },
    {
      "epoch": 2.1012658227848102,
      "grad_norm": 1.2854681015014648,
      "learning_rate": 5e-05,
      "loss": 1.2932,
      "step": 1660
    },
    {
      "epoch": 2.1025316455696204,
      "grad_norm": 1.2947646379470825,
      "learning_rate": 5e-05,
      "loss": 1.2927,
      "step": 1661
    },
    {
      "epoch": 2.1037974683544305,
      "grad_norm": 1.3187286853790283,
      "learning_rate": 5e-05,
      "loss": 1.3196,
      "step": 1662
    },
    {
      "epoch": 2.1050632911392406,
      "grad_norm": 1.329195499420166,
      "learning_rate": 5e-05,
      "loss": 1.3227,
      "step": 1663
    },
    {
      "epoch": 2.1063291139240508,
      "grad_norm": 1.3222306966781616,
      "learning_rate": 5e-05,
      "loss": 1.3321,
      "step": 1664
    },
    {
      "epoch": 2.107594936708861,
      "grad_norm": 1.3816595077514648,
      "learning_rate": 5e-05,
      "loss": 1.4411,
      "step": 1665
    },
    {
      "epoch": 2.108860759493671,
      "grad_norm": 1.3324437141418457,
      "learning_rate": 5e-05,
      "loss": 1.3807,
      "step": 1666
    },
    {
      "epoch": 2.110126582278481,
      "grad_norm": 1.3123801946640015,
      "learning_rate": 5e-05,
      "loss": 1.3283,
      "step": 1667
    },
    {
      "epoch": 2.1113924050632913,
      "grad_norm": 1.3341984748840332,
      "learning_rate": 5e-05,
      "loss": 1.4237,
      "step": 1668
    },
    {
      "epoch": 2.1126582278481014,
      "grad_norm": 1.3040887117385864,
      "learning_rate": 5e-05,
      "loss": 1.2898,
      "step": 1669
    },
    {
      "epoch": 2.1139240506329116,
      "grad_norm": 1.3330416679382324,
      "learning_rate": 5e-05,
      "loss": 1.4023,
      "step": 1670
    },
    {
      "epoch": 2.1151898734177217,
      "grad_norm": 1.3454502820968628,
      "learning_rate": 5e-05,
      "loss": 1.351,
      "step": 1671
    },
    {
      "epoch": 2.116455696202532,
      "grad_norm": 1.3407413959503174,
      "learning_rate": 5e-05,
      "loss": 1.4188,
      "step": 1672
    },
    {
      "epoch": 2.117721518987342,
      "grad_norm": 1.2988197803497314,
      "learning_rate": 5e-05,
      "loss": 1.4157,
      "step": 1673
    },
    {
      "epoch": 2.118987341772152,
      "grad_norm": 1.3377189636230469,
      "learning_rate": 5e-05,
      "loss": 1.3709,
      "step": 1674
    },
    {
      "epoch": 2.1202531645569622,
      "grad_norm": 1.2905985116958618,
      "learning_rate": 5e-05,
      "loss": 1.3482,
      "step": 1675
    },
    {
      "epoch": 2.1215189873417724,
      "grad_norm": 1.2805438041687012,
      "learning_rate": 5e-05,
      "loss": 1.3334,
      "step": 1676
    },
    {
      "epoch": 2.1227848101265825,
      "grad_norm": 1.2829806804656982,
      "learning_rate": 5e-05,
      "loss": 1.36,
      "step": 1677
    },
    {
      "epoch": 2.124050632911392,
      "grad_norm": 1.3418859243392944,
      "learning_rate": 5e-05,
      "loss": 1.3793,
      "step": 1678
    },
    {
      "epoch": 2.1253164556962023,
      "grad_norm": 1.3138753175735474,
      "learning_rate": 5e-05,
      "loss": 1.3524,
      "step": 1679
    },
    {
      "epoch": 2.1265822784810124,
      "grad_norm": 1.301148772239685,
      "learning_rate": 5e-05,
      "loss": 1.3883,
      "step": 1680
    },
    {
      "epoch": 2.1278481012658226,
      "grad_norm": 1.2946661710739136,
      "learning_rate": 5e-05,
      "loss": 1.2942,
      "step": 1681
    },
    {
      "epoch": 2.1291139240506327,
      "grad_norm": 1.2788361310958862,
      "learning_rate": 5e-05,
      "loss": 1.3755,
      "step": 1682
    },
    {
      "epoch": 2.130379746835443,
      "grad_norm": 1.3223756551742554,
      "learning_rate": 5e-05,
      "loss": 1.322,
      "step": 1683
    },
    {
      "epoch": 2.131645569620253,
      "grad_norm": 1.3186649084091187,
      "learning_rate": 5e-05,
      "loss": 1.4048,
      "step": 1684
    },
    {
      "epoch": 2.132911392405063,
      "grad_norm": 1.3384983539581299,
      "learning_rate": 5e-05,
      "loss": 1.3985,
      "step": 1685
    },
    {
      "epoch": 2.1341772151898732,
      "grad_norm": 1.3596422672271729,
      "learning_rate": 5e-05,
      "loss": 1.3335,
      "step": 1686
    },
    {
      "epoch": 2.1354430379746834,
      "grad_norm": 1.3117003440856934,
      "learning_rate": 5e-05,
      "loss": 1.3048,
      "step": 1687
    },
    {
      "epoch": 2.1367088607594935,
      "grad_norm": 1.3200101852416992,
      "learning_rate": 5e-05,
      "loss": 1.3755,
      "step": 1688
    },
    {
      "epoch": 2.1379746835443036,
      "grad_norm": 1.2865899801254272,
      "learning_rate": 5e-05,
      "loss": 1.3535,
      "step": 1689
    },
    {
      "epoch": 2.1392405063291138,
      "grad_norm": 1.29763925075531,
      "learning_rate": 5e-05,
      "loss": 1.338,
      "step": 1690
    },
    {
      "epoch": 2.140506329113924,
      "grad_norm": 1.3346449136734009,
      "learning_rate": 5e-05,
      "loss": 1.4126,
      "step": 1691
    },
    {
      "epoch": 2.141772151898734,
      "grad_norm": 1.3422000408172607,
      "learning_rate": 5e-05,
      "loss": 1.3667,
      "step": 1692
    },
    {
      "epoch": 2.143037974683544,
      "grad_norm": 1.3316075801849365,
      "learning_rate": 5e-05,
      "loss": 1.338,
      "step": 1693
    },
    {
      "epoch": 2.1443037974683543,
      "grad_norm": 1.304583191871643,
      "learning_rate": 5e-05,
      "loss": 1.3665,
      "step": 1694
    },
    {
      "epoch": 2.1455696202531644,
      "grad_norm": 1.3409578800201416,
      "learning_rate": 5e-05,
      "loss": 1.3261,
      "step": 1695
    },
    {
      "epoch": 2.1468354430379746,
      "grad_norm": 1.2896757125854492,
      "learning_rate": 5e-05,
      "loss": 1.3343,
      "step": 1696
    },
    {
      "epoch": 2.1481012658227847,
      "grad_norm": 1.288805603981018,
      "learning_rate": 5e-05,
      "loss": 1.2909,
      "step": 1697
    },
    {
      "epoch": 2.149367088607595,
      "grad_norm": 1.3701773881912231,
      "learning_rate": 5e-05,
      "loss": 1.3681,
      "step": 1698
    },
    {
      "epoch": 2.150632911392405,
      "grad_norm": 1.2901614904403687,
      "learning_rate": 5e-05,
      "loss": 1.3211,
      "step": 1699
    },
    {
      "epoch": 2.151898734177215,
      "grad_norm": 1.292531967163086,
      "learning_rate": 5e-05,
      "loss": 1.2702,
      "step": 1700
    },
    {
      "epoch": 2.1531645569620252,
      "grad_norm": 1.3193894624710083,
      "learning_rate": 5e-05,
      "loss": 1.2942,
      "step": 1701
    },
    {
      "epoch": 2.1544303797468354,
      "grad_norm": 1.3178613185882568,
      "learning_rate": 5e-05,
      "loss": 1.2962,
      "step": 1702
    },
    {
      "epoch": 2.1556962025316455,
      "grad_norm": 1.3745157718658447,
      "learning_rate": 5e-05,
      "loss": 1.3725,
      "step": 1703
    },
    {
      "epoch": 2.1569620253164556,
      "grad_norm": 1.3176378011703491,
      "learning_rate": 5e-05,
      "loss": 1.333,
      "step": 1704
    },
    {
      "epoch": 2.1582278481012658,
      "grad_norm": 1.3258886337280273,
      "learning_rate": 5e-05,
      "loss": 1.3805,
      "step": 1705
    },
    {
      "epoch": 2.159493670886076,
      "grad_norm": 1.292289137840271,
      "learning_rate": 5e-05,
      "loss": 1.3372,
      "step": 1706
    },
    {
      "epoch": 2.160759493670886,
      "grad_norm": 1.2927700281143188,
      "learning_rate": 5e-05,
      "loss": 1.2966,
      "step": 1707
    },
    {
      "epoch": 2.162025316455696,
      "grad_norm": 1.3129210472106934,
      "learning_rate": 5e-05,
      "loss": 1.3909,
      "step": 1708
    },
    {
      "epoch": 2.1632911392405063,
      "grad_norm": 1.322373867034912,
      "learning_rate": 5e-05,
      "loss": 1.3532,
      "step": 1709
    },
    {
      "epoch": 2.1645569620253164,
      "grad_norm": 1.302223801612854,
      "learning_rate": 5e-05,
      "loss": 1.347,
      "step": 1710
    },
    {
      "epoch": 2.1658227848101266,
      "grad_norm": 1.289402961730957,
      "learning_rate": 5e-05,
      "loss": 1.3417,
      "step": 1711
    },
    {
      "epoch": 2.1670886075949367,
      "grad_norm": 1.2671902179718018,
      "learning_rate": 5e-05,
      "loss": 1.2103,
      "step": 1712
    },
    {
      "epoch": 2.168354430379747,
      "grad_norm": 1.268099069595337,
      "learning_rate": 5e-05,
      "loss": 1.3214,
      "step": 1713
    },
    {
      "epoch": 2.169620253164557,
      "grad_norm": 1.313371181488037,
      "learning_rate": 5e-05,
      "loss": 1.3278,
      "step": 1714
    },
    {
      "epoch": 2.170886075949367,
      "grad_norm": 1.3633757829666138,
      "learning_rate": 5e-05,
      "loss": 1.3497,
      "step": 1715
    },
    {
      "epoch": 2.1721518987341772,
      "grad_norm": 1.335319995880127,
      "learning_rate": 5e-05,
      "loss": 1.4161,
      "step": 1716
    },
    {
      "epoch": 2.1734177215189874,
      "grad_norm": 1.2834059000015259,
      "learning_rate": 5e-05,
      "loss": 1.3343,
      "step": 1717
    },
    {
      "epoch": 2.1746835443037975,
      "grad_norm": 1.3067991733551025,
      "learning_rate": 5e-05,
      "loss": 1.354,
      "step": 1718
    },
    {
      "epoch": 2.1759493670886076,
      "grad_norm": 1.28459894657135,
      "learning_rate": 5e-05,
      "loss": 1.264,
      "step": 1719
    },
    {
      "epoch": 2.1772151898734178,
      "grad_norm": 1.3916585445404053,
      "learning_rate": 5e-05,
      "loss": 1.3489,
      "step": 1720
    },
    {
      "epoch": 2.178481012658228,
      "grad_norm": 1.2859792709350586,
      "learning_rate": 5e-05,
      "loss": 1.293,
      "step": 1721
    },
    {
      "epoch": 2.179746835443038,
      "grad_norm": 1.3262557983398438,
      "learning_rate": 5e-05,
      "loss": 1.3853,
      "step": 1722
    },
    {
      "epoch": 2.181012658227848,
      "grad_norm": 1.3298929929733276,
      "learning_rate": 5e-05,
      "loss": 1.3749,
      "step": 1723
    },
    {
      "epoch": 2.1822784810126583,
      "grad_norm": 1.320779800415039,
      "learning_rate": 5e-05,
      "loss": 1.3403,
      "step": 1724
    },
    {
      "epoch": 2.1835443037974684,
      "grad_norm": 1.32694673538208,
      "learning_rate": 5e-05,
      "loss": 1.3318,
      "step": 1725
    },
    {
      "epoch": 2.1848101265822786,
      "grad_norm": 1.338884949684143,
      "learning_rate": 5e-05,
      "loss": 1.3126,
      "step": 1726
    },
    {
      "epoch": 2.1860759493670887,
      "grad_norm": 1.2927340269088745,
      "learning_rate": 5e-05,
      "loss": 1.272,
      "step": 1727
    },
    {
      "epoch": 2.187341772151899,
      "grad_norm": 1.3277662992477417,
      "learning_rate": 5e-05,
      "loss": 1.3282,
      "step": 1728
    },
    {
      "epoch": 2.188607594936709,
      "grad_norm": 1.3230870962142944,
      "learning_rate": 5e-05,
      "loss": 1.339,
      "step": 1729
    },
    {
      "epoch": 2.189873417721519,
      "grad_norm": 1.2958805561065674,
      "learning_rate": 5e-05,
      "loss": 1.3141,
      "step": 1730
    },
    {
      "epoch": 2.1911392405063292,
      "grad_norm": 1.3280112743377686,
      "learning_rate": 5e-05,
      "loss": 1.3727,
      "step": 1731
    },
    {
      "epoch": 2.1924050632911394,
      "grad_norm": 1.3201735019683838,
      "learning_rate": 5e-05,
      "loss": 1.3716,
      "step": 1732
    },
    {
      "epoch": 2.1936708860759495,
      "grad_norm": 1.382603645324707,
      "learning_rate": 5e-05,
      "loss": 1.4304,
      "step": 1733
    },
    {
      "epoch": 2.1949367088607596,
      "grad_norm": 1.3258670568466187,
      "learning_rate": 5e-05,
      "loss": 1.3462,
      "step": 1734
    },
    {
      "epoch": 2.1962025316455698,
      "grad_norm": 1.3404096364974976,
      "learning_rate": 5e-05,
      "loss": 1.3134,
      "step": 1735
    },
    {
      "epoch": 2.19746835443038,
      "grad_norm": 1.3261204957962036,
      "learning_rate": 5e-05,
      "loss": 1.2864,
      "step": 1736
    },
    {
      "epoch": 2.19873417721519,
      "grad_norm": 1.3317222595214844,
      "learning_rate": 5e-05,
      "loss": 1.3729,
      "step": 1737
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.3066078424453735,
      "learning_rate": 5e-05,
      "loss": 1.3138,
      "step": 1738
    },
    {
      "epoch": 2.2012658227848103,
      "grad_norm": 1.301581621170044,
      "learning_rate": 5e-05,
      "loss": 1.2857,
      "step": 1739
    },
    {
      "epoch": 2.2025316455696204,
      "grad_norm": 1.326877236366272,
      "learning_rate": 5e-05,
      "loss": 1.4029,
      "step": 1740
    },
    {
      "epoch": 2.2037974683544306,
      "grad_norm": 1.336077332496643,
      "learning_rate": 5e-05,
      "loss": 1.3572,
      "step": 1741
    },
    {
      "epoch": 2.2050632911392407,
      "grad_norm": 1.323493242263794,
      "learning_rate": 5e-05,
      "loss": 1.3487,
      "step": 1742
    },
    {
      "epoch": 2.206329113924051,
      "grad_norm": 1.3260366916656494,
      "learning_rate": 5e-05,
      "loss": 1.3619,
      "step": 1743
    },
    {
      "epoch": 2.207594936708861,
      "grad_norm": 1.313657522201538,
      "learning_rate": 5e-05,
      "loss": 1.4007,
      "step": 1744
    },
    {
      "epoch": 2.208860759493671,
      "grad_norm": 1.3384475708007812,
      "learning_rate": 5e-05,
      "loss": 1.347,
      "step": 1745
    },
    {
      "epoch": 2.2101265822784812,
      "grad_norm": 1.321634292602539,
      "learning_rate": 5e-05,
      "loss": 1.3475,
      "step": 1746
    },
    {
      "epoch": 2.211392405063291,
      "grad_norm": 1.3316559791564941,
      "learning_rate": 5e-05,
      "loss": 1.3399,
      "step": 1747
    },
    {
      "epoch": 2.212658227848101,
      "grad_norm": 1.3126543760299683,
      "learning_rate": 5e-05,
      "loss": 1.3656,
      "step": 1748
    },
    {
      "epoch": 2.213924050632911,
      "grad_norm": 1.2652697563171387,
      "learning_rate": 5e-05,
      "loss": 1.3517,
      "step": 1749
    },
    {
      "epoch": 2.2151898734177213,
      "grad_norm": 1.3517003059387207,
      "learning_rate": 5e-05,
      "loss": 1.3718,
      "step": 1750
    },
    {
      "epoch": 2.2164556962025315,
      "grad_norm": 1.307289481163025,
      "learning_rate": 5e-05,
      "loss": 1.295,
      "step": 1751
    },
    {
      "epoch": 2.2177215189873416,
      "grad_norm": 1.3595695495605469,
      "learning_rate": 5e-05,
      "loss": 1.3918,
      "step": 1752
    },
    {
      "epoch": 2.2189873417721517,
      "grad_norm": 1.3132320642471313,
      "learning_rate": 5e-05,
      "loss": 1.2969,
      "step": 1753
    },
    {
      "epoch": 2.220253164556962,
      "grad_norm": 1.3221415281295776,
      "learning_rate": 5e-05,
      "loss": 1.3348,
      "step": 1754
    },
    {
      "epoch": 2.221518987341772,
      "grad_norm": 1.3463388681411743,
      "learning_rate": 5e-05,
      "loss": 1.2642,
      "step": 1755
    },
    {
      "epoch": 2.222784810126582,
      "grad_norm": 1.29871666431427,
      "learning_rate": 5e-05,
      "loss": 1.2817,
      "step": 1756
    },
    {
      "epoch": 2.2240506329113923,
      "grad_norm": 1.319070816040039,
      "learning_rate": 5e-05,
      "loss": 1.3408,
      "step": 1757
    },
    {
      "epoch": 2.2253164556962024,
      "grad_norm": 1.2603981494903564,
      "learning_rate": 5e-05,
      "loss": 1.295,
      "step": 1758
    },
    {
      "epoch": 2.2265822784810125,
      "grad_norm": 1.3130635023117065,
      "learning_rate": 5e-05,
      "loss": 1.3144,
      "step": 1759
    },
    {
      "epoch": 2.2278481012658227,
      "grad_norm": 1.3322827816009521,
      "learning_rate": 5e-05,
      "loss": 1.3359,
      "step": 1760
    },
    {
      "epoch": 2.229113924050633,
      "grad_norm": 1.3552477359771729,
      "learning_rate": 5e-05,
      "loss": 1.3264,
      "step": 1761
    },
    {
      "epoch": 2.230379746835443,
      "grad_norm": 1.3137831687927246,
      "learning_rate": 5e-05,
      "loss": 1.3808,
      "step": 1762
    },
    {
      "epoch": 2.231645569620253,
      "grad_norm": 1.3761810064315796,
      "learning_rate": 5e-05,
      "loss": 1.3225,
      "step": 1763
    },
    {
      "epoch": 2.232911392405063,
      "grad_norm": 1.3253400325775146,
      "learning_rate": 5e-05,
      "loss": 1.3705,
      "step": 1764
    },
    {
      "epoch": 2.2341772151898733,
      "grad_norm": 1.3107435703277588,
      "learning_rate": 5e-05,
      "loss": 1.3135,
      "step": 1765
    },
    {
      "epoch": 2.2354430379746835,
      "grad_norm": 1.3083152770996094,
      "learning_rate": 5e-05,
      "loss": 1.3256,
      "step": 1766
    },
    {
      "epoch": 2.2367088607594936,
      "grad_norm": 1.3986185789108276,
      "learning_rate": 5e-05,
      "loss": 1.4162,
      "step": 1767
    },
    {
      "epoch": 2.2379746835443037,
      "grad_norm": 1.3390644788742065,
      "learning_rate": 5e-05,
      "loss": 1.2868,
      "step": 1768
    },
    {
      "epoch": 2.239240506329114,
      "grad_norm": 1.3257113695144653,
      "learning_rate": 5e-05,
      "loss": 1.34,
      "step": 1769
    },
    {
      "epoch": 2.240506329113924,
      "grad_norm": 1.2812243700027466,
      "learning_rate": 5e-05,
      "loss": 1.2595,
      "step": 1770
    },
    {
      "epoch": 2.241772151898734,
      "grad_norm": 1.302341341972351,
      "learning_rate": 5e-05,
      "loss": 1.2566,
      "step": 1771
    },
    {
      "epoch": 2.2430379746835443,
      "grad_norm": 1.353172779083252,
      "learning_rate": 5e-05,
      "loss": 1.4275,
      "step": 1772
    },
    {
      "epoch": 2.2443037974683544,
      "grad_norm": 1.3014898300170898,
      "learning_rate": 5e-05,
      "loss": 1.296,
      "step": 1773
    },
    {
      "epoch": 2.2455696202531645,
      "grad_norm": 1.3197238445281982,
      "learning_rate": 5e-05,
      "loss": 1.3431,
      "step": 1774
    },
    {
      "epoch": 2.2468354430379747,
      "grad_norm": 1.317381739616394,
      "learning_rate": 5e-05,
      "loss": 1.3229,
      "step": 1775
    },
    {
      "epoch": 2.248101265822785,
      "grad_norm": 1.3282527923583984,
      "learning_rate": 5e-05,
      "loss": 1.3204,
      "step": 1776
    },
    {
      "epoch": 2.249367088607595,
      "grad_norm": 1.2988131046295166,
      "learning_rate": 5e-05,
      "loss": 1.3205,
      "step": 1777
    },
    {
      "epoch": 2.250632911392405,
      "grad_norm": 1.3041677474975586,
      "learning_rate": 5e-05,
      "loss": 1.3446,
      "step": 1778
    },
    {
      "epoch": 2.251898734177215,
      "grad_norm": 1.3152568340301514,
      "learning_rate": 5e-05,
      "loss": 1.316,
      "step": 1779
    },
    {
      "epoch": 2.2531645569620253,
      "grad_norm": 1.2682676315307617,
      "learning_rate": 5e-05,
      "loss": 1.2596,
      "step": 1780
    },
    {
      "epoch": 2.2544303797468355,
      "grad_norm": 1.3753608465194702,
      "learning_rate": 5e-05,
      "loss": 1.3617,
      "step": 1781
    },
    {
      "epoch": 2.2556962025316456,
      "grad_norm": 1.3383240699768066,
      "learning_rate": 5e-05,
      "loss": 1.3635,
      "step": 1782
    },
    {
      "epoch": 2.2569620253164557,
      "grad_norm": 1.2901896238327026,
      "learning_rate": 5e-05,
      "loss": 1.2903,
      "step": 1783
    },
    {
      "epoch": 2.258227848101266,
      "grad_norm": 1.3707213401794434,
      "learning_rate": 5e-05,
      "loss": 1.385,
      "step": 1784
    },
    {
      "epoch": 2.259493670886076,
      "grad_norm": 1.3320561647415161,
      "learning_rate": 5e-05,
      "loss": 1.3348,
      "step": 1785
    },
    {
      "epoch": 2.260759493670886,
      "grad_norm": 1.323791742324829,
      "learning_rate": 5e-05,
      "loss": 1.3163,
      "step": 1786
    },
    {
      "epoch": 2.2620253164556963,
      "grad_norm": 1.2950453758239746,
      "learning_rate": 5e-05,
      "loss": 1.3005,
      "step": 1787
    },
    {
      "epoch": 2.2632911392405064,
      "grad_norm": 1.3460017442703247,
      "learning_rate": 5e-05,
      "loss": 1.3112,
      "step": 1788
    },
    {
      "epoch": 2.2645569620253165,
      "grad_norm": 1.2569423913955688,
      "learning_rate": 5e-05,
      "loss": 1.2829,
      "step": 1789
    },
    {
      "epoch": 2.2658227848101267,
      "grad_norm": 1.3215447664260864,
      "learning_rate": 5e-05,
      "loss": 1.3418,
      "step": 1790
    },
    {
      "epoch": 2.267088607594937,
      "grad_norm": 1.3520598411560059,
      "learning_rate": 5e-05,
      "loss": 1.3964,
      "step": 1791
    },
    {
      "epoch": 2.268354430379747,
      "grad_norm": 1.3235454559326172,
      "learning_rate": 5e-05,
      "loss": 1.3026,
      "step": 1792
    },
    {
      "epoch": 2.269620253164557,
      "grad_norm": 1.2949848175048828,
      "learning_rate": 5e-05,
      "loss": 1.3378,
      "step": 1793
    },
    {
      "epoch": 2.270886075949367,
      "grad_norm": 1.3207210302352905,
      "learning_rate": 5e-05,
      "loss": 1.3122,
      "step": 1794
    },
    {
      "epoch": 2.2721518987341773,
      "grad_norm": 1.3333505392074585,
      "learning_rate": 5e-05,
      "loss": 1.3489,
      "step": 1795
    },
    {
      "epoch": 2.2734177215189875,
      "grad_norm": 1.309191346168518,
      "learning_rate": 5e-05,
      "loss": 1.2811,
      "step": 1796
    },
    {
      "epoch": 2.2746835443037976,
      "grad_norm": 1.3602200746536255,
      "learning_rate": 5e-05,
      "loss": 1.3797,
      "step": 1797
    },
    {
      "epoch": 2.2759493670886077,
      "grad_norm": 1.3506475687026978,
      "learning_rate": 5e-05,
      "loss": 1.3354,
      "step": 1798
    },
    {
      "epoch": 2.277215189873418,
      "grad_norm": 1.3807483911514282,
      "learning_rate": 5e-05,
      "loss": 1.3942,
      "step": 1799
    },
    {
      "epoch": 2.278481012658228,
      "grad_norm": 1.3009593486785889,
      "learning_rate": 5e-05,
      "loss": 1.3123,
      "step": 1800
    },
    {
      "epoch": 2.279746835443038,
      "grad_norm": 1.3708583116531372,
      "learning_rate": 5e-05,
      "loss": 1.3979,
      "step": 1801
    },
    {
      "epoch": 2.2810126582278483,
      "grad_norm": 1.3065401315689087,
      "learning_rate": 5e-05,
      "loss": 1.2824,
      "step": 1802
    },
    {
      "epoch": 2.2822784810126584,
      "grad_norm": 1.314782738685608,
      "learning_rate": 5e-05,
      "loss": 1.3171,
      "step": 1803
    },
    {
      "epoch": 2.2835443037974685,
      "grad_norm": 1.3506252765655518,
      "learning_rate": 5e-05,
      "loss": 1.3811,
      "step": 1804
    },
    {
      "epoch": 2.2848101265822787,
      "grad_norm": 1.299006700515747,
      "learning_rate": 5e-05,
      "loss": 1.3,
      "step": 1805
    },
    {
      "epoch": 2.286075949367089,
      "grad_norm": 1.2856144905090332,
      "learning_rate": 5e-05,
      "loss": 1.2334,
      "step": 1806
    },
    {
      "epoch": 2.2873417721518985,
      "grad_norm": 1.3028292655944824,
      "learning_rate": 5e-05,
      "loss": 1.3212,
      "step": 1807
    },
    {
      "epoch": 2.2886075949367086,
      "grad_norm": 1.3314870595932007,
      "learning_rate": 5e-05,
      "loss": 1.3729,
      "step": 1808
    },
    {
      "epoch": 2.2898734177215188,
      "grad_norm": 1.2788645029067993,
      "learning_rate": 5e-05,
      "loss": 1.2792,
      "step": 1809
    },
    {
      "epoch": 2.291139240506329,
      "grad_norm": 1.3239072561264038,
      "learning_rate": 5e-05,
      "loss": 1.3784,
      "step": 1810
    },
    {
      "epoch": 2.292405063291139,
      "grad_norm": 1.3233760595321655,
      "learning_rate": 5e-05,
      "loss": 1.3413,
      "step": 1811
    },
    {
      "epoch": 2.293670886075949,
      "grad_norm": 1.307205319404602,
      "learning_rate": 5e-05,
      "loss": 1.2826,
      "step": 1812
    },
    {
      "epoch": 2.2949367088607593,
      "grad_norm": 1.3727777004241943,
      "learning_rate": 5e-05,
      "loss": 1.3346,
      "step": 1813
    },
    {
      "epoch": 2.2962025316455694,
      "grad_norm": 1.3381098508834839,
      "learning_rate": 5e-05,
      "loss": 1.3416,
      "step": 1814
    },
    {
      "epoch": 2.2974683544303796,
      "grad_norm": 1.3258931636810303,
      "learning_rate": 5e-05,
      "loss": 1.3081,
      "step": 1815
    },
    {
      "epoch": 2.2987341772151897,
      "grad_norm": 1.3002434968948364,
      "learning_rate": 5e-05,
      "loss": 1.3077,
      "step": 1816
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.2855931520462036,
      "learning_rate": 5e-05,
      "loss": 1.3357,
      "step": 1817
    },
    {
      "epoch": 2.30126582278481,
      "grad_norm": 1.2603569030761719,
      "learning_rate": 5e-05,
      "loss": 1.2244,
      "step": 1818
    },
    {
      "epoch": 2.30253164556962,
      "grad_norm": 1.2967804670333862,
      "learning_rate": 5e-05,
      "loss": 1.2565,
      "step": 1819
    },
    {
      "epoch": 2.3037974683544302,
      "grad_norm": 1.3073474168777466,
      "learning_rate": 5e-05,
      "loss": 1.3095,
      "step": 1820
    },
    {
      "epoch": 2.3050632911392404,
      "grad_norm": 1.3492339849472046,
      "learning_rate": 5e-05,
      "loss": 1.4056,
      "step": 1821
    },
    {
      "epoch": 2.3063291139240505,
      "grad_norm": 1.3177263736724854,
      "learning_rate": 5e-05,
      "loss": 1.2822,
      "step": 1822
    },
    {
      "epoch": 2.3075949367088606,
      "grad_norm": 1.3451699018478394,
      "learning_rate": 5e-05,
      "loss": 1.3619,
      "step": 1823
    },
    {
      "epoch": 2.3088607594936708,
      "grad_norm": 1.3563003540039062,
      "learning_rate": 5e-05,
      "loss": 1.3244,
      "step": 1824
    },
    {
      "epoch": 2.310126582278481,
      "grad_norm": 1.2820206880569458,
      "learning_rate": 5e-05,
      "loss": 1.2949,
      "step": 1825
    },
    {
      "epoch": 2.311392405063291,
      "grad_norm": 1.3450628519058228,
      "learning_rate": 5e-05,
      "loss": 1.3555,
      "step": 1826
    },
    {
      "epoch": 2.312658227848101,
      "grad_norm": 1.3342771530151367,
      "learning_rate": 5e-05,
      "loss": 1.3002,
      "step": 1827
    },
    {
      "epoch": 2.3139240506329113,
      "grad_norm": 1.3028590679168701,
      "learning_rate": 5e-05,
      "loss": 1.319,
      "step": 1828
    },
    {
      "epoch": 2.3151898734177214,
      "grad_norm": 1.3537086248397827,
      "learning_rate": 5e-05,
      "loss": 1.4134,
      "step": 1829
    },
    {
      "epoch": 2.3164556962025316,
      "grad_norm": 1.2960253953933716,
      "learning_rate": 5e-05,
      "loss": 1.3167,
      "step": 1830
    },
    {
      "epoch": 2.3177215189873417,
      "grad_norm": 1.2916845083236694,
      "learning_rate": 5e-05,
      "loss": 1.3802,
      "step": 1831
    },
    {
      "epoch": 2.318987341772152,
      "grad_norm": 1.328977346420288,
      "learning_rate": 5e-05,
      "loss": 1.3504,
      "step": 1832
    },
    {
      "epoch": 2.320253164556962,
      "grad_norm": 1.315025806427002,
      "learning_rate": 5e-05,
      "loss": 1.2386,
      "step": 1833
    },
    {
      "epoch": 2.321518987341772,
      "grad_norm": 1.3014793395996094,
      "learning_rate": 5e-05,
      "loss": 1.2874,
      "step": 1834
    },
    {
      "epoch": 2.3227848101265822,
      "grad_norm": 1.3552722930908203,
      "learning_rate": 5e-05,
      "loss": 1.3247,
      "step": 1835
    },
    {
      "epoch": 2.3240506329113924,
      "grad_norm": 1.2867985963821411,
      "learning_rate": 5e-05,
      "loss": 1.2753,
      "step": 1836
    },
    {
      "epoch": 2.3253164556962025,
      "grad_norm": 1.3603248596191406,
      "learning_rate": 5e-05,
      "loss": 1.3676,
      "step": 1837
    },
    {
      "epoch": 2.3265822784810126,
      "grad_norm": 1.3021575212478638,
      "learning_rate": 5e-05,
      "loss": 1.2796,
      "step": 1838
    },
    {
      "epoch": 2.3278481012658228,
      "grad_norm": 1.3032249212265015,
      "learning_rate": 5e-05,
      "loss": 1.3523,
      "step": 1839
    },
    {
      "epoch": 2.329113924050633,
      "grad_norm": 1.3390346765518188,
      "learning_rate": 5e-05,
      "loss": 1.2985,
      "step": 1840
    },
    {
      "epoch": 2.330379746835443,
      "grad_norm": 1.3405139446258545,
      "learning_rate": 5e-05,
      "loss": 1.4179,
      "step": 1841
    },
    {
      "epoch": 2.331645569620253,
      "grad_norm": 1.298160195350647,
      "learning_rate": 5e-05,
      "loss": 1.2778,
      "step": 1842
    },
    {
      "epoch": 2.3329113924050633,
      "grad_norm": 1.3330057859420776,
      "learning_rate": 5e-05,
      "loss": 1.3358,
      "step": 1843
    },
    {
      "epoch": 2.3341772151898734,
      "grad_norm": 1.2951637506484985,
      "learning_rate": 5e-05,
      "loss": 1.2996,
      "step": 1844
    },
    {
      "epoch": 2.3354430379746836,
      "grad_norm": 1.2992457151412964,
      "learning_rate": 5e-05,
      "loss": 1.4052,
      "step": 1845
    },
    {
      "epoch": 2.3367088607594937,
      "grad_norm": 1.323734164237976,
      "learning_rate": 5e-05,
      "loss": 1.3793,
      "step": 1846
    },
    {
      "epoch": 2.337974683544304,
      "grad_norm": 1.3327829837799072,
      "learning_rate": 5e-05,
      "loss": 1.3444,
      "step": 1847
    },
    {
      "epoch": 2.339240506329114,
      "grad_norm": 1.326854944229126,
      "learning_rate": 5e-05,
      "loss": 1.2792,
      "step": 1848
    },
    {
      "epoch": 2.340506329113924,
      "grad_norm": 1.2824163436889648,
      "learning_rate": 5e-05,
      "loss": 1.3357,
      "step": 1849
    },
    {
      "epoch": 2.3417721518987342,
      "grad_norm": 1.3207426071166992,
      "learning_rate": 5e-05,
      "loss": 1.2463,
      "step": 1850
    },
    {
      "epoch": 2.3430379746835444,
      "grad_norm": 1.325993537902832,
      "learning_rate": 5e-05,
      "loss": 1.267,
      "step": 1851
    },
    {
      "epoch": 2.3443037974683545,
      "grad_norm": 1.3100478649139404,
      "learning_rate": 5e-05,
      "loss": 1.3026,
      "step": 1852
    },
    {
      "epoch": 2.3455696202531646,
      "grad_norm": 1.3219537734985352,
      "learning_rate": 5e-05,
      "loss": 1.323,
      "step": 1853
    },
    {
      "epoch": 2.3468354430379748,
      "grad_norm": 1.3038196563720703,
      "learning_rate": 5e-05,
      "loss": 1.2881,
      "step": 1854
    },
    {
      "epoch": 2.348101265822785,
      "grad_norm": 1.3260453939437866,
      "learning_rate": 5e-05,
      "loss": 1.2896,
      "step": 1855
    },
    {
      "epoch": 2.349367088607595,
      "grad_norm": 1.3209949731826782,
      "learning_rate": 5e-05,
      "loss": 1.28,
      "step": 1856
    },
    {
      "epoch": 2.350632911392405,
      "grad_norm": 1.312745213508606,
      "learning_rate": 5e-05,
      "loss": 1.3096,
      "step": 1857
    },
    {
      "epoch": 2.3518987341772153,
      "grad_norm": 1.3469609022140503,
      "learning_rate": 5e-05,
      "loss": 1.4009,
      "step": 1858
    },
    {
      "epoch": 2.3531645569620254,
      "grad_norm": 1.2926750183105469,
      "learning_rate": 5e-05,
      "loss": 1.2997,
      "step": 1859
    },
    {
      "epoch": 2.3544303797468356,
      "grad_norm": 1.3398911952972412,
      "learning_rate": 5e-05,
      "loss": 1.313,
      "step": 1860
    },
    {
      "epoch": 2.3556962025316457,
      "grad_norm": 1.3020802736282349,
      "learning_rate": 5e-05,
      "loss": 1.3165,
      "step": 1861
    },
    {
      "epoch": 2.356962025316456,
      "grad_norm": 1.3378171920776367,
      "learning_rate": 5e-05,
      "loss": 1.2738,
      "step": 1862
    },
    {
      "epoch": 2.358227848101266,
      "grad_norm": 1.3497270345687866,
      "learning_rate": 5e-05,
      "loss": 1.3343,
      "step": 1863
    },
    {
      "epoch": 2.359493670886076,
      "grad_norm": 1.3199228048324585,
      "learning_rate": 5e-05,
      "loss": 1.2406,
      "step": 1864
    },
    {
      "epoch": 2.3607594936708862,
      "grad_norm": 1.3368284702301025,
      "learning_rate": 5e-05,
      "loss": 1.341,
      "step": 1865
    },
    {
      "epoch": 2.3620253164556964,
      "grad_norm": 1.3828322887420654,
      "learning_rate": 5e-05,
      "loss": 1.3996,
      "step": 1866
    },
    {
      "epoch": 2.3632911392405065,
      "grad_norm": 1.3431437015533447,
      "learning_rate": 5e-05,
      "loss": 1.3888,
      "step": 1867
    },
    {
      "epoch": 2.3645569620253166,
      "grad_norm": 1.2952394485473633,
      "learning_rate": 5e-05,
      "loss": 1.2799,
      "step": 1868
    },
    {
      "epoch": 2.3658227848101268,
      "grad_norm": 1.2958519458770752,
      "learning_rate": 5e-05,
      "loss": 1.287,
      "step": 1869
    },
    {
      "epoch": 2.367088607594937,
      "grad_norm": 1.3012776374816895,
      "learning_rate": 5e-05,
      "loss": 1.2596,
      "step": 1870
    },
    {
      "epoch": 2.368354430379747,
      "grad_norm": 1.330906629562378,
      "learning_rate": 5e-05,
      "loss": 1.318,
      "step": 1871
    },
    {
      "epoch": 2.369620253164557,
      "grad_norm": 1.3151705265045166,
      "learning_rate": 5e-05,
      "loss": 1.3009,
      "step": 1872
    },
    {
      "epoch": 2.3708860759493673,
      "grad_norm": 1.335842490196228,
      "learning_rate": 5e-05,
      "loss": 1.2882,
      "step": 1873
    },
    {
      "epoch": 2.3721518987341774,
      "grad_norm": 1.3208320140838623,
      "learning_rate": 5e-05,
      "loss": 1.313,
      "step": 1874
    },
    {
      "epoch": 2.3734177215189876,
      "grad_norm": 1.301229476928711,
      "learning_rate": 5e-05,
      "loss": 1.312,
      "step": 1875
    },
    {
      "epoch": 2.3746835443037977,
      "grad_norm": 1.3392841815948486,
      "learning_rate": 5e-05,
      "loss": 1.2838,
      "step": 1876
    },
    {
      "epoch": 2.375949367088608,
      "grad_norm": 1.2872272729873657,
      "learning_rate": 5e-05,
      "loss": 1.2565,
      "step": 1877
    },
    {
      "epoch": 2.377215189873418,
      "grad_norm": 1.325483798980713,
      "learning_rate": 5e-05,
      "loss": 1.2934,
      "step": 1878
    },
    {
      "epoch": 2.378481012658228,
      "grad_norm": 1.3114179372787476,
      "learning_rate": 5e-05,
      "loss": 1.3315,
      "step": 1879
    },
    {
      "epoch": 2.379746835443038,
      "grad_norm": 1.3315656185150146,
      "learning_rate": 5e-05,
      "loss": 1.2797,
      "step": 1880
    },
    {
      "epoch": 2.381012658227848,
      "grad_norm": 1.320920467376709,
      "learning_rate": 5e-05,
      "loss": 1.3039,
      "step": 1881
    },
    {
      "epoch": 2.382278481012658,
      "grad_norm": 1.305793046951294,
      "learning_rate": 5e-05,
      "loss": 1.3473,
      "step": 1882
    },
    {
      "epoch": 2.383544303797468,
      "grad_norm": 1.3732268810272217,
      "learning_rate": 5e-05,
      "loss": 1.3439,
      "step": 1883
    },
    {
      "epoch": 2.3848101265822783,
      "grad_norm": 1.3722846508026123,
      "learning_rate": 5e-05,
      "loss": 1.3531,
      "step": 1884
    },
    {
      "epoch": 2.3860759493670884,
      "grad_norm": 1.3559837341308594,
      "learning_rate": 5e-05,
      "loss": 1.318,
      "step": 1885
    },
    {
      "epoch": 2.3873417721518986,
      "grad_norm": 1.3352848291397095,
      "learning_rate": 5e-05,
      "loss": 1.3069,
      "step": 1886
    },
    {
      "epoch": 2.3886075949367087,
      "grad_norm": 1.3192545175552368,
      "learning_rate": 5e-05,
      "loss": 1.2158,
      "step": 1887
    },
    {
      "epoch": 2.389873417721519,
      "grad_norm": 1.3119325637817383,
      "learning_rate": 5e-05,
      "loss": 1.2719,
      "step": 1888
    },
    {
      "epoch": 2.391139240506329,
      "grad_norm": 1.3175544738769531,
      "learning_rate": 5e-05,
      "loss": 1.2697,
      "step": 1889
    },
    {
      "epoch": 2.392405063291139,
      "grad_norm": 1.3690531253814697,
      "learning_rate": 5e-05,
      "loss": 1.3526,
      "step": 1890
    },
    {
      "epoch": 2.3936708860759492,
      "grad_norm": 1.3029522895812988,
      "learning_rate": 5e-05,
      "loss": 1.3261,
      "step": 1891
    },
    {
      "epoch": 2.3949367088607594,
      "grad_norm": 1.3521344661712646,
      "learning_rate": 5e-05,
      "loss": 1.3584,
      "step": 1892
    },
    {
      "epoch": 2.3962025316455695,
      "grad_norm": 1.306739330291748,
      "learning_rate": 5e-05,
      "loss": 1.2712,
      "step": 1893
    },
    {
      "epoch": 2.3974683544303796,
      "grad_norm": 1.3344897031784058,
      "learning_rate": 5e-05,
      "loss": 1.3204,
      "step": 1894
    },
    {
      "epoch": 2.3987341772151898,
      "grad_norm": 1.3450450897216797,
      "learning_rate": 5e-05,
      "loss": 1.3734,
      "step": 1895
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.3273601531982422,
      "learning_rate": 5e-05,
      "loss": 1.3347,
      "step": 1896
    },
    {
      "epoch": 2.40126582278481,
      "grad_norm": 1.3139562606811523,
      "learning_rate": 5e-05,
      "loss": 1.283,
      "step": 1897
    },
    {
      "epoch": 2.40253164556962,
      "grad_norm": 1.2827844619750977,
      "learning_rate": 5e-05,
      "loss": 1.2635,
      "step": 1898
    },
    {
      "epoch": 2.4037974683544303,
      "grad_norm": 1.301542043685913,
      "learning_rate": 5e-05,
      "loss": 1.2227,
      "step": 1899
    },
    {
      "epoch": 2.4050632911392404,
      "grad_norm": 1.2606406211853027,
      "learning_rate": 5e-05,
      "loss": 1.2061,
      "step": 1900
    },
    {
      "epoch": 2.4063291139240506,
      "grad_norm": 1.2893913984298706,
      "learning_rate": 5e-05,
      "loss": 1.2963,
      "step": 1901
    },
    {
      "epoch": 2.4075949367088607,
      "grad_norm": 1.352904200553894,
      "learning_rate": 5e-05,
      "loss": 1.3537,
      "step": 1902
    },
    {
      "epoch": 2.408860759493671,
      "grad_norm": 1.2964682579040527,
      "learning_rate": 5e-05,
      "loss": 1.2611,
      "step": 1903
    },
    {
      "epoch": 2.410126582278481,
      "grad_norm": 1.3373152017593384,
      "learning_rate": 5e-05,
      "loss": 1.2754,
      "step": 1904
    },
    {
      "epoch": 2.411392405063291,
      "grad_norm": 1.3047831058502197,
      "learning_rate": 5e-05,
      "loss": 1.2796,
      "step": 1905
    },
    {
      "epoch": 2.4126582278481012,
      "grad_norm": 1.3316045999526978,
      "learning_rate": 5e-05,
      "loss": 1.2958,
      "step": 1906
    },
    {
      "epoch": 2.4139240506329114,
      "grad_norm": 1.306222915649414,
      "learning_rate": 5e-05,
      "loss": 1.2746,
      "step": 1907
    },
    {
      "epoch": 2.4151898734177215,
      "grad_norm": 1.326035499572754,
      "learning_rate": 5e-05,
      "loss": 1.2507,
      "step": 1908
    },
    {
      "epoch": 2.4164556962025316,
      "grad_norm": 1.3149683475494385,
      "learning_rate": 5e-05,
      "loss": 1.1925,
      "step": 1909
    },
    {
      "epoch": 2.4177215189873418,
      "grad_norm": 1.3069154024124146,
      "learning_rate": 5e-05,
      "loss": 1.3044,
      "step": 1910
    },
    {
      "epoch": 2.418987341772152,
      "grad_norm": 1.300500512123108,
      "learning_rate": 5e-05,
      "loss": 1.2526,
      "step": 1911
    },
    {
      "epoch": 2.420253164556962,
      "grad_norm": 1.3220020532608032,
      "learning_rate": 5e-05,
      "loss": 1.2886,
      "step": 1912
    },
    {
      "epoch": 2.421518987341772,
      "grad_norm": 1.3739067316055298,
      "learning_rate": 5e-05,
      "loss": 1.3336,
      "step": 1913
    },
    {
      "epoch": 2.4227848101265823,
      "grad_norm": 1.3338854312896729,
      "learning_rate": 5e-05,
      "loss": 1.3129,
      "step": 1914
    },
    {
      "epoch": 2.4240506329113924,
      "grad_norm": 1.3609970808029175,
      "learning_rate": 5e-05,
      "loss": 1.2897,
      "step": 1915
    },
    {
      "epoch": 2.4253164556962026,
      "grad_norm": 1.337767481803894,
      "learning_rate": 5e-05,
      "loss": 1.3298,
      "step": 1916
    },
    {
      "epoch": 2.4265822784810127,
      "grad_norm": 1.3612191677093506,
      "learning_rate": 5e-05,
      "loss": 1.3117,
      "step": 1917
    },
    {
      "epoch": 2.427848101265823,
      "grad_norm": 1.3245741128921509,
      "learning_rate": 5e-05,
      "loss": 1.318,
      "step": 1918
    },
    {
      "epoch": 2.429113924050633,
      "grad_norm": 1.3386781215667725,
      "learning_rate": 5e-05,
      "loss": 1.2794,
      "step": 1919
    },
    {
      "epoch": 2.430379746835443,
      "grad_norm": 1.3292789459228516,
      "learning_rate": 5e-05,
      "loss": 1.2887,
      "step": 1920
    },
    {
      "epoch": 2.4316455696202532,
      "grad_norm": 1.3120062351226807,
      "learning_rate": 5e-05,
      "loss": 1.2912,
      "step": 1921
    },
    {
      "epoch": 2.4329113924050634,
      "grad_norm": 1.2989742755889893,
      "learning_rate": 5e-05,
      "loss": 1.2586,
      "step": 1922
    },
    {
      "epoch": 2.4341772151898735,
      "grad_norm": 1.3204957246780396,
      "learning_rate": 5e-05,
      "loss": 1.3003,
      "step": 1923
    },
    {
      "epoch": 2.4354430379746836,
      "grad_norm": 1.297869324684143,
      "learning_rate": 5e-05,
      "loss": 1.3415,
      "step": 1924
    },
    {
      "epoch": 2.4367088607594938,
      "grad_norm": 1.3097225427627563,
      "learning_rate": 5e-05,
      "loss": 1.2725,
      "step": 1925
    },
    {
      "epoch": 2.437974683544304,
      "grad_norm": 1.3168327808380127,
      "learning_rate": 5e-05,
      "loss": 1.3265,
      "step": 1926
    },
    {
      "epoch": 2.439240506329114,
      "grad_norm": 1.3301477432250977,
      "learning_rate": 5e-05,
      "loss": 1.4044,
      "step": 1927
    },
    {
      "epoch": 2.440506329113924,
      "grad_norm": 1.2840303182601929,
      "learning_rate": 5e-05,
      "loss": 1.2689,
      "step": 1928
    },
    {
      "epoch": 2.4417721518987343,
      "grad_norm": 1.307565450668335,
      "learning_rate": 5e-05,
      "loss": 1.2285,
      "step": 1929
    },
    {
      "epoch": 2.4430379746835444,
      "grad_norm": 1.2920970916748047,
      "learning_rate": 5e-05,
      "loss": 1.2211,
      "step": 1930
    },
    {
      "epoch": 2.4443037974683546,
      "grad_norm": 1.2607096433639526,
      "learning_rate": 5e-05,
      "loss": 1.2037,
      "step": 1931
    },
    {
      "epoch": 2.4455696202531647,
      "grad_norm": 1.296578288078308,
      "learning_rate": 5e-05,
      "loss": 1.2502,
      "step": 1932
    },
    {
      "epoch": 2.446835443037975,
      "grad_norm": 1.32290518283844,
      "learning_rate": 5e-05,
      "loss": 1.2823,
      "step": 1933
    },
    {
      "epoch": 2.448101265822785,
      "grad_norm": 1.363900065422058,
      "learning_rate": 5e-05,
      "loss": 1.3467,
      "step": 1934
    },
    {
      "epoch": 2.449367088607595,
      "grad_norm": 1.3138610124588013,
      "learning_rate": 5e-05,
      "loss": 1.2543,
      "step": 1935
    },
    {
      "epoch": 2.4506329113924052,
      "grad_norm": 1.3121106624603271,
      "learning_rate": 5e-05,
      "loss": 1.233,
      "step": 1936
    },
    {
      "epoch": 2.4518987341772154,
      "grad_norm": 1.2635880708694458,
      "learning_rate": 5e-05,
      "loss": 1.2324,
      "step": 1937
    },
    {
      "epoch": 2.453164556962025,
      "grad_norm": 1.270768404006958,
      "learning_rate": 5e-05,
      "loss": 1.2251,
      "step": 1938
    },
    {
      "epoch": 2.454430379746835,
      "grad_norm": 1.2980488538742065,
      "learning_rate": 5e-05,
      "loss": 1.2671,
      "step": 1939
    },
    {
      "epoch": 2.4556962025316453,
      "grad_norm": 1.2723723649978638,
      "learning_rate": 5e-05,
      "loss": 1.2456,
      "step": 1940
    },
    {
      "epoch": 2.4569620253164555,
      "grad_norm": 1.3573306798934937,
      "learning_rate": 5e-05,
      "loss": 1.3247,
      "step": 1941
    },
    {
      "epoch": 2.4582278481012656,
      "grad_norm": 1.3350191116333008,
      "learning_rate": 5e-05,
      "loss": 1.3143,
      "step": 1942
    },
    {
      "epoch": 2.4594936708860757,
      "grad_norm": 1.3598315715789795,
      "learning_rate": 5e-05,
      "loss": 1.3301,
      "step": 1943
    },
    {
      "epoch": 2.460759493670886,
      "grad_norm": 1.3095968961715698,
      "learning_rate": 5e-05,
      "loss": 1.237,
      "step": 1944
    },
    {
      "epoch": 2.462025316455696,
      "grad_norm": 1.3660715818405151,
      "learning_rate": 5e-05,
      "loss": 1.3261,
      "step": 1945
    },
    {
      "epoch": 2.463291139240506,
      "grad_norm": 1.3275467157363892,
      "learning_rate": 5e-05,
      "loss": 1.2932,
      "step": 1946
    },
    {
      "epoch": 2.4645569620253163,
      "grad_norm": 1.331892967224121,
      "learning_rate": 5e-05,
      "loss": 1.2953,
      "step": 1947
    },
    {
      "epoch": 2.4658227848101264,
      "grad_norm": 1.314638614654541,
      "learning_rate": 5e-05,
      "loss": 1.3273,
      "step": 1948
    },
    {
      "epoch": 2.4670886075949365,
      "grad_norm": 1.29532790184021,
      "learning_rate": 5e-05,
      "loss": 1.2862,
      "step": 1949
    },
    {
      "epoch": 2.4683544303797467,
      "grad_norm": 1.347723126411438,
      "learning_rate": 5e-05,
      "loss": 1.2443,
      "step": 1950
    },
    {
      "epoch": 2.469620253164557,
      "grad_norm": 1.3098347187042236,
      "learning_rate": 5e-05,
      "loss": 1.2498,
      "step": 1951
    },
    {
      "epoch": 2.470886075949367,
      "grad_norm": 1.2995469570159912,
      "learning_rate": 5e-05,
      "loss": 1.2828,
      "step": 1952
    },
    {
      "epoch": 2.472151898734177,
      "grad_norm": 1.3420814275741577,
      "learning_rate": 5e-05,
      "loss": 1.3084,
      "step": 1953
    },
    {
      "epoch": 2.473417721518987,
      "grad_norm": 1.326003909111023,
      "learning_rate": 5e-05,
      "loss": 1.342,
      "step": 1954
    },
    {
      "epoch": 2.4746835443037973,
      "grad_norm": 1.3245162963867188,
      "learning_rate": 5e-05,
      "loss": 1.2739,
      "step": 1955
    },
    {
      "epoch": 2.4759493670886075,
      "grad_norm": 1.339202642440796,
      "learning_rate": 5e-05,
      "loss": 1.2949,
      "step": 1956
    },
    {
      "epoch": 2.4772151898734176,
      "grad_norm": 1.3465179204940796,
      "learning_rate": 5e-05,
      "loss": 1.2916,
      "step": 1957
    },
    {
      "epoch": 2.4784810126582277,
      "grad_norm": 1.3562638759613037,
      "learning_rate": 5e-05,
      "loss": 1.2928,
      "step": 1958
    },
    {
      "epoch": 2.479746835443038,
      "grad_norm": 1.3395806550979614,
      "learning_rate": 5e-05,
      "loss": 1.2907,
      "step": 1959
    },
    {
      "epoch": 2.481012658227848,
      "grad_norm": 1.3172695636749268,
      "learning_rate": 5e-05,
      "loss": 1.3126,
      "step": 1960
    },
    {
      "epoch": 2.482278481012658,
      "grad_norm": 1.2791109085083008,
      "learning_rate": 5e-05,
      "loss": 1.2116,
      "step": 1961
    },
    {
      "epoch": 2.4835443037974683,
      "grad_norm": 1.3146617412567139,
      "learning_rate": 5e-05,
      "loss": 1.2882,
      "step": 1962
    },
    {
      "epoch": 2.4848101265822784,
      "grad_norm": 1.375213861465454,
      "learning_rate": 5e-05,
      "loss": 1.3369,
      "step": 1963
    },
    {
      "epoch": 2.4860759493670885,
      "grad_norm": 1.2987539768218994,
      "learning_rate": 5e-05,
      "loss": 1.292,
      "step": 1964
    },
    {
      "epoch": 2.4873417721518987,
      "grad_norm": 1.3508365154266357,
      "learning_rate": 5e-05,
      "loss": 1.2901,
      "step": 1965
    },
    {
      "epoch": 2.488607594936709,
      "grad_norm": 1.3912980556488037,
      "learning_rate": 5e-05,
      "loss": 1.3314,
      "step": 1966
    },
    {
      "epoch": 2.489873417721519,
      "grad_norm": 1.3160487413406372,
      "learning_rate": 5e-05,
      "loss": 1.2642,
      "step": 1967
    },
    {
      "epoch": 2.491139240506329,
      "grad_norm": 1.3091038465499878,
      "learning_rate": 5e-05,
      "loss": 1.2346,
      "step": 1968
    },
    {
      "epoch": 2.492405063291139,
      "grad_norm": 1.343620777130127,
      "learning_rate": 5e-05,
      "loss": 1.2859,
      "step": 1969
    },
    {
      "epoch": 2.4936708860759493,
      "grad_norm": 1.320168375968933,
      "learning_rate": 5e-05,
      "loss": 1.3099,
      "step": 1970
    },
    {
      "epoch": 2.4949367088607595,
      "grad_norm": 1.286399245262146,
      "learning_rate": 5e-05,
      "loss": 1.2456,
      "step": 1971
    },
    {
      "epoch": 2.4962025316455696,
      "grad_norm": 1.3550258874893188,
      "learning_rate": 5e-05,
      "loss": 1.2807,
      "step": 1972
    },
    {
      "epoch": 2.4974683544303797,
      "grad_norm": 1.316794514656067,
      "learning_rate": 5e-05,
      "loss": 1.2343,
      "step": 1973
    },
    {
      "epoch": 2.49873417721519,
      "grad_norm": 1.317385196685791,
      "learning_rate": 5e-05,
      "loss": 1.2762,
      "step": 1974
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.3280586004257202,
      "learning_rate": 5e-05,
      "loss": 1.2472,
      "step": 1975
    },
    {
      "epoch": 2.5,
      "eval_loss": 2.0170059204101562,
      "eval_runtime": 753.9187,
      "eval_samples_per_second": 119.217,
      "eval_steps_per_second": 0.932,
      "step": 1975
    },
    {
      "epoch": 2.50126582278481,
      "grad_norm": 1.3709732294082642,
      "learning_rate": 5e-05,
      "loss": 1.285,
      "step": 1976
    },
    {
      "epoch": 2.5025316455696203,
      "grad_norm": 1.3468310832977295,
      "learning_rate": 5e-05,
      "loss": 1.308,
      "step": 1977
    },
    {
      "epoch": 2.5037974683544304,
      "grad_norm": 1.3073506355285645,
      "learning_rate": 5e-05,
      "loss": 1.1958,
      "step": 1978
    },
    {
      "epoch": 2.5050632911392405,
      "grad_norm": 1.2870970964431763,
      "learning_rate": 5e-05,
      "loss": 1.1715,
      "step": 1979
    },
    {
      "epoch": 2.5063291139240507,
      "grad_norm": 1.315454363822937,
      "learning_rate": 5e-05,
      "loss": 1.3076,
      "step": 1980
    },
    {
      "epoch": 2.507594936708861,
      "grad_norm": 1.2881779670715332,
      "learning_rate": 5e-05,
      "loss": 1.2423,
      "step": 1981
    },
    {
      "epoch": 2.508860759493671,
      "grad_norm": 1.3022910356521606,
      "learning_rate": 5e-05,
      "loss": 1.2482,
      "step": 1982
    },
    {
      "epoch": 2.510126582278481,
      "grad_norm": 1.3270049095153809,
      "learning_rate": 5e-05,
      "loss": 1.2254,
      "step": 1983
    },
    {
      "epoch": 2.511392405063291,
      "grad_norm": 1.334287166595459,
      "learning_rate": 5e-05,
      "loss": 1.3125,
      "step": 1984
    },
    {
      "epoch": 2.5126582278481013,
      "grad_norm": 1.2788604497909546,
      "learning_rate": 5e-05,
      "loss": 1.2253,
      "step": 1985
    },
    {
      "epoch": 2.5139240506329115,
      "grad_norm": 1.3240755796432495,
      "learning_rate": 5e-05,
      "loss": 1.3192,
      "step": 1986
    },
    {
      "epoch": 2.5151898734177216,
      "grad_norm": 1.3619898557662964,
      "learning_rate": 5e-05,
      "loss": 1.2399,
      "step": 1987
    },
    {
      "epoch": 2.5164556962025317,
      "grad_norm": 1.3427791595458984,
      "learning_rate": 5e-05,
      "loss": 1.338,
      "step": 1988
    },
    {
      "epoch": 2.517721518987342,
      "grad_norm": 1.2852952480316162,
      "learning_rate": 5e-05,
      "loss": 1.2792,
      "step": 1989
    },
    {
      "epoch": 2.518987341772152,
      "grad_norm": 1.373461127281189,
      "learning_rate": 5e-05,
      "loss": 1.3636,
      "step": 1990
    },
    {
      "epoch": 2.520253164556962,
      "grad_norm": 1.3283839225769043,
      "learning_rate": 5e-05,
      "loss": 1.3292,
      "step": 1991
    },
    {
      "epoch": 2.5215189873417723,
      "grad_norm": 1.331844687461853,
      "learning_rate": 5e-05,
      "loss": 1.3078,
      "step": 1992
    },
    {
      "epoch": 2.5227848101265824,
      "grad_norm": 1.2962493896484375,
      "learning_rate": 5e-05,
      "loss": 1.2586,
      "step": 1993
    },
    {
      "epoch": 2.5240506329113925,
      "grad_norm": 1.3129279613494873,
      "learning_rate": 5e-05,
      "loss": 1.2076,
      "step": 1994
    },
    {
      "epoch": 2.5253164556962027,
      "grad_norm": 1.3026039600372314,
      "learning_rate": 5e-05,
      "loss": 1.2361,
      "step": 1995
    },
    {
      "epoch": 2.526582278481013,
      "grad_norm": 1.2671358585357666,
      "learning_rate": 5e-05,
      "loss": 1.2187,
      "step": 1996
    },
    {
      "epoch": 2.527848101265823,
      "grad_norm": 1.3624788522720337,
      "learning_rate": 5e-05,
      "loss": 1.2683,
      "step": 1997
    },
    {
      "epoch": 2.529113924050633,
      "grad_norm": 1.3196499347686768,
      "learning_rate": 5e-05,
      "loss": 1.2476,
      "step": 1998
    },
    {
      "epoch": 2.530379746835443,
      "grad_norm": 1.3165159225463867,
      "learning_rate": 5e-05,
      "loss": 1.2857,
      "step": 1999
    },
    {
      "epoch": 2.5316455696202533,
      "grad_norm": 1.3125452995300293,
      "learning_rate": 5e-05,
      "loss": 1.2543,
      "step": 2000
    },
    {
      "epoch": 2.5329113924050635,
      "grad_norm": 1.3397157192230225,
      "learning_rate": 5e-05,
      "loss": 1.3191,
      "step": 2001
    },
    {
      "epoch": 2.5341772151898736,
      "grad_norm": 1.311571717262268,
      "learning_rate": 5e-05,
      "loss": 1.3101,
      "step": 2002
    },
    {
      "epoch": 2.5354430379746837,
      "grad_norm": 1.331860065460205,
      "learning_rate": 5e-05,
      "loss": 1.2884,
      "step": 2003
    },
    {
      "epoch": 2.536708860759494,
      "grad_norm": 1.2829521894454956,
      "learning_rate": 5e-05,
      "loss": 1.1925,
      "step": 2004
    },
    {
      "epoch": 2.537974683544304,
      "grad_norm": 1.3298457860946655,
      "learning_rate": 5e-05,
      "loss": 1.3065,
      "step": 2005
    },
    {
      "epoch": 2.539240506329114,
      "grad_norm": 1.311737060546875,
      "learning_rate": 5e-05,
      "loss": 1.2609,
      "step": 2006
    },
    {
      "epoch": 2.5405063291139243,
      "grad_norm": 1.324102759361267,
      "learning_rate": 5e-05,
      "loss": 1.2361,
      "step": 2007
    },
    {
      "epoch": 2.5417721518987344,
      "grad_norm": 1.304705262184143,
      "learning_rate": 5e-05,
      "loss": 1.2527,
      "step": 2008
    },
    {
      "epoch": 2.5430379746835445,
      "grad_norm": 1.31430983543396,
      "learning_rate": 5e-05,
      "loss": 1.3355,
      "step": 2009
    },
    {
      "epoch": 2.5443037974683547,
      "grad_norm": 1.339000940322876,
      "learning_rate": 5e-05,
      "loss": 1.2308,
      "step": 2010
    },
    {
      "epoch": 2.545569620253165,
      "grad_norm": 1.3236947059631348,
      "learning_rate": 5e-05,
      "loss": 1.2971,
      "step": 2011
    },
    {
      "epoch": 2.546835443037975,
      "grad_norm": 1.3314024209976196,
      "learning_rate": 5e-05,
      "loss": 1.3209,
      "step": 2012
    },
    {
      "epoch": 2.548101265822785,
      "grad_norm": 1.315269112586975,
      "learning_rate": 5e-05,
      "loss": 1.2645,
      "step": 2013
    },
    {
      "epoch": 2.549367088607595,
      "grad_norm": 1.3352108001708984,
      "learning_rate": 5e-05,
      "loss": 1.3665,
      "step": 2014
    },
    {
      "epoch": 2.5506329113924053,
      "grad_norm": 1.27658212184906,
      "learning_rate": 5e-05,
      "loss": 1.2266,
      "step": 2015
    },
    {
      "epoch": 2.5518987341772155,
      "grad_norm": 1.2779465913772583,
      "learning_rate": 5e-05,
      "loss": 1.2488,
      "step": 2016
    },
    {
      "epoch": 2.553164556962025,
      "grad_norm": 1.3052425384521484,
      "learning_rate": 5e-05,
      "loss": 1.2502,
      "step": 2017
    },
    {
      "epoch": 2.5544303797468353,
      "grad_norm": 1.313004493713379,
      "learning_rate": 5e-05,
      "loss": 1.2633,
      "step": 2018
    },
    {
      "epoch": 2.5556962025316454,
      "grad_norm": 1.2570666074752808,
      "learning_rate": 5e-05,
      "loss": 1.1507,
      "step": 2019
    },
    {
      "epoch": 2.5569620253164556,
      "grad_norm": 1.3213740587234497,
      "learning_rate": 5e-05,
      "loss": 1.249,
      "step": 2020
    },
    {
      "epoch": 2.5582278481012657,
      "grad_norm": 1.3365179300308228,
      "learning_rate": 5e-05,
      "loss": 1.3107,
      "step": 2021
    },
    {
      "epoch": 2.559493670886076,
      "grad_norm": 1.3570611476898193,
      "learning_rate": 5e-05,
      "loss": 1.3441,
      "step": 2022
    },
    {
      "epoch": 2.560759493670886,
      "grad_norm": 1.2962123155593872,
      "learning_rate": 5e-05,
      "loss": 1.2693,
      "step": 2023
    },
    {
      "epoch": 2.562025316455696,
      "grad_norm": 1.2876436710357666,
      "learning_rate": 5e-05,
      "loss": 1.2331,
      "step": 2024
    },
    {
      "epoch": 2.5632911392405062,
      "grad_norm": 1.3634692430496216,
      "learning_rate": 5e-05,
      "loss": 1.2912,
      "step": 2025
    },
    {
      "epoch": 2.5645569620253164,
      "grad_norm": 1.31529700756073,
      "learning_rate": 5e-05,
      "loss": 1.2371,
      "step": 2026
    },
    {
      "epoch": 2.5658227848101265,
      "grad_norm": 1.3420039415359497,
      "learning_rate": 5e-05,
      "loss": 1.2667,
      "step": 2027
    },
    {
      "epoch": 2.5670886075949366,
      "grad_norm": 1.3137049674987793,
      "learning_rate": 5e-05,
      "loss": 1.1895,
      "step": 2028
    },
    {
      "epoch": 2.5683544303797468,
      "grad_norm": 1.3267781734466553,
      "learning_rate": 5e-05,
      "loss": 1.2826,
      "step": 2029
    },
    {
      "epoch": 2.569620253164557,
      "grad_norm": 1.2741072177886963,
      "learning_rate": 5e-05,
      "loss": 1.2339,
      "step": 2030
    },
    {
      "epoch": 2.570886075949367,
      "grad_norm": 1.416266918182373,
      "learning_rate": 5e-05,
      "loss": 1.3075,
      "step": 2031
    },
    {
      "epoch": 2.572151898734177,
      "grad_norm": 1.301805853843689,
      "learning_rate": 5e-05,
      "loss": 1.2543,
      "step": 2032
    },
    {
      "epoch": 2.5734177215189873,
      "grad_norm": 1.3031225204467773,
      "learning_rate": 5e-05,
      "loss": 1.2336,
      "step": 2033
    },
    {
      "epoch": 2.5746835443037974,
      "grad_norm": 1.3343876600265503,
      "learning_rate": 5e-05,
      "loss": 1.2981,
      "step": 2034
    },
    {
      "epoch": 2.5759493670886076,
      "grad_norm": 1.3507262468338013,
      "learning_rate": 5e-05,
      "loss": 1.282,
      "step": 2035
    },
    {
      "epoch": 2.5772151898734177,
      "grad_norm": 1.310327410697937,
      "learning_rate": 5e-05,
      "loss": 1.2304,
      "step": 2036
    },
    {
      "epoch": 2.578481012658228,
      "grad_norm": 1.3124088048934937,
      "learning_rate": 5e-05,
      "loss": 1.2412,
      "step": 2037
    },
    {
      "epoch": 2.579746835443038,
      "grad_norm": 1.3287057876586914,
      "learning_rate": 5e-05,
      "loss": 1.2937,
      "step": 2038
    },
    {
      "epoch": 2.581012658227848,
      "grad_norm": 1.2949063777923584,
      "learning_rate": 5e-05,
      "loss": 1.2229,
      "step": 2039
    },
    {
      "epoch": 2.5822784810126582,
      "grad_norm": 1.2899906635284424,
      "learning_rate": 5e-05,
      "loss": 1.2219,
      "step": 2040
    },
    {
      "epoch": 2.5835443037974684,
      "grad_norm": 1.3228468894958496,
      "learning_rate": 5e-05,
      "loss": 1.2992,
      "step": 2041
    },
    {
      "epoch": 2.5848101265822785,
      "grad_norm": 1.359642505645752,
      "learning_rate": 5e-05,
      "loss": 1.281,
      "step": 2042
    },
    {
      "epoch": 2.5860759493670886,
      "grad_norm": 1.3078457117080688,
      "learning_rate": 5e-05,
      "loss": 1.2354,
      "step": 2043
    },
    {
      "epoch": 2.5873417721518988,
      "grad_norm": 1.3060173988342285,
      "learning_rate": 5e-05,
      "loss": 1.2094,
      "step": 2044
    },
    {
      "epoch": 2.588607594936709,
      "grad_norm": 1.3026331663131714,
      "learning_rate": 5e-05,
      "loss": 1.2773,
      "step": 2045
    },
    {
      "epoch": 2.589873417721519,
      "grad_norm": 1.3298559188842773,
      "learning_rate": 5e-05,
      "loss": 1.2972,
      "step": 2046
    },
    {
      "epoch": 2.591139240506329,
      "grad_norm": 1.3488293886184692,
      "learning_rate": 5e-05,
      "loss": 1.2714,
      "step": 2047
    },
    {
      "epoch": 2.5924050632911393,
      "grad_norm": 1.3444463014602661,
      "learning_rate": 5e-05,
      "loss": 1.242,
      "step": 2048
    },
    {
      "epoch": 2.5936708860759494,
      "grad_norm": 1.302474856376648,
      "learning_rate": 5e-05,
      "loss": 1.2492,
      "step": 2049
    },
    {
      "epoch": 2.5949367088607596,
      "grad_norm": 1.3508135080337524,
      "learning_rate": 5e-05,
      "loss": 1.2002,
      "step": 2050
    },
    {
      "epoch": 2.5962025316455697,
      "grad_norm": 1.299946665763855,
      "learning_rate": 5e-05,
      "loss": 1.2446,
      "step": 2051
    },
    {
      "epoch": 2.59746835443038,
      "grad_norm": 1.3106651306152344,
      "learning_rate": 5e-05,
      "loss": 1.2583,
      "step": 2052
    },
    {
      "epoch": 2.59873417721519,
      "grad_norm": 1.3272626399993896,
      "learning_rate": 5e-05,
      "loss": 1.2955,
      "step": 2053
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.347967267036438,
      "learning_rate": 5e-05,
      "loss": 1.2574,
      "step": 2054
    },
    {
      "epoch": 2.6012658227848102,
      "grad_norm": 1.3170667886734009,
      "learning_rate": 5e-05,
      "loss": 1.2505,
      "step": 2055
    },
    {
      "epoch": 2.6025316455696204,
      "grad_norm": 1.3348695039749146,
      "learning_rate": 5e-05,
      "loss": 1.2647,
      "step": 2056
    },
    {
      "epoch": 2.6037974683544305,
      "grad_norm": 1.3457518815994263,
      "learning_rate": 5e-05,
      "loss": 1.3039,
      "step": 2057
    },
    {
      "epoch": 2.6050632911392406,
      "grad_norm": 1.282444953918457,
      "learning_rate": 5e-05,
      "loss": 1.246,
      "step": 2058
    },
    {
      "epoch": 2.6063291139240508,
      "grad_norm": 1.2931482791900635,
      "learning_rate": 5e-05,
      "loss": 1.2204,
      "step": 2059
    },
    {
      "epoch": 2.607594936708861,
      "grad_norm": 1.309838056564331,
      "learning_rate": 5e-05,
      "loss": 1.2386,
      "step": 2060
    },
    {
      "epoch": 2.608860759493671,
      "grad_norm": 1.306515097618103,
      "learning_rate": 5e-05,
      "loss": 1.2001,
      "step": 2061
    },
    {
      "epoch": 2.610126582278481,
      "grad_norm": 1.3505232334136963,
      "learning_rate": 5e-05,
      "loss": 1.2825,
      "step": 2062
    },
    {
      "epoch": 2.6113924050632913,
      "grad_norm": 1.3116750717163086,
      "learning_rate": 5e-05,
      "loss": 1.2048,
      "step": 2063
    },
    {
      "epoch": 2.6126582278481014,
      "grad_norm": 1.3404589891433716,
      "learning_rate": 5e-05,
      "loss": 1.3543,
      "step": 2064
    },
    {
      "epoch": 2.6139240506329116,
      "grad_norm": 1.318993330001831,
      "learning_rate": 5e-05,
      "loss": 1.2644,
      "step": 2065
    },
    {
      "epoch": 2.6151898734177212,
      "grad_norm": 1.3292773962020874,
      "learning_rate": 5e-05,
      "loss": 1.2434,
      "step": 2066
    },
    {
      "epoch": 2.6164556962025314,
      "grad_norm": 1.3244444131851196,
      "learning_rate": 5e-05,
      "loss": 1.2868,
      "step": 2067
    },
    {
      "epoch": 2.6177215189873415,
      "grad_norm": 1.2970480918884277,
      "learning_rate": 5e-05,
      "loss": 1.2518,
      "step": 2068
    },
    {
      "epoch": 2.6189873417721516,
      "grad_norm": 1.3300728797912598,
      "learning_rate": 5e-05,
      "loss": 1.289,
      "step": 2069
    },
    {
      "epoch": 2.620253164556962,
      "grad_norm": 1.3475337028503418,
      "learning_rate": 5e-05,
      "loss": 1.2701,
      "step": 2070
    },
    {
      "epoch": 2.621518987341772,
      "grad_norm": 1.3006080389022827,
      "learning_rate": 5e-05,
      "loss": 1.2577,
      "step": 2071
    },
    {
      "epoch": 2.622784810126582,
      "grad_norm": 1.353991985321045,
      "learning_rate": 5e-05,
      "loss": 1.2486,
      "step": 2072
    },
    {
      "epoch": 2.624050632911392,
      "grad_norm": 1.330864667892456,
      "learning_rate": 5e-05,
      "loss": 1.2335,
      "step": 2073
    },
    {
      "epoch": 2.6253164556962023,
      "grad_norm": 1.3239022493362427,
      "learning_rate": 5e-05,
      "loss": 1.3119,
      "step": 2074
    },
    {
      "epoch": 2.6265822784810124,
      "grad_norm": 1.296552062034607,
      "learning_rate": 5e-05,
      "loss": 1.2232,
      "step": 2075
    },
    {
      "epoch": 2.6278481012658226,
      "grad_norm": 1.278541088104248,
      "learning_rate": 5e-05,
      "loss": 1.229,
      "step": 2076
    },
    {
      "epoch": 2.6291139240506327,
      "grad_norm": 1.3180475234985352,
      "learning_rate": 5e-05,
      "loss": 1.2374,
      "step": 2077
    },
    {
      "epoch": 2.630379746835443,
      "grad_norm": 1.303837776184082,
      "learning_rate": 5e-05,
      "loss": 1.1853,
      "step": 2078
    },
    {
      "epoch": 2.631645569620253,
      "grad_norm": 1.2998028993606567,
      "learning_rate": 5e-05,
      "loss": 1.2542,
      "step": 2079
    },
    {
      "epoch": 2.632911392405063,
      "grad_norm": 1.312501072883606,
      "learning_rate": 5e-05,
      "loss": 1.2443,
      "step": 2080
    },
    {
      "epoch": 2.6341772151898732,
      "grad_norm": 1.2905454635620117,
      "learning_rate": 5e-05,
      "loss": 1.192,
      "step": 2081
    },
    {
      "epoch": 2.6354430379746834,
      "grad_norm": 1.3172180652618408,
      "learning_rate": 5e-05,
      "loss": 1.2604,
      "step": 2082
    },
    {
      "epoch": 2.6367088607594935,
      "grad_norm": 1.2913419008255005,
      "learning_rate": 5e-05,
      "loss": 1.2548,
      "step": 2083
    },
    {
      "epoch": 2.6379746835443036,
      "grad_norm": 1.3171979188919067,
      "learning_rate": 5e-05,
      "loss": 1.2407,
      "step": 2084
    },
    {
      "epoch": 2.6392405063291138,
      "grad_norm": 1.3211337327957153,
      "learning_rate": 5e-05,
      "loss": 1.2705,
      "step": 2085
    },
    {
      "epoch": 2.640506329113924,
      "grad_norm": 1.3291256427764893,
      "learning_rate": 5e-05,
      "loss": 1.2412,
      "step": 2086
    },
    {
      "epoch": 2.641772151898734,
      "grad_norm": 1.2855675220489502,
      "learning_rate": 5e-05,
      "loss": 1.2365,
      "step": 2087
    },
    {
      "epoch": 2.643037974683544,
      "grad_norm": 1.3139595985412598,
      "learning_rate": 5e-05,
      "loss": 1.2173,
      "step": 2088
    },
    {
      "epoch": 2.6443037974683543,
      "grad_norm": 1.2962486743927002,
      "learning_rate": 5e-05,
      "loss": 1.1994,
      "step": 2089
    },
    {
      "epoch": 2.6455696202531644,
      "grad_norm": 1.3598641157150269,
      "learning_rate": 5e-05,
      "loss": 1.2423,
      "step": 2090
    },
    {
      "epoch": 2.6468354430379746,
      "grad_norm": 1.331277847290039,
      "learning_rate": 5e-05,
      "loss": 1.2501,
      "step": 2091
    },
    {
      "epoch": 2.6481012658227847,
      "grad_norm": 1.2991338968276978,
      "learning_rate": 5e-05,
      "loss": 1.2308,
      "step": 2092
    },
    {
      "epoch": 2.649367088607595,
      "grad_norm": 1.3188128471374512,
      "learning_rate": 5e-05,
      "loss": 1.3143,
      "step": 2093
    },
    {
      "epoch": 2.650632911392405,
      "grad_norm": 1.3529263734817505,
      "learning_rate": 5e-05,
      "loss": 1.2477,
      "step": 2094
    },
    {
      "epoch": 2.651898734177215,
      "grad_norm": 1.2924526929855347,
      "learning_rate": 5e-05,
      "loss": 1.2764,
      "step": 2095
    },
    {
      "epoch": 2.6531645569620252,
      "grad_norm": 1.3190617561340332,
      "learning_rate": 5e-05,
      "loss": 1.2155,
      "step": 2096
    },
    {
      "epoch": 2.6544303797468354,
      "grad_norm": 1.2943636178970337,
      "learning_rate": 5e-05,
      "loss": 1.1893,
      "step": 2097
    },
    {
      "epoch": 2.6556962025316455,
      "grad_norm": 1.3396859169006348,
      "learning_rate": 5e-05,
      "loss": 1.2442,
      "step": 2098
    },
    {
      "epoch": 2.6569620253164556,
      "grad_norm": 1.3396613597869873,
      "learning_rate": 5e-05,
      "loss": 1.2739,
      "step": 2099
    },
    {
      "epoch": 2.6582278481012658,
      "grad_norm": 1.2987043857574463,
      "learning_rate": 5e-05,
      "loss": 1.1616,
      "step": 2100
    },
    {
      "epoch": 2.659493670886076,
      "grad_norm": 1.2938508987426758,
      "learning_rate": 5e-05,
      "loss": 1.1956,
      "step": 2101
    },
    {
      "epoch": 2.660759493670886,
      "grad_norm": 1.3026494979858398,
      "learning_rate": 5e-05,
      "loss": 1.2208,
      "step": 2102
    },
    {
      "epoch": 2.662025316455696,
      "grad_norm": 1.3347182273864746,
      "learning_rate": 5e-05,
      "loss": 1.3011,
      "step": 2103
    },
    {
      "epoch": 2.6632911392405063,
      "grad_norm": 1.3192453384399414,
      "learning_rate": 5e-05,
      "loss": 1.2685,
      "step": 2104
    },
    {
      "epoch": 2.6645569620253164,
      "grad_norm": 1.3159383535385132,
      "learning_rate": 5e-05,
      "loss": 1.288,
      "step": 2105
    },
    {
      "epoch": 2.6658227848101266,
      "grad_norm": 1.303666353225708,
      "learning_rate": 5e-05,
      "loss": 1.2781,
      "step": 2106
    },
    {
      "epoch": 2.6670886075949367,
      "grad_norm": 1.3310275077819824,
      "learning_rate": 5e-05,
      "loss": 1.2648,
      "step": 2107
    },
    {
      "epoch": 2.668354430379747,
      "grad_norm": 1.2795491218566895,
      "learning_rate": 5e-05,
      "loss": 1.1543,
      "step": 2108
    },
    {
      "epoch": 2.669620253164557,
      "grad_norm": 1.3343247175216675,
      "learning_rate": 5e-05,
      "loss": 1.2636,
      "step": 2109
    },
    {
      "epoch": 2.670886075949367,
      "grad_norm": 1.3313289880752563,
      "learning_rate": 5e-05,
      "loss": 1.2833,
      "step": 2110
    },
    {
      "epoch": 2.6721518987341772,
      "grad_norm": 1.3513861894607544,
      "learning_rate": 5e-05,
      "loss": 1.2888,
      "step": 2111
    },
    {
      "epoch": 2.6734177215189874,
      "grad_norm": 1.2567874193191528,
      "learning_rate": 5e-05,
      "loss": 1.1156,
      "step": 2112
    },
    {
      "epoch": 2.6746835443037975,
      "grad_norm": 1.3257521390914917,
      "learning_rate": 5e-05,
      "loss": 1.2645,
      "step": 2113
    },
    {
      "epoch": 2.6759493670886076,
      "grad_norm": 1.3607267141342163,
      "learning_rate": 5e-05,
      "loss": 1.2647,
      "step": 2114
    },
    {
      "epoch": 2.6772151898734178,
      "grad_norm": 1.336713433265686,
      "learning_rate": 5e-05,
      "loss": 1.331,
      "step": 2115
    },
    {
      "epoch": 2.678481012658228,
      "grad_norm": 1.3490968942642212,
      "learning_rate": 5e-05,
      "loss": 1.2567,
      "step": 2116
    },
    {
      "epoch": 2.679746835443038,
      "grad_norm": 1.3551005125045776,
      "learning_rate": 5e-05,
      "loss": 1.2366,
      "step": 2117
    },
    {
      "epoch": 2.681012658227848,
      "grad_norm": 1.3129570484161377,
      "learning_rate": 5e-05,
      "loss": 1.2012,
      "step": 2118
    },
    {
      "epoch": 2.6822784810126583,
      "grad_norm": 1.3468340635299683,
      "learning_rate": 5e-05,
      "loss": 1.2943,
      "step": 2119
    },
    {
      "epoch": 2.6835443037974684,
      "grad_norm": 1.3293397426605225,
      "learning_rate": 5e-05,
      "loss": 1.2345,
      "step": 2120
    },
    {
      "epoch": 2.6848101265822786,
      "grad_norm": 1.335371971130371,
      "learning_rate": 5e-05,
      "loss": 1.2232,
      "step": 2121
    },
    {
      "epoch": 2.6860759493670887,
      "grad_norm": 1.275345802307129,
      "learning_rate": 5e-05,
      "loss": 1.1192,
      "step": 2122
    },
    {
      "epoch": 2.687341772151899,
      "grad_norm": 1.349124550819397,
      "learning_rate": 5e-05,
      "loss": 1.2606,
      "step": 2123
    },
    {
      "epoch": 2.688607594936709,
      "grad_norm": 1.3052939176559448,
      "learning_rate": 5e-05,
      "loss": 1.2955,
      "step": 2124
    },
    {
      "epoch": 2.689873417721519,
      "grad_norm": 1.2834290266036987,
      "learning_rate": 5e-05,
      "loss": 1.2123,
      "step": 2125
    },
    {
      "epoch": 2.6911392405063292,
      "grad_norm": 1.3229619264602661,
      "learning_rate": 5e-05,
      "loss": 1.233,
      "step": 2126
    },
    {
      "epoch": 2.6924050632911394,
      "grad_norm": 1.3620046377182007,
      "learning_rate": 5e-05,
      "loss": 1.3031,
      "step": 2127
    },
    {
      "epoch": 2.6936708860759495,
      "grad_norm": 1.2976804971694946,
      "learning_rate": 5e-05,
      "loss": 1.2296,
      "step": 2128
    },
    {
      "epoch": 2.6949367088607596,
      "grad_norm": 1.2908105850219727,
      "learning_rate": 5e-05,
      "loss": 1.2246,
      "step": 2129
    },
    {
      "epoch": 2.6962025316455698,
      "grad_norm": 1.3531744480133057,
      "learning_rate": 5e-05,
      "loss": 1.2658,
      "step": 2130
    },
    {
      "epoch": 2.69746835443038,
      "grad_norm": 1.3068969249725342,
      "learning_rate": 5e-05,
      "loss": 1.2384,
      "step": 2131
    },
    {
      "epoch": 2.69873417721519,
      "grad_norm": 1.3336716890335083,
      "learning_rate": 5e-05,
      "loss": 1.2446,
      "step": 2132
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.3417123556137085,
      "learning_rate": 5e-05,
      "loss": 1.2884,
      "step": 2133
    },
    {
      "epoch": 2.7012658227848103,
      "grad_norm": 1.3790347576141357,
      "learning_rate": 5e-05,
      "loss": 1.3225,
      "step": 2134
    },
    {
      "epoch": 2.7025316455696204,
      "grad_norm": 1.3427332639694214,
      "learning_rate": 5e-05,
      "loss": 1.2526,
      "step": 2135
    },
    {
      "epoch": 2.7037974683544306,
      "grad_norm": 1.3479015827178955,
      "learning_rate": 5e-05,
      "loss": 1.2699,
      "step": 2136
    },
    {
      "epoch": 2.7050632911392407,
      "grad_norm": 1.2959638833999634,
      "learning_rate": 5e-05,
      "loss": 1.2258,
      "step": 2137
    },
    {
      "epoch": 2.706329113924051,
      "grad_norm": 1.3531324863433838,
      "learning_rate": 5e-05,
      "loss": 1.2567,
      "step": 2138
    },
    {
      "epoch": 2.707594936708861,
      "grad_norm": 1.3203928470611572,
      "learning_rate": 5e-05,
      "loss": 1.2865,
      "step": 2139
    },
    {
      "epoch": 2.708860759493671,
      "grad_norm": 1.29862380027771,
      "learning_rate": 5e-05,
      "loss": 1.2492,
      "step": 2140
    },
    {
      "epoch": 2.7101265822784812,
      "grad_norm": 1.3416270017623901,
      "learning_rate": 5e-05,
      "loss": 1.2621,
      "step": 2141
    },
    {
      "epoch": 2.7113924050632914,
      "grad_norm": 1.3082574605941772,
      "learning_rate": 5e-05,
      "loss": 1.288,
      "step": 2142
    },
    {
      "epoch": 2.7126582278481015,
      "grad_norm": 1.3517872095108032,
      "learning_rate": 5e-05,
      "loss": 1.306,
      "step": 2143
    },
    {
      "epoch": 2.7139240506329116,
      "grad_norm": 1.3237303495407104,
      "learning_rate": 5e-05,
      "loss": 1.1958,
      "step": 2144
    },
    {
      "epoch": 2.7151898734177218,
      "grad_norm": 1.311353087425232,
      "learning_rate": 5e-05,
      "loss": 1.2375,
      "step": 2145
    },
    {
      "epoch": 2.716455696202532,
      "grad_norm": 1.3181134462356567,
      "learning_rate": 5e-05,
      "loss": 1.2003,
      "step": 2146
    },
    {
      "epoch": 2.717721518987342,
      "grad_norm": 1.3343451023101807,
      "learning_rate": 5e-05,
      "loss": 1.3046,
      "step": 2147
    },
    {
      "epoch": 2.7189873417721517,
      "grad_norm": 1.3066288232803345,
      "learning_rate": 5e-05,
      "loss": 1.2504,
      "step": 2148
    },
    {
      "epoch": 2.720253164556962,
      "grad_norm": 1.356825828552246,
      "learning_rate": 5e-05,
      "loss": 1.2607,
      "step": 2149
    },
    {
      "epoch": 2.721518987341772,
      "grad_norm": 1.3019394874572754,
      "learning_rate": 5e-05,
      "loss": 1.1934,
      "step": 2150
    },
    {
      "epoch": 2.722784810126582,
      "grad_norm": 1.3087133169174194,
      "learning_rate": 5e-05,
      "loss": 1.2512,
      "step": 2151
    },
    {
      "epoch": 2.7240506329113923,
      "grad_norm": 1.3119364976882935,
      "learning_rate": 5e-05,
      "loss": 1.2546,
      "step": 2152
    },
    {
      "epoch": 2.7253164556962024,
      "grad_norm": 1.3333537578582764,
      "learning_rate": 5e-05,
      "loss": 1.2319,
      "step": 2153
    },
    {
      "epoch": 2.7265822784810125,
      "grad_norm": 1.3321701288223267,
      "learning_rate": 5e-05,
      "loss": 1.2765,
      "step": 2154
    },
    {
      "epoch": 2.7278481012658227,
      "grad_norm": 1.3672362565994263,
      "learning_rate": 5e-05,
      "loss": 1.2799,
      "step": 2155
    },
    {
      "epoch": 2.729113924050633,
      "grad_norm": 1.3477369546890259,
      "learning_rate": 5e-05,
      "loss": 1.3425,
      "step": 2156
    },
    {
      "epoch": 2.730379746835443,
      "grad_norm": 1.3150392770767212,
      "learning_rate": 5e-05,
      "loss": 1.2506,
      "step": 2157
    },
    {
      "epoch": 2.731645569620253,
      "grad_norm": 1.373779296875,
      "learning_rate": 5e-05,
      "loss": 1.2285,
      "step": 2158
    },
    {
      "epoch": 2.732911392405063,
      "grad_norm": 1.278868556022644,
      "learning_rate": 5e-05,
      "loss": 1.2571,
      "step": 2159
    },
    {
      "epoch": 2.7341772151898733,
      "grad_norm": 1.3591290712356567,
      "learning_rate": 5e-05,
      "loss": 1.1874,
      "step": 2160
    },
    {
      "epoch": 2.7354430379746835,
      "grad_norm": 1.3173047304153442,
      "learning_rate": 5e-05,
      "loss": 1.234,
      "step": 2161
    },
    {
      "epoch": 2.7367088607594936,
      "grad_norm": 1.2786030769348145,
      "learning_rate": 5e-05,
      "loss": 1.1896,
      "step": 2162
    },
    {
      "epoch": 2.7379746835443037,
      "grad_norm": 1.3517661094665527,
      "learning_rate": 5e-05,
      "loss": 1.3218,
      "step": 2163
    },
    {
      "epoch": 2.739240506329114,
      "grad_norm": 1.3192874193191528,
      "learning_rate": 5e-05,
      "loss": 1.2138,
      "step": 2164
    },
    {
      "epoch": 2.740506329113924,
      "grad_norm": 1.3188889026641846,
      "learning_rate": 5e-05,
      "loss": 1.279,
      "step": 2165
    },
    {
      "epoch": 2.741772151898734,
      "grad_norm": 1.3585394620895386,
      "learning_rate": 5e-05,
      "loss": 1.3048,
      "step": 2166
    },
    {
      "epoch": 2.7430379746835443,
      "grad_norm": 1.296189785003662,
      "learning_rate": 5e-05,
      "loss": 1.2071,
      "step": 2167
    },
    {
      "epoch": 2.7443037974683544,
      "grad_norm": 1.3281134366989136,
      "learning_rate": 5e-05,
      "loss": 1.2045,
      "step": 2168
    },
    {
      "epoch": 2.7455696202531645,
      "grad_norm": 1.3269091844558716,
      "learning_rate": 5e-05,
      "loss": 1.2646,
      "step": 2169
    },
    {
      "epoch": 2.7468354430379747,
      "grad_norm": 1.2947007417678833,
      "learning_rate": 5e-05,
      "loss": 1.1679,
      "step": 2170
    },
    {
      "epoch": 2.748101265822785,
      "grad_norm": 1.3087835311889648,
      "learning_rate": 5e-05,
      "loss": 1.2502,
      "step": 2171
    },
    {
      "epoch": 2.749367088607595,
      "grad_norm": 1.2999154329299927,
      "learning_rate": 5e-05,
      "loss": 1.216,
      "step": 2172
    },
    {
      "epoch": 2.750632911392405,
      "grad_norm": 1.3325204849243164,
      "learning_rate": 5e-05,
      "loss": 1.2649,
      "step": 2173
    },
    {
      "epoch": 2.751898734177215,
      "grad_norm": 1.2861356735229492,
      "learning_rate": 5e-05,
      "loss": 1.1466,
      "step": 2174
    },
    {
      "epoch": 2.7531645569620253,
      "grad_norm": 1.304313063621521,
      "learning_rate": 5e-05,
      "loss": 1.227,
      "step": 2175
    },
    {
      "epoch": 2.7544303797468355,
      "grad_norm": 1.2966153621673584,
      "learning_rate": 5e-05,
      "loss": 1.1631,
      "step": 2176
    },
    {
      "epoch": 2.7556962025316456,
      "grad_norm": 1.2845995426177979,
      "learning_rate": 5e-05,
      "loss": 1.1831,
      "step": 2177
    },
    {
      "epoch": 2.7569620253164557,
      "grad_norm": 1.313779592514038,
      "learning_rate": 5e-05,
      "loss": 1.2204,
      "step": 2178
    },
    {
      "epoch": 2.758227848101266,
      "grad_norm": 1.3018797636032104,
      "learning_rate": 5e-05,
      "loss": 1.1719,
      "step": 2179
    },
    {
      "epoch": 2.759493670886076,
      "grad_norm": 1.307151198387146,
      "learning_rate": 5e-05,
      "loss": 1.202,
      "step": 2180
    },
    {
      "epoch": 2.760759493670886,
      "grad_norm": 1.2921744585037231,
      "learning_rate": 5e-05,
      "loss": 1.1694,
      "step": 2181
    },
    {
      "epoch": 2.7620253164556963,
      "grad_norm": 1.2832671403884888,
      "learning_rate": 5e-05,
      "loss": 1.2155,
      "step": 2182
    },
    {
      "epoch": 2.7632911392405064,
      "grad_norm": 1.3381041288375854,
      "learning_rate": 5e-05,
      "loss": 1.2531,
      "step": 2183
    },
    {
      "epoch": 2.7645569620253165,
      "grad_norm": 1.2735192775726318,
      "learning_rate": 5e-05,
      "loss": 1.1943,
      "step": 2184
    },
    {
      "epoch": 2.7658227848101267,
      "grad_norm": 1.3346495628356934,
      "learning_rate": 5e-05,
      "loss": 1.2042,
      "step": 2185
    },
    {
      "epoch": 2.767088607594937,
      "grad_norm": 1.3164784908294678,
      "learning_rate": 5e-05,
      "loss": 1.2559,
      "step": 2186
    },
    {
      "epoch": 2.768354430379747,
      "grad_norm": 1.3377987146377563,
      "learning_rate": 5e-05,
      "loss": 1.2522,
      "step": 2187
    },
    {
      "epoch": 2.769620253164557,
      "grad_norm": 1.356816291809082,
      "learning_rate": 5e-05,
      "loss": 1.2602,
      "step": 2188
    },
    {
      "epoch": 2.770886075949367,
      "grad_norm": 1.3331243991851807,
      "learning_rate": 5e-05,
      "loss": 1.2366,
      "step": 2189
    },
    {
      "epoch": 2.7721518987341773,
      "grad_norm": 1.3121471405029297,
      "learning_rate": 5e-05,
      "loss": 1.2264,
      "step": 2190
    },
    {
      "epoch": 2.7734177215189875,
      "grad_norm": 1.3407988548278809,
      "learning_rate": 5e-05,
      "loss": 1.2335,
      "step": 2191
    },
    {
      "epoch": 2.7746835443037976,
      "grad_norm": 1.3262896537780762,
      "learning_rate": 5e-05,
      "loss": 1.245,
      "step": 2192
    },
    {
      "epoch": 2.7759493670886077,
      "grad_norm": 1.2737131118774414,
      "learning_rate": 5e-05,
      "loss": 1.1708,
      "step": 2193
    },
    {
      "epoch": 2.777215189873418,
      "grad_norm": 1.3141940832138062,
      "learning_rate": 5e-05,
      "loss": 1.211,
      "step": 2194
    },
    {
      "epoch": 2.778481012658228,
      "grad_norm": 1.2968820333480835,
      "learning_rate": 5e-05,
      "loss": 1.2037,
      "step": 2195
    },
    {
      "epoch": 2.779746835443038,
      "grad_norm": 1.3218531608581543,
      "learning_rate": 5e-05,
      "loss": 1.2093,
      "step": 2196
    },
    {
      "epoch": 2.7810126582278483,
      "grad_norm": 1.3045541048049927,
      "learning_rate": 5e-05,
      "loss": 1.2027,
      "step": 2197
    },
    {
      "epoch": 2.782278481012658,
      "grad_norm": 1.3257207870483398,
      "learning_rate": 5e-05,
      "loss": 1.2755,
      "step": 2198
    },
    {
      "epoch": 2.783544303797468,
      "grad_norm": 1.2994599342346191,
      "learning_rate": 5e-05,
      "loss": 1.2073,
      "step": 2199
    },
    {
      "epoch": 2.7848101265822782,
      "grad_norm": 1.374430537223816,
      "learning_rate": 5e-05,
      "loss": 1.2298,
      "step": 2200
    },
    {
      "epoch": 2.7860759493670884,
      "grad_norm": 1.332518219947815,
      "learning_rate": 5e-05,
      "loss": 1.2031,
      "step": 2201
    },
    {
      "epoch": 2.7873417721518985,
      "grad_norm": 1.3214311599731445,
      "learning_rate": 5e-05,
      "loss": 1.2307,
      "step": 2202
    },
    {
      "epoch": 2.7886075949367086,
      "grad_norm": 1.2769404649734497,
      "learning_rate": 5e-05,
      "loss": 1.2286,
      "step": 2203
    },
    {
      "epoch": 2.7898734177215188,
      "grad_norm": 1.3068363666534424,
      "learning_rate": 5e-05,
      "loss": 1.2001,
      "step": 2204
    },
    {
      "epoch": 2.791139240506329,
      "grad_norm": 1.315186619758606,
      "learning_rate": 5e-05,
      "loss": 1.2377,
      "step": 2205
    },
    {
      "epoch": 2.792405063291139,
      "grad_norm": 1.3206247091293335,
      "learning_rate": 5e-05,
      "loss": 1.2063,
      "step": 2206
    },
    {
      "epoch": 2.793670886075949,
      "grad_norm": 1.30023992061615,
      "learning_rate": 5e-05,
      "loss": 1.1732,
      "step": 2207
    },
    {
      "epoch": 2.7949367088607593,
      "grad_norm": 1.3305277824401855,
      "learning_rate": 5e-05,
      "loss": 1.2705,
      "step": 2208
    },
    {
      "epoch": 2.7962025316455694,
      "grad_norm": 1.316227912902832,
      "learning_rate": 5e-05,
      "loss": 1.1903,
      "step": 2209
    },
    {
      "epoch": 2.7974683544303796,
      "grad_norm": 1.2769577503204346,
      "learning_rate": 5e-05,
      "loss": 1.1302,
      "step": 2210
    },
    {
      "epoch": 2.7987341772151897,
      "grad_norm": 1.3368078470230103,
      "learning_rate": 5e-05,
      "loss": 1.2212,
      "step": 2211
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.346047282218933,
      "learning_rate": 5e-05,
      "loss": 1.2235,
      "step": 2212
    },
    {
      "epoch": 2.80126582278481,
      "grad_norm": 1.3206814527511597,
      "learning_rate": 5e-05,
      "loss": 1.231,
      "step": 2213
    },
    {
      "epoch": 2.80253164556962,
      "grad_norm": 1.3261407613754272,
      "learning_rate": 5e-05,
      "loss": 1.2598,
      "step": 2214
    },
    {
      "epoch": 2.8037974683544302,
      "grad_norm": 1.298661470413208,
      "learning_rate": 5e-05,
      "loss": 1.1821,
      "step": 2215
    },
    {
      "epoch": 2.8050632911392404,
      "grad_norm": 1.2706506252288818,
      "learning_rate": 5e-05,
      "loss": 1.1954,
      "step": 2216
    },
    {
      "epoch": 2.8063291139240505,
      "grad_norm": 1.3000186681747437,
      "learning_rate": 5e-05,
      "loss": 1.1745,
      "step": 2217
    },
    {
      "epoch": 2.8075949367088606,
      "grad_norm": 1.351256251335144,
      "learning_rate": 5e-05,
      "loss": 1.301,
      "step": 2218
    },
    {
      "epoch": 2.8088607594936708,
      "grad_norm": 1.3535206317901611,
      "learning_rate": 5e-05,
      "loss": 1.1777,
      "step": 2219
    },
    {
      "epoch": 2.810126582278481,
      "grad_norm": 1.3335071802139282,
      "learning_rate": 5e-05,
      "loss": 1.1999,
      "step": 2220
    },
    {
      "epoch": 2.811392405063291,
      "grad_norm": 1.3279017210006714,
      "learning_rate": 5e-05,
      "loss": 1.1692,
      "step": 2221
    },
    {
      "epoch": 2.812658227848101,
      "grad_norm": 1.3485913276672363,
      "learning_rate": 5e-05,
      "loss": 1.2311,
      "step": 2222
    },
    {
      "epoch": 2.8139240506329113,
      "grad_norm": 1.319462776184082,
      "learning_rate": 5e-05,
      "loss": 1.2583,
      "step": 2223
    },
    {
      "epoch": 2.8151898734177214,
      "grad_norm": 1.3276814222335815,
      "learning_rate": 5e-05,
      "loss": 1.2242,
      "step": 2224
    },
    {
      "epoch": 2.8164556962025316,
      "grad_norm": 1.3564587831497192,
      "learning_rate": 5e-05,
      "loss": 1.1914,
      "step": 2225
    },
    {
      "epoch": 2.8177215189873417,
      "grad_norm": 1.3398059606552124,
      "learning_rate": 5e-05,
      "loss": 1.2445,
      "step": 2226
    },
    {
      "epoch": 2.818987341772152,
      "grad_norm": 1.3110164403915405,
      "learning_rate": 5e-05,
      "loss": 1.2023,
      "step": 2227
    },
    {
      "epoch": 2.820253164556962,
      "grad_norm": 1.3268226385116577,
      "learning_rate": 5e-05,
      "loss": 1.2064,
      "step": 2228
    },
    {
      "epoch": 2.821518987341772,
      "grad_norm": 1.3333566188812256,
      "learning_rate": 5e-05,
      "loss": 1.2059,
      "step": 2229
    },
    {
      "epoch": 2.8227848101265822,
      "grad_norm": 1.333412528038025,
      "learning_rate": 5e-05,
      "loss": 1.2348,
      "step": 2230
    },
    {
      "epoch": 2.8240506329113924,
      "grad_norm": 1.2998840808868408,
      "learning_rate": 5e-05,
      "loss": 1.1437,
      "step": 2231
    },
    {
      "epoch": 2.8253164556962025,
      "grad_norm": 1.3417342901229858,
      "learning_rate": 5e-05,
      "loss": 1.2436,
      "step": 2232
    },
    {
      "epoch": 2.8265822784810126,
      "grad_norm": 1.280349612236023,
      "learning_rate": 5e-05,
      "loss": 1.2014,
      "step": 2233
    },
    {
      "epoch": 2.8278481012658228,
      "grad_norm": 1.3097970485687256,
      "learning_rate": 5e-05,
      "loss": 1.23,
      "step": 2234
    },
    {
      "epoch": 2.829113924050633,
      "grad_norm": 1.2755987644195557,
      "learning_rate": 5e-05,
      "loss": 1.1761,
      "step": 2235
    },
    {
      "epoch": 2.830379746835443,
      "grad_norm": 1.338606834411621,
      "learning_rate": 5e-05,
      "loss": 1.2422,
      "step": 2236
    },
    {
      "epoch": 2.831645569620253,
      "grad_norm": 1.325246810913086,
      "learning_rate": 5e-05,
      "loss": 1.1918,
      "step": 2237
    },
    {
      "epoch": 2.8329113924050633,
      "grad_norm": 1.3021076917648315,
      "learning_rate": 5e-05,
      "loss": 1.2415,
      "step": 2238
    },
    {
      "epoch": 2.8341772151898734,
      "grad_norm": 1.3154414892196655,
      "learning_rate": 5e-05,
      "loss": 1.2493,
      "step": 2239
    },
    {
      "epoch": 2.8354430379746836,
      "grad_norm": 1.3293695449829102,
      "learning_rate": 5e-05,
      "loss": 1.2647,
      "step": 2240
    },
    {
      "epoch": 2.8367088607594937,
      "grad_norm": 1.3534408807754517,
      "learning_rate": 5e-05,
      "loss": 1.2767,
      "step": 2241
    },
    {
      "epoch": 2.837974683544304,
      "grad_norm": 1.3135193586349487,
      "learning_rate": 5e-05,
      "loss": 1.2072,
      "step": 2242
    },
    {
      "epoch": 2.839240506329114,
      "grad_norm": 1.3008371591567993,
      "learning_rate": 5e-05,
      "loss": 1.2262,
      "step": 2243
    },
    {
      "epoch": 2.840506329113924,
      "grad_norm": 1.3199760913848877,
      "learning_rate": 5e-05,
      "loss": 1.2586,
      "step": 2244
    },
    {
      "epoch": 2.8417721518987342,
      "grad_norm": 1.3425043821334839,
      "learning_rate": 5e-05,
      "loss": 1.2666,
      "step": 2245
    },
    {
      "epoch": 2.8430379746835444,
      "grad_norm": 1.3011103868484497,
      "learning_rate": 5e-05,
      "loss": 1.2045,
      "step": 2246
    },
    {
      "epoch": 2.8443037974683545,
      "grad_norm": 1.3238180875778198,
      "learning_rate": 5e-05,
      "loss": 1.2061,
      "step": 2247
    },
    {
      "epoch": 2.8455696202531646,
      "grad_norm": 1.2994619607925415,
      "learning_rate": 5e-05,
      "loss": 1.2035,
      "step": 2248
    },
    {
      "epoch": 2.8468354430379748,
      "grad_norm": 1.302599310874939,
      "learning_rate": 5e-05,
      "loss": 1.2387,
      "step": 2249
    },
    {
      "epoch": 2.848101265822785,
      "grad_norm": 1.2786210775375366,
      "learning_rate": 5e-05,
      "loss": 1.17,
      "step": 2250
    },
    {
      "epoch": 2.849367088607595,
      "grad_norm": 1.2843748331069946,
      "learning_rate": 5e-05,
      "loss": 1.1627,
      "step": 2251
    },
    {
      "epoch": 2.850632911392405,
      "grad_norm": 1.2992743253707886,
      "learning_rate": 5e-05,
      "loss": 1.1586,
      "step": 2252
    },
    {
      "epoch": 2.8518987341772153,
      "grad_norm": 1.3422266244888306,
      "learning_rate": 5e-05,
      "loss": 1.246,
      "step": 2253
    },
    {
      "epoch": 2.8531645569620254,
      "grad_norm": 1.304351568222046,
      "learning_rate": 5e-05,
      "loss": 1.2389,
      "step": 2254
    },
    {
      "epoch": 2.8544303797468356,
      "grad_norm": 1.2887845039367676,
      "learning_rate": 5e-05,
      "loss": 1.1663,
      "step": 2255
    },
    {
      "epoch": 2.8556962025316457,
      "grad_norm": 1.317855954170227,
      "learning_rate": 5e-05,
      "loss": 1.2809,
      "step": 2256
    },
    {
      "epoch": 2.856962025316456,
      "grad_norm": 1.2467316389083862,
      "learning_rate": 5e-05,
      "loss": 1.1646,
      "step": 2257
    },
    {
      "epoch": 2.858227848101266,
      "grad_norm": 1.2764630317687988,
      "learning_rate": 5e-05,
      "loss": 1.1769,
      "step": 2258
    },
    {
      "epoch": 2.859493670886076,
      "grad_norm": 1.316908359527588,
      "learning_rate": 5e-05,
      "loss": 1.2118,
      "step": 2259
    },
    {
      "epoch": 2.8607594936708862,
      "grad_norm": 1.3380047082901,
      "learning_rate": 5e-05,
      "loss": 1.217,
      "step": 2260
    },
    {
      "epoch": 2.8620253164556964,
      "grad_norm": 1.3308511972427368,
      "learning_rate": 5e-05,
      "loss": 1.2654,
      "step": 2261
    },
    {
      "epoch": 2.8632911392405065,
      "grad_norm": 1.3350569009780884,
      "learning_rate": 5e-05,
      "loss": 1.2706,
      "step": 2262
    },
    {
      "epoch": 2.8645569620253166,
      "grad_norm": 1.3037524223327637,
      "learning_rate": 5e-05,
      "loss": 1.2211,
      "step": 2263
    },
    {
      "epoch": 2.8658227848101268,
      "grad_norm": 1.384155035018921,
      "learning_rate": 5e-05,
      "loss": 1.2936,
      "step": 2264
    },
    {
      "epoch": 2.867088607594937,
      "grad_norm": 1.2990368604660034,
      "learning_rate": 5e-05,
      "loss": 1.2118,
      "step": 2265
    },
    {
      "epoch": 2.868354430379747,
      "grad_norm": 1.2825922966003418,
      "learning_rate": 5e-05,
      "loss": 1.1367,
      "step": 2266
    },
    {
      "epoch": 2.869620253164557,
      "grad_norm": 1.347646951675415,
      "learning_rate": 5e-05,
      "loss": 1.2198,
      "step": 2267
    },
    {
      "epoch": 2.8708860759493673,
      "grad_norm": 1.3112776279449463,
      "learning_rate": 5e-05,
      "loss": 1.2207,
      "step": 2268
    },
    {
      "epoch": 2.8721518987341774,
      "grad_norm": 1.3616900444030762,
      "learning_rate": 5e-05,
      "loss": 1.2713,
      "step": 2269
    },
    {
      "epoch": 2.8734177215189876,
      "grad_norm": 1.3201675415039062,
      "learning_rate": 5e-05,
      "loss": 1.2266,
      "step": 2270
    },
    {
      "epoch": 2.8746835443037977,
      "grad_norm": 1.3189440965652466,
      "learning_rate": 5e-05,
      "loss": 1.183,
      "step": 2271
    },
    {
      "epoch": 2.875949367088608,
      "grad_norm": 1.3373560905456543,
      "learning_rate": 5e-05,
      "loss": 1.1854,
      "step": 2272
    },
    {
      "epoch": 2.877215189873418,
      "grad_norm": 1.3474717140197754,
      "learning_rate": 5e-05,
      "loss": 1.2423,
      "step": 2273
    },
    {
      "epoch": 2.878481012658228,
      "grad_norm": 1.3511621952056885,
      "learning_rate": 5e-05,
      "loss": 1.2538,
      "step": 2274
    },
    {
      "epoch": 2.879746835443038,
      "grad_norm": 1.3272042274475098,
      "learning_rate": 5e-05,
      "loss": 1.2391,
      "step": 2275
    },
    {
      "epoch": 2.8810126582278484,
      "grad_norm": 1.3370838165283203,
      "learning_rate": 5e-05,
      "loss": 1.2413,
      "step": 2276
    },
    {
      "epoch": 2.8822784810126585,
      "grad_norm": 1.3628323078155518,
      "learning_rate": 5e-05,
      "loss": 1.2274,
      "step": 2277
    },
    {
      "epoch": 2.8835443037974686,
      "grad_norm": 1.3152188062667847,
      "learning_rate": 5e-05,
      "loss": 1.1763,
      "step": 2278
    },
    {
      "epoch": 2.8848101265822788,
      "grad_norm": 1.371894359588623,
      "learning_rate": 5e-05,
      "loss": 1.2778,
      "step": 2279
    },
    {
      "epoch": 2.8860759493670884,
      "grad_norm": 1.3396633863449097,
      "learning_rate": 5e-05,
      "loss": 1.2686,
      "step": 2280
    },
    {
      "epoch": 2.8873417721518986,
      "grad_norm": 1.3416646718978882,
      "learning_rate": 5e-05,
      "loss": 1.2112,
      "step": 2281
    },
    {
      "epoch": 2.8886075949367087,
      "grad_norm": 1.3214308023452759,
      "learning_rate": 5e-05,
      "loss": 1.2458,
      "step": 2282
    },
    {
      "epoch": 2.889873417721519,
      "grad_norm": 1.347852110862732,
      "learning_rate": 5e-05,
      "loss": 1.2025,
      "step": 2283
    },
    {
      "epoch": 2.891139240506329,
      "grad_norm": 1.3186534643173218,
      "learning_rate": 5e-05,
      "loss": 1.2373,
      "step": 2284
    },
    {
      "epoch": 2.892405063291139,
      "grad_norm": 1.3732712268829346,
      "learning_rate": 5e-05,
      "loss": 1.2481,
      "step": 2285
    },
    {
      "epoch": 2.8936708860759492,
      "grad_norm": 1.3148107528686523,
      "learning_rate": 5e-05,
      "loss": 1.2273,
      "step": 2286
    },
    {
      "epoch": 2.8949367088607594,
      "grad_norm": 1.2885085344314575,
      "learning_rate": 5e-05,
      "loss": 1.185,
      "step": 2287
    },
    {
      "epoch": 2.8962025316455695,
      "grad_norm": 1.279801368713379,
      "learning_rate": 5e-05,
      "loss": 1.1608,
      "step": 2288
    },
    {
      "epoch": 2.8974683544303796,
      "grad_norm": 1.3476226329803467,
      "learning_rate": 5e-05,
      "loss": 1.2269,
      "step": 2289
    },
    {
      "epoch": 2.8987341772151898,
      "grad_norm": 1.2881051301956177,
      "learning_rate": 5e-05,
      "loss": 1.1494,
      "step": 2290
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.3364242315292358,
      "learning_rate": 5e-05,
      "loss": 1.2352,
      "step": 2291
    },
    {
      "epoch": 2.90126582278481,
      "grad_norm": 1.3190159797668457,
      "learning_rate": 5e-05,
      "loss": 1.2051,
      "step": 2292
    },
    {
      "epoch": 2.90253164556962,
      "grad_norm": 1.3112595081329346,
      "learning_rate": 5e-05,
      "loss": 1.1865,
      "step": 2293
    },
    {
      "epoch": 2.9037974683544303,
      "grad_norm": 1.3460453748703003,
      "learning_rate": 5e-05,
      "loss": 1.2649,
      "step": 2294
    },
    {
      "epoch": 2.9050632911392404,
      "grad_norm": 1.3086962699890137,
      "learning_rate": 5e-05,
      "loss": 1.1623,
      "step": 2295
    },
    {
      "epoch": 2.9063291139240506,
      "grad_norm": 1.3257607221603394,
      "learning_rate": 5e-05,
      "loss": 1.121,
      "step": 2296
    },
    {
      "epoch": 2.9075949367088607,
      "grad_norm": 1.3396592140197754,
      "learning_rate": 5e-05,
      "loss": 1.2463,
      "step": 2297
    },
    {
      "epoch": 2.908860759493671,
      "grad_norm": 1.313649296760559,
      "learning_rate": 5e-05,
      "loss": 1.2213,
      "step": 2298
    },
    {
      "epoch": 2.910126582278481,
      "grad_norm": 1.2900645732879639,
      "learning_rate": 5e-05,
      "loss": 1.2039,
      "step": 2299
    },
    {
      "epoch": 2.911392405063291,
      "grad_norm": 1.3043200969696045,
      "learning_rate": 5e-05,
      "loss": 1.1903,
      "step": 2300
    },
    {
      "epoch": 2.9126582278481012,
      "grad_norm": 1.3100451231002808,
      "learning_rate": 5e-05,
      "loss": 1.1785,
      "step": 2301
    },
    {
      "epoch": 2.9139240506329114,
      "grad_norm": 1.30105459690094,
      "learning_rate": 5e-05,
      "loss": 1.188,
      "step": 2302
    },
    {
      "epoch": 2.9151898734177215,
      "grad_norm": 1.3469783067703247,
      "learning_rate": 5e-05,
      "loss": 1.2348,
      "step": 2303
    },
    {
      "epoch": 2.9164556962025316,
      "grad_norm": 1.3389432430267334,
      "learning_rate": 5e-05,
      "loss": 1.1779,
      "step": 2304
    },
    {
      "epoch": 2.9177215189873418,
      "grad_norm": 1.3023916482925415,
      "learning_rate": 5e-05,
      "loss": 1.1385,
      "step": 2305
    },
    {
      "epoch": 2.918987341772152,
      "grad_norm": 1.281930685043335,
      "learning_rate": 5e-05,
      "loss": 1.1643,
      "step": 2306
    },
    {
      "epoch": 2.920253164556962,
      "grad_norm": 1.2660292387008667,
      "learning_rate": 5e-05,
      "loss": 1.1578,
      "step": 2307
    },
    {
      "epoch": 2.921518987341772,
      "grad_norm": 1.3398739099502563,
      "learning_rate": 5e-05,
      "loss": 1.2533,
      "step": 2308
    },
    {
      "epoch": 2.9227848101265823,
      "grad_norm": 1.348513126373291,
      "learning_rate": 5e-05,
      "loss": 1.239,
      "step": 2309
    },
    {
      "epoch": 2.9240506329113924,
      "grad_norm": 1.3215898275375366,
      "learning_rate": 5e-05,
      "loss": 1.2259,
      "step": 2310
    },
    {
      "epoch": 2.9253164556962026,
      "grad_norm": 1.3283883333206177,
      "learning_rate": 5e-05,
      "loss": 1.2288,
      "step": 2311
    },
    {
      "epoch": 2.9265822784810127,
      "grad_norm": 1.3500844240188599,
      "learning_rate": 5e-05,
      "loss": 1.2778,
      "step": 2312
    },
    {
      "epoch": 2.927848101265823,
      "grad_norm": 1.3486087322235107,
      "learning_rate": 5e-05,
      "loss": 1.2106,
      "step": 2313
    },
    {
      "epoch": 2.929113924050633,
      "grad_norm": 1.3472177982330322,
      "learning_rate": 5e-05,
      "loss": 1.2231,
      "step": 2314
    },
    {
      "epoch": 2.930379746835443,
      "grad_norm": 1.2830276489257812,
      "learning_rate": 5e-05,
      "loss": 1.2039,
      "step": 2315
    },
    {
      "epoch": 2.9316455696202532,
      "grad_norm": 1.3052548170089722,
      "learning_rate": 5e-05,
      "loss": 1.1749,
      "step": 2316
    },
    {
      "epoch": 2.9329113924050634,
      "grad_norm": 1.3042558431625366,
      "learning_rate": 5e-05,
      "loss": 1.237,
      "step": 2317
    },
    {
      "epoch": 2.9341772151898735,
      "grad_norm": 1.3396496772766113,
      "learning_rate": 5e-05,
      "loss": 1.2222,
      "step": 2318
    },
    {
      "epoch": 2.9354430379746836,
      "grad_norm": 1.2778240442276,
      "learning_rate": 5e-05,
      "loss": 1.1873,
      "step": 2319
    },
    {
      "epoch": 2.9367088607594938,
      "grad_norm": 1.3333501815795898,
      "learning_rate": 5e-05,
      "loss": 1.1798,
      "step": 2320
    },
    {
      "epoch": 2.937974683544304,
      "grad_norm": 1.316009759902954,
      "learning_rate": 5e-05,
      "loss": 1.1911,
      "step": 2321
    },
    {
      "epoch": 2.939240506329114,
      "grad_norm": 1.3087528944015503,
      "learning_rate": 5e-05,
      "loss": 1.1543,
      "step": 2322
    },
    {
      "epoch": 2.940506329113924,
      "grad_norm": 1.2912333011627197,
      "learning_rate": 5e-05,
      "loss": 1.1552,
      "step": 2323
    },
    {
      "epoch": 2.9417721518987343,
      "grad_norm": 1.2968227863311768,
      "learning_rate": 5e-05,
      "loss": 1.1726,
      "step": 2324
    },
    {
      "epoch": 2.9430379746835444,
      "grad_norm": 1.3364392518997192,
      "learning_rate": 5e-05,
      "loss": 1.2876,
      "step": 2325
    },
    {
      "epoch": 2.9443037974683546,
      "grad_norm": 1.2854493856430054,
      "learning_rate": 5e-05,
      "loss": 1.1916,
      "step": 2326
    },
    {
      "epoch": 2.9455696202531647,
      "grad_norm": 1.2988263368606567,
      "learning_rate": 5e-05,
      "loss": 1.1716,
      "step": 2327
    },
    {
      "epoch": 2.946835443037975,
      "grad_norm": 1.3503425121307373,
      "learning_rate": 5e-05,
      "loss": 1.3393,
      "step": 2328
    },
    {
      "epoch": 2.9481012658227845,
      "grad_norm": 1.3913558721542358,
      "learning_rate": 5e-05,
      "loss": 1.279,
      "step": 2329
    },
    {
      "epoch": 2.9493670886075947,
      "grad_norm": 1.3326094150543213,
      "learning_rate": 5e-05,
      "loss": 1.174,
      "step": 2330
    },
    {
      "epoch": 2.950632911392405,
      "grad_norm": 1.3459478616714478,
      "learning_rate": 5e-05,
      "loss": 1.1628,
      "step": 2331
    },
    {
      "epoch": 2.951898734177215,
      "grad_norm": 1.2950693368911743,
      "learning_rate": 5e-05,
      "loss": 1.1679,
      "step": 2332
    },
    {
      "epoch": 2.953164556962025,
      "grad_norm": 1.3499222993850708,
      "learning_rate": 5e-05,
      "loss": 1.238,
      "step": 2333
    },
    {
      "epoch": 2.954430379746835,
      "grad_norm": 1.3196009397506714,
      "learning_rate": 5e-05,
      "loss": 1.1988,
      "step": 2334
    },
    {
      "epoch": 2.9556962025316453,
      "grad_norm": 1.3489683866500854,
      "learning_rate": 5e-05,
      "loss": 1.2628,
      "step": 2335
    },
    {
      "epoch": 2.9569620253164555,
      "grad_norm": 1.294455647468567,
      "learning_rate": 5e-05,
      "loss": 1.223,
      "step": 2336
    },
    {
      "epoch": 2.9582278481012656,
      "grad_norm": 1.337517499923706,
      "learning_rate": 5e-05,
      "loss": 1.1953,
      "step": 2337
    },
    {
      "epoch": 2.9594936708860757,
      "grad_norm": 1.3625903129577637,
      "learning_rate": 5e-05,
      "loss": 1.2361,
      "step": 2338
    },
    {
      "epoch": 2.960759493670886,
      "grad_norm": 1.2748850584030151,
      "learning_rate": 5e-05,
      "loss": 1.2206,
      "step": 2339
    },
    {
      "epoch": 2.962025316455696,
      "grad_norm": 1.3379415273666382,
      "learning_rate": 5e-05,
      "loss": 1.2278,
      "step": 2340
    },
    {
      "epoch": 2.963291139240506,
      "grad_norm": 1.3684443235397339,
      "learning_rate": 5e-05,
      "loss": 1.212,
      "step": 2341
    },
    {
      "epoch": 2.9645569620253163,
      "grad_norm": 1.3310490846633911,
      "learning_rate": 5e-05,
      "loss": 1.1539,
      "step": 2342
    },
    {
      "epoch": 2.9658227848101264,
      "grad_norm": 1.2913146018981934,
      "learning_rate": 5e-05,
      "loss": 1.1736,
      "step": 2343
    },
    {
      "epoch": 2.9670886075949365,
      "grad_norm": 1.3892865180969238,
      "learning_rate": 5e-05,
      "loss": 1.1796,
      "step": 2344
    },
    {
      "epoch": 2.9683544303797467,
      "grad_norm": 1.2879555225372314,
      "learning_rate": 5e-05,
      "loss": 1.124,
      "step": 2345
    },
    {
      "epoch": 2.969620253164557,
      "grad_norm": 1.336491346359253,
      "learning_rate": 5e-05,
      "loss": 1.2329,
      "step": 2346
    },
    {
      "epoch": 2.970886075949367,
      "grad_norm": 1.3531545400619507,
      "learning_rate": 5e-05,
      "loss": 1.2043,
      "step": 2347
    },
    {
      "epoch": 2.972151898734177,
      "grad_norm": 1.3113402128219604,
      "learning_rate": 5e-05,
      "loss": 1.1861,
      "step": 2348
    },
    {
      "epoch": 2.973417721518987,
      "grad_norm": 1.309456706047058,
      "learning_rate": 5e-05,
      "loss": 1.1757,
      "step": 2349
    },
    {
      "epoch": 2.9746835443037973,
      "grad_norm": 1.3400003910064697,
      "learning_rate": 5e-05,
      "loss": 1.2139,
      "step": 2350
    },
    {
      "epoch": 2.9759493670886075,
      "grad_norm": 1.2826896905899048,
      "learning_rate": 5e-05,
      "loss": 1.1453,
      "step": 2351
    },
    {
      "epoch": 2.9772151898734176,
      "grad_norm": 1.2842779159545898,
      "learning_rate": 5e-05,
      "loss": 1.1756,
      "step": 2352
    },
    {
      "epoch": 2.9784810126582277,
      "grad_norm": 1.3295254707336426,
      "learning_rate": 5e-05,
      "loss": 1.1824,
      "step": 2353
    },
    {
      "epoch": 2.979746835443038,
      "grad_norm": 1.2920745611190796,
      "learning_rate": 5e-05,
      "loss": 1.122,
      "step": 2354
    },
    {
      "epoch": 2.981012658227848,
      "grad_norm": 1.3027721643447876,
      "learning_rate": 5e-05,
      "loss": 1.2148,
      "step": 2355
    },
    {
      "epoch": 2.982278481012658,
      "grad_norm": 1.3496097326278687,
      "learning_rate": 5e-05,
      "loss": 1.2508,
      "step": 2356
    },
    {
      "epoch": 2.9835443037974683,
      "grad_norm": 1.3195841312408447,
      "learning_rate": 5e-05,
      "loss": 1.2025,
      "step": 2357
    },
    {
      "epoch": 2.9848101265822784,
      "grad_norm": 1.3086061477661133,
      "learning_rate": 5e-05,
      "loss": 1.2008,
      "step": 2358
    },
    {
      "epoch": 2.9860759493670885,
      "grad_norm": 1.3070383071899414,
      "learning_rate": 5e-05,
      "loss": 1.2627,
      "step": 2359
    },
    {
      "epoch": 2.9873417721518987,
      "grad_norm": 1.3485995531082153,
      "learning_rate": 5e-05,
      "loss": 1.2043,
      "step": 2360
    },
    {
      "epoch": 2.988607594936709,
      "grad_norm": 1.2932641506195068,
      "learning_rate": 5e-05,
      "loss": 1.2067,
      "step": 2361
    },
    {
      "epoch": 2.989873417721519,
      "grad_norm": 1.3113659620285034,
      "learning_rate": 5e-05,
      "loss": 1.2435,
      "step": 2362
    },
    {
      "epoch": 2.991139240506329,
      "grad_norm": 1.307693600654602,
      "learning_rate": 5e-05,
      "loss": 1.1544,
      "step": 2363
    },
    {
      "epoch": 2.992405063291139,
      "grad_norm": 1.3376941680908203,
      "learning_rate": 5e-05,
      "loss": 1.2322,
      "step": 2364
    },
    {
      "epoch": 2.9936708860759493,
      "grad_norm": 1.295185923576355,
      "learning_rate": 5e-05,
      "loss": 1.1759,
      "step": 2365
    },
    {
      "epoch": 2.9949367088607595,
      "grad_norm": 1.344977617263794,
      "learning_rate": 5e-05,
      "loss": 1.2119,
      "step": 2366
    },
    {
      "epoch": 2.9962025316455696,
      "grad_norm": 1.358130931854248,
      "learning_rate": 5e-05,
      "loss": 1.2,
      "step": 2367
    },
    {
      "epoch": 2.9974683544303797,
      "grad_norm": 1.315894365310669,
      "learning_rate": 5e-05,
      "loss": 1.1617,
      "step": 2368
    },
    {
      "epoch": 2.99873417721519,
      "grad_norm": 1.3194787502288818,
      "learning_rate": 5e-05,
      "loss": 1.1993,
      "step": 2369
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.3126848936080933,
      "learning_rate": 5e-05,
      "loss": 1.2239,
      "step": 2370
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.03720760345459,
      "eval_runtime": 163.9511,
      "eval_samples_per_second": 548.212,
      "eval_steps_per_second": 4.288,
      "step": 2370
    },
    {
      "epoch": 3.00126582278481,
      "grad_norm": 1.3155126571655273,
      "learning_rate": 5e-05,
      "loss": 1.1143,
      "step": 2371
    },
    {
      "epoch": 3.0025316455696203,
      "grad_norm": 1.3224631547927856,
      "learning_rate": 5e-05,
      "loss": 1.1349,
      "step": 2372
    },
    {
      "epoch": 3.0037974683544304,
      "grad_norm": 1.2641839981079102,
      "learning_rate": 5e-05,
      "loss": 1.1794,
      "step": 2373
    },
    {
      "epoch": 3.0050632911392405,
      "grad_norm": 1.3049954175949097,
      "learning_rate": 5e-05,
      "loss": 1.1879,
      "step": 2374
    },
    {
      "epoch": 3.0063291139240507,
      "grad_norm": 1.295275330543518,
      "learning_rate": 5e-05,
      "loss": 1.1273,
      "step": 2375
    },
    {
      "epoch": 3.007594936708861,
      "grad_norm": 1.305314064025879,
      "learning_rate": 5e-05,
      "loss": 1.0903,
      "step": 2376
    },
    {
      "epoch": 3.008860759493671,
      "grad_norm": 1.304819107055664,
      "learning_rate": 5e-05,
      "loss": 1.1713,
      "step": 2377
    },
    {
      "epoch": 3.010126582278481,
      "grad_norm": 1.3083268404006958,
      "learning_rate": 5e-05,
      "loss": 1.1662,
      "step": 2378
    },
    {
      "epoch": 3.011392405063291,
      "grad_norm": 1.2969138622283936,
      "learning_rate": 5e-05,
      "loss": 1.1335,
      "step": 2379
    },
    {
      "epoch": 3.0126582278481013,
      "grad_norm": 1.3032468557357788,
      "learning_rate": 5e-05,
      "loss": 1.1751,
      "step": 2380
    },
    {
      "epoch": 3.0139240506329115,
      "grad_norm": 1.266042709350586,
      "learning_rate": 5e-05,
      "loss": 1.1232,
      "step": 2381
    },
    {
      "epoch": 3.0151898734177216,
      "grad_norm": 1.2584114074707031,
      "learning_rate": 5e-05,
      "loss": 1.1255,
      "step": 2382
    },
    {
      "epoch": 3.0164556962025317,
      "grad_norm": 1.3005138635635376,
      "learning_rate": 5e-05,
      "loss": 1.1231,
      "step": 2383
    },
    {
      "epoch": 3.017721518987342,
      "grad_norm": 1.27886962890625,
      "learning_rate": 5e-05,
      "loss": 1.0974,
      "step": 2384
    },
    {
      "epoch": 3.018987341772152,
      "grad_norm": 1.2802815437316895,
      "learning_rate": 5e-05,
      "loss": 1.193,
      "step": 2385
    },
    {
      "epoch": 3.020253164556962,
      "grad_norm": 1.2527568340301514,
      "learning_rate": 5e-05,
      "loss": 1.1111,
      "step": 2386
    },
    {
      "epoch": 3.0215189873417723,
      "grad_norm": 1.292548656463623,
      "learning_rate": 5e-05,
      "loss": 1.0803,
      "step": 2387
    },
    {
      "epoch": 3.0227848101265824,
      "grad_norm": 1.297741174697876,
      "learning_rate": 5e-05,
      "loss": 1.1827,
      "step": 2388
    },
    {
      "epoch": 3.0240506329113925,
      "grad_norm": 1.297577977180481,
      "learning_rate": 5e-05,
      "loss": 1.1245,
      "step": 2389
    },
    {
      "epoch": 3.0253164556962027,
      "grad_norm": 1.325022578239441,
      "learning_rate": 5e-05,
      "loss": 1.1462,
      "step": 2390
    },
    {
      "epoch": 3.026582278481013,
      "grad_norm": 1.3082153797149658,
      "learning_rate": 5e-05,
      "loss": 1.1873,
      "step": 2391
    },
    {
      "epoch": 3.027848101265823,
      "grad_norm": 1.322274923324585,
      "learning_rate": 5e-05,
      "loss": 1.1491,
      "step": 2392
    },
    {
      "epoch": 3.029113924050633,
      "grad_norm": 1.2709468603134155,
      "learning_rate": 5e-05,
      "loss": 1.0743,
      "step": 2393
    },
    {
      "epoch": 3.030379746835443,
      "grad_norm": 1.3558878898620605,
      "learning_rate": 5e-05,
      "loss": 1.2388,
      "step": 2394
    },
    {
      "epoch": 3.0316455696202533,
      "grad_norm": 1.301396369934082,
      "learning_rate": 5e-05,
      "loss": 1.1272,
      "step": 2395
    },
    {
      "epoch": 3.0329113924050635,
      "grad_norm": 1.324160099029541,
      "learning_rate": 5e-05,
      "loss": 1.1626,
      "step": 2396
    },
    {
      "epoch": 3.0341772151898736,
      "grad_norm": 1.2717264890670776,
      "learning_rate": 5e-05,
      "loss": 1.1126,
      "step": 2397
    },
    {
      "epoch": 3.0354430379746837,
      "grad_norm": 1.3253099918365479,
      "learning_rate": 5e-05,
      "loss": 1.1056,
      "step": 2398
    },
    {
      "epoch": 3.036708860759494,
      "grad_norm": 1.3128048181533813,
      "learning_rate": 5e-05,
      "loss": 1.1366,
      "step": 2399
    },
    {
      "epoch": 3.037974683544304,
      "grad_norm": 1.238421082496643,
      "learning_rate": 5e-05,
      "loss": 1.1133,
      "step": 2400
    },
    {
      "epoch": 3.039240506329114,
      "grad_norm": 1.3080909252166748,
      "learning_rate": 5e-05,
      "loss": 1.1008,
      "step": 2401
    },
    {
      "epoch": 3.0405063291139243,
      "grad_norm": 1.2755248546600342,
      "learning_rate": 5e-05,
      "loss": 1.0932,
      "step": 2402
    },
    {
      "epoch": 3.0417721518987344,
      "grad_norm": 1.345920205116272,
      "learning_rate": 5e-05,
      "loss": 1.1565,
      "step": 2403
    },
    {
      "epoch": 3.043037974683544,
      "grad_norm": 1.2860859632492065,
      "learning_rate": 5e-05,
      "loss": 1.1712,
      "step": 2404
    },
    {
      "epoch": 3.0443037974683542,
      "grad_norm": 1.314213752746582,
      "learning_rate": 5e-05,
      "loss": 1.1679,
      "step": 2405
    },
    {
      "epoch": 3.0455696202531644,
      "grad_norm": 1.3254116773605347,
      "learning_rate": 5e-05,
      "loss": 1.1893,
      "step": 2406
    },
    {
      "epoch": 3.0468354430379745,
      "grad_norm": 1.3102935552597046,
      "learning_rate": 5e-05,
      "loss": 1.1997,
      "step": 2407
    },
    {
      "epoch": 3.0481012658227846,
      "grad_norm": 1.2673797607421875,
      "learning_rate": 5e-05,
      "loss": 1.0884,
      "step": 2408
    },
    {
      "epoch": 3.0493670886075948,
      "grad_norm": 1.2910048961639404,
      "learning_rate": 5e-05,
      "loss": 1.1523,
      "step": 2409
    },
    {
      "epoch": 3.050632911392405,
      "grad_norm": 1.2773476839065552,
      "learning_rate": 5e-05,
      "loss": 1.1384,
      "step": 2410
    },
    {
      "epoch": 3.051898734177215,
      "grad_norm": 1.3381242752075195,
      "learning_rate": 5e-05,
      "loss": 1.1617,
      "step": 2411
    },
    {
      "epoch": 3.053164556962025,
      "grad_norm": 1.3101983070373535,
      "learning_rate": 5e-05,
      "loss": 1.1444,
      "step": 2412
    },
    {
      "epoch": 3.0544303797468353,
      "grad_norm": 1.3002676963806152,
      "learning_rate": 5e-05,
      "loss": 1.2161,
      "step": 2413
    },
    {
      "epoch": 3.0556962025316454,
      "grad_norm": 1.2985832691192627,
      "learning_rate": 5e-05,
      "loss": 1.1313,
      "step": 2414
    },
    {
      "epoch": 3.0569620253164556,
      "grad_norm": 1.3183513879776,
      "learning_rate": 5e-05,
      "loss": 1.1399,
      "step": 2415
    },
    {
      "epoch": 3.0582278481012657,
      "grad_norm": 1.321375846862793,
      "learning_rate": 5e-05,
      "loss": 1.0659,
      "step": 2416
    },
    {
      "epoch": 3.059493670886076,
      "grad_norm": 1.321326732635498,
      "learning_rate": 5e-05,
      "loss": 1.1164,
      "step": 2417
    },
    {
      "epoch": 3.060759493670886,
      "grad_norm": 1.295511245727539,
      "learning_rate": 5e-05,
      "loss": 1.1454,
      "step": 2418
    },
    {
      "epoch": 3.062025316455696,
      "grad_norm": 1.3327332735061646,
      "learning_rate": 5e-05,
      "loss": 1.1912,
      "step": 2419
    },
    {
      "epoch": 3.0632911392405062,
      "grad_norm": 1.3005307912826538,
      "learning_rate": 5e-05,
      "loss": 1.1316,
      "step": 2420
    },
    {
      "epoch": 3.0645569620253164,
      "grad_norm": 1.2870374917984009,
      "learning_rate": 5e-05,
      "loss": 1.116,
      "step": 2421
    },
    {
      "epoch": 3.0658227848101265,
      "grad_norm": 1.3228645324707031,
      "learning_rate": 5e-05,
      "loss": 1.1272,
      "step": 2422
    },
    {
      "epoch": 3.0670886075949366,
      "grad_norm": 1.3250137567520142,
      "learning_rate": 5e-05,
      "loss": 1.1532,
      "step": 2423
    },
    {
      "epoch": 3.0683544303797468,
      "grad_norm": 1.285709261894226,
      "learning_rate": 5e-05,
      "loss": 1.1111,
      "step": 2424
    },
    {
      "epoch": 3.069620253164557,
      "grad_norm": 1.2623103857040405,
      "learning_rate": 5e-05,
      "loss": 1.0698,
      "step": 2425
    },
    {
      "epoch": 3.070886075949367,
      "grad_norm": 1.3211177587509155,
      "learning_rate": 5e-05,
      "loss": 1.1045,
      "step": 2426
    },
    {
      "epoch": 3.072151898734177,
      "grad_norm": 1.2917990684509277,
      "learning_rate": 5e-05,
      "loss": 1.1074,
      "step": 2427
    },
    {
      "epoch": 3.0734177215189873,
      "grad_norm": 1.284803032875061,
      "learning_rate": 5e-05,
      "loss": 1.0623,
      "step": 2428
    },
    {
      "epoch": 3.0746835443037974,
      "grad_norm": 1.2893950939178467,
      "learning_rate": 5e-05,
      "loss": 1.1255,
      "step": 2429
    },
    {
      "epoch": 3.0759493670886076,
      "grad_norm": 1.2861676216125488,
      "learning_rate": 5e-05,
      "loss": 1.1147,
      "step": 2430
    },
    {
      "epoch": 3.0772151898734177,
      "grad_norm": 1.3061809539794922,
      "learning_rate": 5e-05,
      "loss": 1.1106,
      "step": 2431
    },
    {
      "epoch": 3.078481012658228,
      "grad_norm": 1.33143150806427,
      "learning_rate": 5e-05,
      "loss": 1.1376,
      "step": 2432
    },
    {
      "epoch": 3.079746835443038,
      "grad_norm": 1.3187472820281982,
      "learning_rate": 5e-05,
      "loss": 1.16,
      "step": 2433
    },
    {
      "epoch": 3.081012658227848,
      "grad_norm": 1.2530614137649536,
      "learning_rate": 5e-05,
      "loss": 0.9736,
      "step": 2434
    },
    {
      "epoch": 3.0822784810126582,
      "grad_norm": 1.3141731023788452,
      "learning_rate": 5e-05,
      "loss": 1.1249,
      "step": 2435
    },
    {
      "epoch": 3.0835443037974684,
      "grad_norm": 1.2729307413101196,
      "learning_rate": 5e-05,
      "loss": 1.1101,
      "step": 2436
    },
    {
      "epoch": 3.0848101265822785,
      "grad_norm": 1.3399486541748047,
      "learning_rate": 5e-05,
      "loss": 1.1285,
      "step": 2437
    },
    {
      "epoch": 3.0860759493670886,
      "grad_norm": 1.2766473293304443,
      "learning_rate": 5e-05,
      "loss": 1.0463,
      "step": 2438
    },
    {
      "epoch": 3.0873417721518988,
      "grad_norm": 1.2888267040252686,
      "learning_rate": 5e-05,
      "loss": 1.1296,
      "step": 2439
    },
    {
      "epoch": 3.088607594936709,
      "grad_norm": 1.365148901939392,
      "learning_rate": 5e-05,
      "loss": 1.1872,
      "step": 2440
    },
    {
      "epoch": 3.089873417721519,
      "grad_norm": 1.3012845516204834,
      "learning_rate": 5e-05,
      "loss": 1.1002,
      "step": 2441
    },
    {
      "epoch": 3.091139240506329,
      "grad_norm": 1.348162293434143,
      "learning_rate": 5e-05,
      "loss": 1.1445,
      "step": 2442
    },
    {
      "epoch": 3.0924050632911393,
      "grad_norm": 1.3002195358276367,
      "learning_rate": 5e-05,
      "loss": 1.2042,
      "step": 2443
    },
    {
      "epoch": 3.0936708860759494,
      "grad_norm": 1.291436791419983,
      "learning_rate": 5e-05,
      "loss": 1.0632,
      "step": 2444
    },
    {
      "epoch": 3.0949367088607596,
      "grad_norm": 1.3086037635803223,
      "learning_rate": 5e-05,
      "loss": 1.1348,
      "step": 2445
    },
    {
      "epoch": 3.0962025316455697,
      "grad_norm": 1.310253381729126,
      "learning_rate": 5e-05,
      "loss": 1.0993,
      "step": 2446
    },
    {
      "epoch": 3.09746835443038,
      "grad_norm": 1.299794316291809,
      "learning_rate": 5e-05,
      "loss": 1.1391,
      "step": 2447
    },
    {
      "epoch": 3.09873417721519,
      "grad_norm": 1.2834627628326416,
      "learning_rate": 5e-05,
      "loss": 1.0548,
      "step": 2448
    },
    {
      "epoch": 3.1,
      "grad_norm": 1.2947664260864258,
      "learning_rate": 5e-05,
      "loss": 1.0738,
      "step": 2449
    },
    {
      "epoch": 3.1012658227848102,
      "grad_norm": 1.2995123863220215,
      "learning_rate": 5e-05,
      "loss": 1.1199,
      "step": 2450
    },
    {
      "epoch": 3.1025316455696204,
      "grad_norm": 1.309324026107788,
      "learning_rate": 5e-05,
      "loss": 1.1511,
      "step": 2451
    },
    {
      "epoch": 3.1037974683544305,
      "grad_norm": 1.3186601400375366,
      "learning_rate": 5e-05,
      "loss": 1.1736,
      "step": 2452
    },
    {
      "epoch": 3.1050632911392406,
      "grad_norm": 1.2880940437316895,
      "learning_rate": 5e-05,
      "loss": 1.079,
      "step": 2453
    },
    {
      "epoch": 3.1063291139240508,
      "grad_norm": 1.331608533859253,
      "learning_rate": 5e-05,
      "loss": 1.1462,
      "step": 2454
    },
    {
      "epoch": 3.107594936708861,
      "grad_norm": 1.2518137693405151,
      "learning_rate": 5e-05,
      "loss": 1.0947,
      "step": 2455
    },
    {
      "epoch": 3.108860759493671,
      "grad_norm": 1.351462483406067,
      "learning_rate": 5e-05,
      "loss": 1.1657,
      "step": 2456
    },
    {
      "epoch": 3.110126582278481,
      "grad_norm": 1.3242751359939575,
      "learning_rate": 5e-05,
      "loss": 1.1411,
      "step": 2457
    },
    {
      "epoch": 3.1113924050632913,
      "grad_norm": 1.2825894355773926,
      "learning_rate": 5e-05,
      "loss": 1.1325,
      "step": 2458
    },
    {
      "epoch": 3.1126582278481014,
      "grad_norm": 1.2795062065124512,
      "learning_rate": 5e-05,
      "loss": 1.1074,
      "step": 2459
    },
    {
      "epoch": 3.1139240506329116,
      "grad_norm": 1.3804795742034912,
      "learning_rate": 5e-05,
      "loss": 1.1585,
      "step": 2460
    },
    {
      "epoch": 3.1151898734177217,
      "grad_norm": 1.343955159187317,
      "learning_rate": 5e-05,
      "loss": 1.1163,
      "step": 2461
    },
    {
      "epoch": 3.116455696202532,
      "grad_norm": 1.3163981437683105,
      "learning_rate": 5e-05,
      "loss": 1.127,
      "step": 2462
    },
    {
      "epoch": 3.117721518987342,
      "grad_norm": 1.294797420501709,
      "learning_rate": 5e-05,
      "loss": 1.1113,
      "step": 2463
    },
    {
      "epoch": 3.118987341772152,
      "grad_norm": 1.287209391593933,
      "learning_rate": 5e-05,
      "loss": 1.1513,
      "step": 2464
    },
    {
      "epoch": 3.1202531645569622,
      "grad_norm": 1.2710330486297607,
      "learning_rate": 5e-05,
      "loss": 1.1456,
      "step": 2465
    },
    {
      "epoch": 3.1215189873417724,
      "grad_norm": 1.3672990798950195,
      "learning_rate": 5e-05,
      "loss": 1.1625,
      "step": 2466
    },
    {
      "epoch": 3.1227848101265825,
      "grad_norm": 1.312242031097412,
      "learning_rate": 5e-05,
      "loss": 1.1349,
      "step": 2467
    },
    {
      "epoch": 3.124050632911392,
      "grad_norm": 1.3621037006378174,
      "learning_rate": 5e-05,
      "loss": 1.1685,
      "step": 2468
    },
    {
      "epoch": 3.1253164556962023,
      "grad_norm": 1.2774306535720825,
      "learning_rate": 5e-05,
      "loss": 1.1411,
      "step": 2469
    },
    {
      "epoch": 3.1265822784810124,
      "grad_norm": 1.3091758489608765,
      "learning_rate": 5e-05,
      "loss": 1.1071,
      "step": 2470
    },
    {
      "epoch": 3.1278481012658226,
      "grad_norm": 1.3376036882400513,
      "learning_rate": 5e-05,
      "loss": 1.1749,
      "step": 2471
    },
    {
      "epoch": 3.1291139240506327,
      "grad_norm": 1.3135336637496948,
      "learning_rate": 5e-05,
      "loss": 1.117,
      "step": 2472
    },
    {
      "epoch": 3.130379746835443,
      "grad_norm": 1.3123300075531006,
      "learning_rate": 5e-05,
      "loss": 1.1032,
      "step": 2473
    },
    {
      "epoch": 3.131645569620253,
      "grad_norm": 1.3007447719573975,
      "learning_rate": 5e-05,
      "loss": 1.1459,
      "step": 2474
    },
    {
      "epoch": 3.132911392405063,
      "grad_norm": 1.319293737411499,
      "learning_rate": 5e-05,
      "loss": 1.1294,
      "step": 2475
    },
    {
      "epoch": 3.1341772151898732,
      "grad_norm": 1.2863374948501587,
      "learning_rate": 5e-05,
      "loss": 1.092,
      "step": 2476
    },
    {
      "epoch": 3.1354430379746834,
      "grad_norm": 1.2825475931167603,
      "learning_rate": 5e-05,
      "loss": 1.047,
      "step": 2477
    },
    {
      "epoch": 3.1367088607594935,
      "grad_norm": 1.2360870838165283,
      "learning_rate": 5e-05,
      "loss": 1.0952,
      "step": 2478
    },
    {
      "epoch": 3.1379746835443036,
      "grad_norm": 1.3277148008346558,
      "learning_rate": 5e-05,
      "loss": 1.056,
      "step": 2479
    },
    {
      "epoch": 3.1392405063291138,
      "grad_norm": 1.3045322895050049,
      "learning_rate": 5e-05,
      "loss": 1.1495,
      "step": 2480
    },
    {
      "epoch": 3.140506329113924,
      "grad_norm": 1.3132922649383545,
      "learning_rate": 5e-05,
      "loss": 1.1114,
      "step": 2481
    },
    {
      "epoch": 3.141772151898734,
      "grad_norm": 1.321575403213501,
      "learning_rate": 5e-05,
      "loss": 1.0905,
      "step": 2482
    },
    {
      "epoch": 3.143037974683544,
      "grad_norm": 1.3397823572158813,
      "learning_rate": 5e-05,
      "loss": 1.2504,
      "step": 2483
    },
    {
      "epoch": 3.1443037974683543,
      "grad_norm": 1.324913501739502,
      "learning_rate": 5e-05,
      "loss": 1.124,
      "step": 2484
    },
    {
      "epoch": 3.1455696202531644,
      "grad_norm": 1.341536521911621,
      "learning_rate": 5e-05,
      "loss": 1.2127,
      "step": 2485
    },
    {
      "epoch": 3.1468354430379746,
      "grad_norm": 1.361648440361023,
      "learning_rate": 5e-05,
      "loss": 1.1733,
      "step": 2486
    },
    {
      "epoch": 3.1481012658227847,
      "grad_norm": 1.3179874420166016,
      "learning_rate": 5e-05,
      "loss": 1.185,
      "step": 2487
    },
    {
      "epoch": 3.149367088607595,
      "grad_norm": 1.268661379814148,
      "learning_rate": 5e-05,
      "loss": 1.073,
      "step": 2488
    },
    {
      "epoch": 3.150632911392405,
      "grad_norm": 1.3227301836013794,
      "learning_rate": 5e-05,
      "loss": 1.193,
      "step": 2489
    },
    {
      "epoch": 3.151898734177215,
      "grad_norm": 1.3201756477355957,
      "learning_rate": 5e-05,
      "loss": 1.1774,
      "step": 2490
    },
    {
      "epoch": 3.1531645569620252,
      "grad_norm": 1.3146659135818481,
      "learning_rate": 5e-05,
      "loss": 1.092,
      "step": 2491
    },
    {
      "epoch": 3.1544303797468354,
      "grad_norm": 1.320520281791687,
      "learning_rate": 5e-05,
      "loss": 1.072,
      "step": 2492
    },
    {
      "epoch": 3.1556962025316455,
      "grad_norm": 1.3197951316833496,
      "learning_rate": 5e-05,
      "loss": 1.1184,
      "step": 2493
    },
    {
      "epoch": 3.1569620253164556,
      "grad_norm": 1.3436928987503052,
      "learning_rate": 5e-05,
      "loss": 1.1116,
      "step": 2494
    },
    {
      "epoch": 3.1582278481012658,
      "grad_norm": 1.2762004137039185,
      "learning_rate": 5e-05,
      "loss": 1.0646,
      "step": 2495
    },
    {
      "epoch": 3.159493670886076,
      "grad_norm": 1.2843165397644043,
      "learning_rate": 5e-05,
      "loss": 1.1489,
      "step": 2496
    },
    {
      "epoch": 3.160759493670886,
      "grad_norm": 1.3287886381149292,
      "learning_rate": 5e-05,
      "loss": 1.168,
      "step": 2497
    },
    {
      "epoch": 3.162025316455696,
      "grad_norm": 1.3698145151138306,
      "learning_rate": 5e-05,
      "loss": 1.1879,
      "step": 2498
    },
    {
      "epoch": 3.1632911392405063,
      "grad_norm": 1.3189080953598022,
      "learning_rate": 5e-05,
      "loss": 1.1258,
      "step": 2499
    },
    {
      "epoch": 3.1645569620253164,
      "grad_norm": 1.365583896636963,
      "learning_rate": 5e-05,
      "loss": 1.12,
      "step": 2500
    },
    {
      "epoch": 3.1658227848101266,
      "grad_norm": 1.3107976913452148,
      "learning_rate": 5e-05,
      "loss": 1.1273,
      "step": 2501
    },
    {
      "epoch": 3.1670886075949367,
      "grad_norm": 1.3855026960372925,
      "learning_rate": 5e-05,
      "loss": 1.1691,
      "step": 2502
    },
    {
      "epoch": 3.168354430379747,
      "grad_norm": 1.339606523513794,
      "learning_rate": 5e-05,
      "loss": 1.0991,
      "step": 2503
    },
    {
      "epoch": 3.169620253164557,
      "grad_norm": 1.3143497705459595,
      "learning_rate": 5e-05,
      "loss": 1.1135,
      "step": 2504
    },
    {
      "epoch": 3.170886075949367,
      "grad_norm": 1.3180948495864868,
      "learning_rate": 5e-05,
      "loss": 1.1328,
      "step": 2505
    },
    {
      "epoch": 3.1721518987341772,
      "grad_norm": 1.294923186302185,
      "learning_rate": 5e-05,
      "loss": 1.1065,
      "step": 2506
    },
    {
      "epoch": 3.1734177215189874,
      "grad_norm": 1.4021235704421997,
      "learning_rate": 5e-05,
      "loss": 1.1785,
      "step": 2507
    },
    {
      "epoch": 3.1746835443037975,
      "grad_norm": 1.28983736038208,
      "learning_rate": 5e-05,
      "loss": 1.131,
      "step": 2508
    },
    {
      "epoch": 3.1759493670886076,
      "grad_norm": 1.3040518760681152,
      "learning_rate": 5e-05,
      "loss": 1.0877,
      "step": 2509
    },
    {
      "epoch": 3.1772151898734178,
      "grad_norm": 1.3167731761932373,
      "learning_rate": 5e-05,
      "loss": 1.099,
      "step": 2510
    },
    {
      "epoch": 3.178481012658228,
      "grad_norm": 1.2821205854415894,
      "learning_rate": 5e-05,
      "loss": 1.1231,
      "step": 2511
    },
    {
      "epoch": 3.179746835443038,
      "grad_norm": 1.3714663982391357,
      "learning_rate": 5e-05,
      "loss": 1.1978,
      "step": 2512
    },
    {
      "epoch": 3.181012658227848,
      "grad_norm": 1.2999142408370972,
      "learning_rate": 5e-05,
      "loss": 1.1212,
      "step": 2513
    },
    {
      "epoch": 3.1822784810126583,
      "grad_norm": 1.3301259279251099,
      "learning_rate": 5e-05,
      "loss": 1.1368,
      "step": 2514
    },
    {
      "epoch": 3.1835443037974684,
      "grad_norm": 1.2924433946609497,
      "learning_rate": 5e-05,
      "loss": 1.1392,
      "step": 2515
    },
    {
      "epoch": 3.1848101265822786,
      "grad_norm": 1.3398709297180176,
      "learning_rate": 5e-05,
      "loss": 1.1507,
      "step": 2516
    },
    {
      "epoch": 3.1860759493670887,
      "grad_norm": 1.2952598333358765,
      "learning_rate": 5e-05,
      "loss": 1.036,
      "step": 2517
    },
    {
      "epoch": 3.187341772151899,
      "grad_norm": 1.3082127571105957,
      "learning_rate": 5e-05,
      "loss": 1.1139,
      "step": 2518
    },
    {
      "epoch": 3.188607594936709,
      "grad_norm": 1.3206113576889038,
      "learning_rate": 5e-05,
      "loss": 1.1656,
      "step": 2519
    },
    {
      "epoch": 3.189873417721519,
      "grad_norm": 1.2938224077224731,
      "learning_rate": 5e-05,
      "loss": 1.0299,
      "step": 2520
    },
    {
      "epoch": 3.1911392405063292,
      "grad_norm": 1.275759220123291,
      "learning_rate": 5e-05,
      "loss": 1.1221,
      "step": 2521
    },
    {
      "epoch": 3.1924050632911394,
      "grad_norm": 1.3150948286056519,
      "learning_rate": 5e-05,
      "loss": 1.0921,
      "step": 2522
    },
    {
      "epoch": 3.1936708860759495,
      "grad_norm": 1.2768568992614746,
      "learning_rate": 5e-05,
      "loss": 1.1265,
      "step": 2523
    },
    {
      "epoch": 3.1949367088607596,
      "grad_norm": 1.3210062980651855,
      "learning_rate": 5e-05,
      "loss": 1.0949,
      "step": 2524
    },
    {
      "epoch": 3.1962025316455698,
      "grad_norm": 1.3625518083572388,
      "learning_rate": 5e-05,
      "loss": 1.1171,
      "step": 2525
    },
    {
      "epoch": 3.19746835443038,
      "grad_norm": 1.4141148328781128,
      "learning_rate": 5e-05,
      "loss": 1.1787,
      "step": 2526
    },
    {
      "epoch": 3.19873417721519,
      "grad_norm": 1.3304636478424072,
      "learning_rate": 5e-05,
      "loss": 1.1539,
      "step": 2527
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.345313310623169,
      "learning_rate": 5e-05,
      "loss": 1.1378,
      "step": 2528
    },
    {
      "epoch": 3.2012658227848103,
      "grad_norm": 1.3091754913330078,
      "learning_rate": 5e-05,
      "loss": 1.0871,
      "step": 2529
    },
    {
      "epoch": 3.2025316455696204,
      "grad_norm": 1.3368918895721436,
      "learning_rate": 5e-05,
      "loss": 1.1691,
      "step": 2530
    },
    {
      "epoch": 3.2037974683544306,
      "grad_norm": 1.3354535102844238,
      "learning_rate": 5e-05,
      "loss": 1.1155,
      "step": 2531
    },
    {
      "epoch": 3.2050632911392407,
      "grad_norm": 1.283767819404602,
      "learning_rate": 5e-05,
      "loss": 1.069,
      "step": 2532
    },
    {
      "epoch": 3.206329113924051,
      "grad_norm": 1.3003840446472168,
      "learning_rate": 5e-05,
      "loss": 1.135,
      "step": 2533
    },
    {
      "epoch": 3.207594936708861,
      "grad_norm": 1.2940683364868164,
      "learning_rate": 5e-05,
      "loss": 1.05,
      "step": 2534
    },
    {
      "epoch": 3.208860759493671,
      "grad_norm": 1.3277616500854492,
      "learning_rate": 5e-05,
      "loss": 1.1301,
      "step": 2535
    },
    {
      "epoch": 3.2101265822784812,
      "grad_norm": 1.3522149324417114,
      "learning_rate": 5e-05,
      "loss": 1.1302,
      "step": 2536
    },
    {
      "epoch": 3.211392405063291,
      "grad_norm": 1.3374751806259155,
      "learning_rate": 5e-05,
      "loss": 1.111,
      "step": 2537
    },
    {
      "epoch": 3.212658227848101,
      "grad_norm": 1.3460487127304077,
      "learning_rate": 5e-05,
      "loss": 1.1476,
      "step": 2538
    },
    {
      "epoch": 3.213924050632911,
      "grad_norm": 1.3284142017364502,
      "learning_rate": 5e-05,
      "loss": 1.0792,
      "step": 2539
    },
    {
      "epoch": 3.2151898734177213,
      "grad_norm": 1.370815634727478,
      "learning_rate": 5e-05,
      "loss": 1.1584,
      "step": 2540
    },
    {
      "epoch": 3.2164556962025315,
      "grad_norm": 1.314853310585022,
      "learning_rate": 5e-05,
      "loss": 1.05,
      "step": 2541
    },
    {
      "epoch": 3.2177215189873416,
      "grad_norm": 1.3186980485916138,
      "learning_rate": 5e-05,
      "loss": 1.0945,
      "step": 2542
    },
    {
      "epoch": 3.2189873417721517,
      "grad_norm": 1.3687065839767456,
      "learning_rate": 5e-05,
      "loss": 1.0954,
      "step": 2543
    },
    {
      "epoch": 3.220253164556962,
      "grad_norm": 1.3019334077835083,
      "learning_rate": 5e-05,
      "loss": 1.1231,
      "step": 2544
    },
    {
      "epoch": 3.221518987341772,
      "grad_norm": 1.3292534351348877,
      "learning_rate": 5e-05,
      "loss": 1.0693,
      "step": 2545
    },
    {
      "epoch": 3.222784810126582,
      "grad_norm": 1.3189836740493774,
      "learning_rate": 5e-05,
      "loss": 1.1028,
      "step": 2546
    },
    {
      "epoch": 3.2240506329113923,
      "grad_norm": 1.3328781127929688,
      "learning_rate": 5e-05,
      "loss": 1.1202,
      "step": 2547
    },
    {
      "epoch": 3.2253164556962024,
      "grad_norm": 1.303240180015564,
      "learning_rate": 5e-05,
      "loss": 1.0913,
      "step": 2548
    },
    {
      "epoch": 3.2265822784810125,
      "grad_norm": 1.2971508502960205,
      "learning_rate": 5e-05,
      "loss": 1.0972,
      "step": 2549
    },
    {
      "epoch": 3.2278481012658227,
      "grad_norm": 1.3560173511505127,
      "learning_rate": 5e-05,
      "loss": 1.1806,
      "step": 2550
    },
    {
      "epoch": 3.229113924050633,
      "grad_norm": 1.3269139528274536,
      "learning_rate": 5e-05,
      "loss": 1.1322,
      "step": 2551
    },
    {
      "epoch": 3.230379746835443,
      "grad_norm": 1.3061418533325195,
      "learning_rate": 5e-05,
      "loss": 1.087,
      "step": 2552
    },
    {
      "epoch": 3.231645569620253,
      "grad_norm": 1.342576026916504,
      "learning_rate": 5e-05,
      "loss": 1.1134,
      "step": 2553
    },
    {
      "epoch": 3.232911392405063,
      "grad_norm": 1.314704418182373,
      "learning_rate": 5e-05,
      "loss": 1.1039,
      "step": 2554
    },
    {
      "epoch": 3.2341772151898733,
      "grad_norm": 1.3141475915908813,
      "learning_rate": 5e-05,
      "loss": 1.0893,
      "step": 2555
    },
    {
      "epoch": 3.2354430379746835,
      "grad_norm": 1.323584794998169,
      "learning_rate": 5e-05,
      "loss": 1.1163,
      "step": 2556
    },
    {
      "epoch": 3.2367088607594936,
      "grad_norm": 1.2860747575759888,
      "learning_rate": 5e-05,
      "loss": 1.0518,
      "step": 2557
    },
    {
      "epoch": 3.2379746835443037,
      "grad_norm": 1.3374683856964111,
      "learning_rate": 5e-05,
      "loss": 1.1612,
      "step": 2558
    },
    {
      "epoch": 3.239240506329114,
      "grad_norm": 1.3542909622192383,
      "learning_rate": 5e-05,
      "loss": 1.1061,
      "step": 2559
    },
    {
      "epoch": 3.240506329113924,
      "grad_norm": 1.3308688402175903,
      "learning_rate": 5e-05,
      "loss": 1.1578,
      "step": 2560
    },
    {
      "epoch": 3.241772151898734,
      "grad_norm": 1.3191255331039429,
      "learning_rate": 5e-05,
      "loss": 1.109,
      "step": 2561
    },
    {
      "epoch": 3.2430379746835443,
      "grad_norm": 1.325626254081726,
      "learning_rate": 5e-05,
      "loss": 1.1586,
      "step": 2562
    },
    {
      "epoch": 3.2443037974683544,
      "grad_norm": 1.3158923387527466,
      "learning_rate": 5e-05,
      "loss": 1.1299,
      "step": 2563
    },
    {
      "epoch": 3.2455696202531645,
      "grad_norm": 1.2947803735733032,
      "learning_rate": 5e-05,
      "loss": 1.0949,
      "step": 2564
    },
    {
      "epoch": 3.2468354430379747,
      "grad_norm": 1.31446373462677,
      "learning_rate": 5e-05,
      "loss": 1.0808,
      "step": 2565
    },
    {
      "epoch": 3.248101265822785,
      "grad_norm": 1.2982417345046997,
      "learning_rate": 5e-05,
      "loss": 1.102,
      "step": 2566
    },
    {
      "epoch": 3.249367088607595,
      "grad_norm": 1.3284748792648315,
      "learning_rate": 5e-05,
      "loss": 1.1552,
      "step": 2567
    },
    {
      "epoch": 3.250632911392405,
      "grad_norm": 1.3681899309158325,
      "learning_rate": 5e-05,
      "loss": 1.1629,
      "step": 2568
    },
    {
      "epoch": 3.251898734177215,
      "grad_norm": 1.297317624092102,
      "learning_rate": 5e-05,
      "loss": 1.0191,
      "step": 2569
    },
    {
      "epoch": 3.2531645569620253,
      "grad_norm": 1.3718276023864746,
      "learning_rate": 5e-05,
      "loss": 1.1114,
      "step": 2570
    },
    {
      "epoch": 3.2544303797468355,
      "grad_norm": 1.3093254566192627,
      "learning_rate": 5e-05,
      "loss": 1.1092,
      "step": 2571
    },
    {
      "epoch": 3.2556962025316456,
      "grad_norm": 1.2612037658691406,
      "learning_rate": 5e-05,
      "loss": 1.0415,
      "step": 2572
    },
    {
      "epoch": 3.2569620253164557,
      "grad_norm": 1.3245141506195068,
      "learning_rate": 5e-05,
      "loss": 1.1331,
      "step": 2573
    },
    {
      "epoch": 3.258227848101266,
      "grad_norm": 1.3106037378311157,
      "learning_rate": 5e-05,
      "loss": 1.0787,
      "step": 2574
    },
    {
      "epoch": 3.259493670886076,
      "grad_norm": 1.3310327529907227,
      "learning_rate": 5e-05,
      "loss": 1.135,
      "step": 2575
    },
    {
      "epoch": 3.260759493670886,
      "grad_norm": 1.2794822454452515,
      "learning_rate": 5e-05,
      "loss": 1.0038,
      "step": 2576
    },
    {
      "epoch": 3.2620253164556963,
      "grad_norm": 1.2874144315719604,
      "learning_rate": 5e-05,
      "loss": 1.0458,
      "step": 2577
    },
    {
      "epoch": 3.2632911392405064,
      "grad_norm": 1.328513264656067,
      "learning_rate": 5e-05,
      "loss": 1.1215,
      "step": 2578
    },
    {
      "epoch": 3.2645569620253165,
      "grad_norm": 1.3306704759597778,
      "learning_rate": 5e-05,
      "loss": 1.0759,
      "step": 2579
    },
    {
      "epoch": 3.2658227848101267,
      "grad_norm": 1.2866655588150024,
      "learning_rate": 5e-05,
      "loss": 1.0853,
      "step": 2580
    },
    {
      "epoch": 3.267088607594937,
      "grad_norm": 1.2961456775665283,
      "learning_rate": 5e-05,
      "loss": 1.0779,
      "step": 2581
    },
    {
      "epoch": 3.268354430379747,
      "grad_norm": 1.3285115957260132,
      "learning_rate": 5e-05,
      "loss": 1.1678,
      "step": 2582
    },
    {
      "epoch": 3.269620253164557,
      "grad_norm": 1.2868740558624268,
      "learning_rate": 5e-05,
      "loss": 1.0693,
      "step": 2583
    },
    {
      "epoch": 3.270886075949367,
      "grad_norm": 1.3283592462539673,
      "learning_rate": 5e-05,
      "loss": 1.1885,
      "step": 2584
    },
    {
      "epoch": 3.2721518987341773,
      "grad_norm": 1.3765276670455933,
      "learning_rate": 5e-05,
      "loss": 1.1169,
      "step": 2585
    },
    {
      "epoch": 3.2734177215189875,
      "grad_norm": 1.319973111152649,
      "learning_rate": 5e-05,
      "loss": 1.075,
      "step": 2586
    },
    {
      "epoch": 3.2746835443037976,
      "grad_norm": 1.3430848121643066,
      "learning_rate": 5e-05,
      "loss": 1.1128,
      "step": 2587
    },
    {
      "epoch": 3.2759493670886077,
      "grad_norm": 1.307713270187378,
      "learning_rate": 5e-05,
      "loss": 1.0817,
      "step": 2588
    },
    {
      "epoch": 3.277215189873418,
      "grad_norm": 1.3924634456634521,
      "learning_rate": 5e-05,
      "loss": 1.1892,
      "step": 2589
    },
    {
      "epoch": 3.278481012658228,
      "grad_norm": 1.2709364891052246,
      "learning_rate": 5e-05,
      "loss": 1.077,
      "step": 2590
    },
    {
      "epoch": 3.279746835443038,
      "grad_norm": 1.343559980392456,
      "learning_rate": 5e-05,
      "loss": 1.0265,
      "step": 2591
    },
    {
      "epoch": 3.2810126582278483,
      "grad_norm": 1.3144705295562744,
      "learning_rate": 5e-05,
      "loss": 1.0929,
      "step": 2592
    },
    {
      "epoch": 3.2822784810126584,
      "grad_norm": 1.3060815334320068,
      "learning_rate": 5e-05,
      "loss": 1.114,
      "step": 2593
    },
    {
      "epoch": 3.2835443037974685,
      "grad_norm": 1.28548264503479,
      "learning_rate": 5e-05,
      "loss": 1.1168,
      "step": 2594
    },
    {
      "epoch": 3.2848101265822787,
      "grad_norm": 1.2997033596038818,
      "learning_rate": 5e-05,
      "loss": 1.0893,
      "step": 2595
    },
    {
      "epoch": 3.286075949367089,
      "grad_norm": 1.3682094812393188,
      "learning_rate": 5e-05,
      "loss": 1.1468,
      "step": 2596
    },
    {
      "epoch": 3.2873417721518985,
      "grad_norm": 1.3461147546768188,
      "learning_rate": 5e-05,
      "loss": 1.1521,
      "step": 2597
    },
    {
      "epoch": 3.2886075949367086,
      "grad_norm": 1.325553059577942,
      "learning_rate": 5e-05,
      "loss": 1.0792,
      "step": 2598
    },
    {
      "epoch": 3.2898734177215188,
      "grad_norm": 1.3244155645370483,
      "learning_rate": 5e-05,
      "loss": 1.1499,
      "step": 2599
    },
    {
      "epoch": 3.291139240506329,
      "grad_norm": 1.319985270500183,
      "learning_rate": 5e-05,
      "loss": 1.068,
      "step": 2600
    },
    {
      "epoch": 3.292405063291139,
      "grad_norm": 1.3578263521194458,
      "learning_rate": 5e-05,
      "loss": 1.1097,
      "step": 2601
    },
    {
      "epoch": 3.293670886075949,
      "grad_norm": 1.3076800107955933,
      "learning_rate": 5e-05,
      "loss": 1.0767,
      "step": 2602
    },
    {
      "epoch": 3.2949367088607593,
      "grad_norm": 1.2837501764297485,
      "learning_rate": 5e-05,
      "loss": 1.0152,
      "step": 2603
    },
    {
      "epoch": 3.2962025316455694,
      "grad_norm": 1.3215781450271606,
      "learning_rate": 5e-05,
      "loss": 1.0342,
      "step": 2604
    },
    {
      "epoch": 3.2974683544303796,
      "grad_norm": 1.2971745729446411,
      "learning_rate": 5e-05,
      "loss": 1.0484,
      "step": 2605
    },
    {
      "epoch": 3.2987341772151897,
      "grad_norm": 1.3416433334350586,
      "learning_rate": 5e-05,
      "loss": 1.0959,
      "step": 2606
    },
    {
      "epoch": 3.3,
      "grad_norm": 1.2915798425674438,
      "learning_rate": 5e-05,
      "loss": 1.0583,
      "step": 2607
    },
    {
      "epoch": 3.30126582278481,
      "grad_norm": 1.3325412273406982,
      "learning_rate": 5e-05,
      "loss": 1.1299,
      "step": 2608
    },
    {
      "epoch": 3.30253164556962,
      "grad_norm": 1.2956175804138184,
      "learning_rate": 5e-05,
      "loss": 1.0583,
      "step": 2609
    },
    {
      "epoch": 3.3037974683544302,
      "grad_norm": 1.268230676651001,
      "learning_rate": 5e-05,
      "loss": 1.1213,
      "step": 2610
    },
    {
      "epoch": 3.3050632911392404,
      "grad_norm": 1.3238937854766846,
      "learning_rate": 5e-05,
      "loss": 1.1211,
      "step": 2611
    },
    {
      "epoch": 3.3063291139240505,
      "grad_norm": 1.3169152736663818,
      "learning_rate": 5e-05,
      "loss": 1.1312,
      "step": 2612
    },
    {
      "epoch": 3.3075949367088606,
      "grad_norm": 1.312941074371338,
      "learning_rate": 5e-05,
      "loss": 1.1646,
      "step": 2613
    },
    {
      "epoch": 3.3088607594936708,
      "grad_norm": 1.3110061883926392,
      "learning_rate": 5e-05,
      "loss": 1.051,
      "step": 2614
    },
    {
      "epoch": 3.310126582278481,
      "grad_norm": 1.301267385482788,
      "learning_rate": 5e-05,
      "loss": 1.0952,
      "step": 2615
    },
    {
      "epoch": 3.311392405063291,
      "grad_norm": 1.2586814165115356,
      "learning_rate": 5e-05,
      "loss": 1.0442,
      "step": 2616
    },
    {
      "epoch": 3.312658227848101,
      "grad_norm": 1.3379625082015991,
      "learning_rate": 5e-05,
      "loss": 1.1162,
      "step": 2617
    },
    {
      "epoch": 3.3139240506329113,
      "grad_norm": 1.3163267374038696,
      "learning_rate": 5e-05,
      "loss": 1.0617,
      "step": 2618
    },
    {
      "epoch": 3.3151898734177214,
      "grad_norm": 1.3030531406402588,
      "learning_rate": 5e-05,
      "loss": 1.0538,
      "step": 2619
    },
    {
      "epoch": 3.3164556962025316,
      "grad_norm": 1.389704942703247,
      "learning_rate": 5e-05,
      "loss": 1.2119,
      "step": 2620
    },
    {
      "epoch": 3.3177215189873417,
      "grad_norm": 1.332107424736023,
      "learning_rate": 5e-05,
      "loss": 1.119,
      "step": 2621
    },
    {
      "epoch": 3.318987341772152,
      "grad_norm": 1.293765664100647,
      "learning_rate": 5e-05,
      "loss": 1.0708,
      "step": 2622
    },
    {
      "epoch": 3.320253164556962,
      "grad_norm": 1.350581169128418,
      "learning_rate": 5e-05,
      "loss": 1.1009,
      "step": 2623
    },
    {
      "epoch": 3.321518987341772,
      "grad_norm": 1.3461805582046509,
      "learning_rate": 5e-05,
      "loss": 1.0572,
      "step": 2624
    },
    {
      "epoch": 3.3227848101265822,
      "grad_norm": 1.3017491102218628,
      "learning_rate": 5e-05,
      "loss": 1.1118,
      "step": 2625
    },
    {
      "epoch": 3.3240506329113924,
      "grad_norm": 1.3527371883392334,
      "learning_rate": 5e-05,
      "loss": 1.1291,
      "step": 2626
    },
    {
      "epoch": 3.3253164556962025,
      "grad_norm": 1.3408730030059814,
      "learning_rate": 5e-05,
      "loss": 1.1584,
      "step": 2627
    },
    {
      "epoch": 3.3265822784810126,
      "grad_norm": 1.2905611991882324,
      "learning_rate": 5e-05,
      "loss": 1.0968,
      "step": 2628
    },
    {
      "epoch": 3.3278481012658228,
      "grad_norm": 1.3188585042953491,
      "learning_rate": 5e-05,
      "loss": 1.0793,
      "step": 2629
    },
    {
      "epoch": 3.329113924050633,
      "grad_norm": 1.3789242506027222,
      "learning_rate": 5e-05,
      "loss": 1.1861,
      "step": 2630
    },
    {
      "epoch": 3.330379746835443,
      "grad_norm": 1.3769725561141968,
      "learning_rate": 5e-05,
      "loss": 1.213,
      "step": 2631
    },
    {
      "epoch": 3.331645569620253,
      "grad_norm": 1.3060742616653442,
      "learning_rate": 5e-05,
      "loss": 1.0323,
      "step": 2632
    },
    {
      "epoch": 3.3329113924050633,
      "grad_norm": 1.3353077173233032,
      "learning_rate": 5e-05,
      "loss": 1.1091,
      "step": 2633
    },
    {
      "epoch": 3.3341772151898734,
      "grad_norm": 1.2865114212036133,
      "learning_rate": 5e-05,
      "loss": 1.1023,
      "step": 2634
    },
    {
      "epoch": 3.3354430379746836,
      "grad_norm": 1.301270604133606,
      "learning_rate": 5e-05,
      "loss": 1.1549,
      "step": 2635
    },
    {
      "epoch": 3.3367088607594937,
      "grad_norm": 1.3631627559661865,
      "learning_rate": 5e-05,
      "loss": 1.0992,
      "step": 2636
    },
    {
      "epoch": 3.337974683544304,
      "grad_norm": 1.3406301736831665,
      "learning_rate": 5e-05,
      "loss": 1.108,
      "step": 2637
    },
    {
      "epoch": 3.339240506329114,
      "grad_norm": 1.3920763731002808,
      "learning_rate": 5e-05,
      "loss": 1.1874,
      "step": 2638
    },
    {
      "epoch": 3.340506329113924,
      "grad_norm": 1.322698712348938,
      "learning_rate": 5e-05,
      "loss": 1.0992,
      "step": 2639
    },
    {
      "epoch": 3.3417721518987342,
      "grad_norm": 1.2469710111618042,
      "learning_rate": 5e-05,
      "loss": 0.9969,
      "step": 2640
    },
    {
      "epoch": 3.3430379746835444,
      "grad_norm": 1.363431453704834,
      "learning_rate": 5e-05,
      "loss": 1.1591,
      "step": 2641
    },
    {
      "epoch": 3.3443037974683545,
      "grad_norm": 1.3329124450683594,
      "learning_rate": 5e-05,
      "loss": 1.1151,
      "step": 2642
    },
    {
      "epoch": 3.3455696202531646,
      "grad_norm": 1.2606887817382812,
      "learning_rate": 5e-05,
      "loss": 1.0274,
      "step": 2643
    },
    {
      "epoch": 3.3468354430379748,
      "grad_norm": 1.2946841716766357,
      "learning_rate": 5e-05,
      "loss": 1.0622,
      "step": 2644
    },
    {
      "epoch": 3.348101265822785,
      "grad_norm": 1.325459361076355,
      "learning_rate": 5e-05,
      "loss": 1.0875,
      "step": 2645
    },
    {
      "epoch": 3.349367088607595,
      "grad_norm": 1.302453875541687,
      "learning_rate": 5e-05,
      "loss": 1.1546,
      "step": 2646
    },
    {
      "epoch": 3.350632911392405,
      "grad_norm": 1.3321505784988403,
      "learning_rate": 5e-05,
      "loss": 1.1239,
      "step": 2647
    },
    {
      "epoch": 3.3518987341772153,
      "grad_norm": 1.3046412467956543,
      "learning_rate": 5e-05,
      "loss": 1.0614,
      "step": 2648
    },
    {
      "epoch": 3.3531645569620254,
      "grad_norm": 1.330203890800476,
      "learning_rate": 5e-05,
      "loss": 1.1094,
      "step": 2649
    },
    {
      "epoch": 3.3544303797468356,
      "grad_norm": 1.3106144666671753,
      "learning_rate": 5e-05,
      "loss": 1.0332,
      "step": 2650
    },
    {
      "epoch": 3.3556962025316457,
      "grad_norm": 1.3070214986801147,
      "learning_rate": 5e-05,
      "loss": 1.0387,
      "step": 2651
    },
    {
      "epoch": 3.356962025316456,
      "grad_norm": 1.359225869178772,
      "learning_rate": 5e-05,
      "loss": 1.1424,
      "step": 2652
    },
    {
      "epoch": 3.358227848101266,
      "grad_norm": 1.334721326828003,
      "learning_rate": 5e-05,
      "loss": 1.1001,
      "step": 2653
    },
    {
      "epoch": 3.359493670886076,
      "grad_norm": 1.3673925399780273,
      "learning_rate": 5e-05,
      "loss": 1.1208,
      "step": 2654
    },
    {
      "epoch": 3.3607594936708862,
      "grad_norm": 1.347304105758667,
      "learning_rate": 5e-05,
      "loss": 1.114,
      "step": 2655
    },
    {
      "epoch": 3.3620253164556964,
      "grad_norm": 1.2973498106002808,
      "learning_rate": 5e-05,
      "loss": 1.0854,
      "step": 2656
    },
    {
      "epoch": 3.3632911392405065,
      "grad_norm": 1.343043565750122,
      "learning_rate": 5e-05,
      "loss": 1.0973,
      "step": 2657
    },
    {
      "epoch": 3.3645569620253166,
      "grad_norm": 1.3252077102661133,
      "learning_rate": 5e-05,
      "loss": 1.0543,
      "step": 2658
    },
    {
      "epoch": 3.3658227848101268,
      "grad_norm": 1.3409919738769531,
      "learning_rate": 5e-05,
      "loss": 1.1563,
      "step": 2659
    },
    {
      "epoch": 3.367088607594937,
      "grad_norm": 1.2622792720794678,
      "learning_rate": 5e-05,
      "loss": 1.0027,
      "step": 2660
    },
    {
      "epoch": 3.368354430379747,
      "grad_norm": 1.3288421630859375,
      "learning_rate": 5e-05,
      "loss": 1.0375,
      "step": 2661
    },
    {
      "epoch": 3.369620253164557,
      "grad_norm": 1.3983995914459229,
      "learning_rate": 5e-05,
      "loss": 1.1032,
      "step": 2662
    },
    {
      "epoch": 3.3708860759493673,
      "grad_norm": 1.2575976848602295,
      "learning_rate": 5e-05,
      "loss": 1.0367,
      "step": 2663
    },
    {
      "epoch": 3.3721518987341774,
      "grad_norm": 1.3083921670913696,
      "learning_rate": 5e-05,
      "loss": 1.0737,
      "step": 2664
    },
    {
      "epoch": 3.3734177215189876,
      "grad_norm": 1.3044596910476685,
      "learning_rate": 5e-05,
      "loss": 1.0725,
      "step": 2665
    },
    {
      "epoch": 3.3746835443037977,
      "grad_norm": 1.3467589616775513,
      "learning_rate": 5e-05,
      "loss": 1.1242,
      "step": 2666
    },
    {
      "epoch": 3.375949367088608,
      "grad_norm": 1.403447151184082,
      "learning_rate": 5e-05,
      "loss": 1.1322,
      "step": 2667
    },
    {
      "epoch": 3.377215189873418,
      "grad_norm": 1.2851048707962036,
      "learning_rate": 5e-05,
      "loss": 1.1047,
      "step": 2668
    },
    {
      "epoch": 3.378481012658228,
      "grad_norm": 1.345271348953247,
      "learning_rate": 5e-05,
      "loss": 1.099,
      "step": 2669
    },
    {
      "epoch": 3.379746835443038,
      "grad_norm": 1.3242383003234863,
      "learning_rate": 5e-05,
      "loss": 1.0411,
      "step": 2670
    },
    {
      "epoch": 3.381012658227848,
      "grad_norm": 1.3073018789291382,
      "learning_rate": 5e-05,
      "loss": 1.1286,
      "step": 2671
    },
    {
      "epoch": 3.382278481012658,
      "grad_norm": 1.3473947048187256,
      "learning_rate": 5e-05,
      "loss": 1.0262,
      "step": 2672
    },
    {
      "epoch": 3.383544303797468,
      "grad_norm": 1.3286207914352417,
      "learning_rate": 5e-05,
      "loss": 1.0725,
      "step": 2673
    },
    {
      "epoch": 3.3848101265822783,
      "grad_norm": 1.3191657066345215,
      "learning_rate": 5e-05,
      "loss": 1.1274,
      "step": 2674
    },
    {
      "epoch": 3.3860759493670884,
      "grad_norm": 1.257765769958496,
      "learning_rate": 5e-05,
      "loss": 1.0552,
      "step": 2675
    },
    {
      "epoch": 3.3873417721518986,
      "grad_norm": 1.3034121990203857,
      "learning_rate": 5e-05,
      "loss": 1.0859,
      "step": 2676
    },
    {
      "epoch": 3.3886075949367087,
      "grad_norm": 1.3231816291809082,
      "learning_rate": 5e-05,
      "loss": 1.0588,
      "step": 2677
    },
    {
      "epoch": 3.389873417721519,
      "grad_norm": 1.3351093530654907,
      "learning_rate": 5e-05,
      "loss": 1.0861,
      "step": 2678
    },
    {
      "epoch": 3.391139240506329,
      "grad_norm": 1.368384599685669,
      "learning_rate": 5e-05,
      "loss": 1.1446,
      "step": 2679
    },
    {
      "epoch": 3.392405063291139,
      "grad_norm": 1.3228965997695923,
      "learning_rate": 5e-05,
      "loss": 1.0488,
      "step": 2680
    },
    {
      "epoch": 3.3936708860759492,
      "grad_norm": 1.2765498161315918,
      "learning_rate": 5e-05,
      "loss": 1.0387,
      "step": 2681
    },
    {
      "epoch": 3.3949367088607594,
      "grad_norm": 1.3514161109924316,
      "learning_rate": 5e-05,
      "loss": 1.1461,
      "step": 2682
    },
    {
      "epoch": 3.3962025316455695,
      "grad_norm": 1.36565363407135,
      "learning_rate": 5e-05,
      "loss": 1.1147,
      "step": 2683
    },
    {
      "epoch": 3.3974683544303796,
      "grad_norm": 1.3064292669296265,
      "learning_rate": 5e-05,
      "loss": 1.075,
      "step": 2684
    },
    {
      "epoch": 3.3987341772151898,
      "grad_norm": 1.2960835695266724,
      "learning_rate": 5e-05,
      "loss": 1.0001,
      "step": 2685
    },
    {
      "epoch": 3.4,
      "grad_norm": 1.2973735332489014,
      "learning_rate": 5e-05,
      "loss": 1.0721,
      "step": 2686
    },
    {
      "epoch": 3.40126582278481,
      "grad_norm": 1.310604214668274,
      "learning_rate": 5e-05,
      "loss": 1.0469,
      "step": 2687
    },
    {
      "epoch": 3.40253164556962,
      "grad_norm": 1.295100450515747,
      "learning_rate": 5e-05,
      "loss": 1.0572,
      "step": 2688
    },
    {
      "epoch": 3.4037974683544303,
      "grad_norm": 1.3029558658599854,
      "learning_rate": 5e-05,
      "loss": 1.074,
      "step": 2689
    },
    {
      "epoch": 3.4050632911392404,
      "grad_norm": 1.3025782108306885,
      "learning_rate": 5e-05,
      "loss": 1.0021,
      "step": 2690
    },
    {
      "epoch": 3.4063291139240506,
      "grad_norm": 1.3587803840637207,
      "learning_rate": 5e-05,
      "loss": 1.1605,
      "step": 2691
    },
    {
      "epoch": 3.4075949367088607,
      "grad_norm": 1.3295584917068481,
      "learning_rate": 5e-05,
      "loss": 1.1061,
      "step": 2692
    },
    {
      "epoch": 3.408860759493671,
      "grad_norm": 1.396026611328125,
      "learning_rate": 5e-05,
      "loss": 1.1349,
      "step": 2693
    },
    {
      "epoch": 3.410126582278481,
      "grad_norm": 1.3150273561477661,
      "learning_rate": 5e-05,
      "loss": 1.1146,
      "step": 2694
    },
    {
      "epoch": 3.411392405063291,
      "grad_norm": 1.2679656744003296,
      "learning_rate": 5e-05,
      "loss": 1.0443,
      "step": 2695
    },
    {
      "epoch": 3.4126582278481012,
      "grad_norm": 1.3205229043960571,
      "learning_rate": 5e-05,
      "loss": 1.0362,
      "step": 2696
    },
    {
      "epoch": 3.4139240506329114,
      "grad_norm": 1.3254058361053467,
      "learning_rate": 5e-05,
      "loss": 1.1025,
      "step": 2697
    },
    {
      "epoch": 3.4151898734177215,
      "grad_norm": 1.320238471031189,
      "learning_rate": 5e-05,
      "loss": 1.1028,
      "step": 2698
    },
    {
      "epoch": 3.4164556962025316,
      "grad_norm": 1.3306257724761963,
      "learning_rate": 5e-05,
      "loss": 1.0925,
      "step": 2699
    },
    {
      "epoch": 3.4177215189873418,
      "grad_norm": 1.3127793073654175,
      "learning_rate": 5e-05,
      "loss": 1.0595,
      "step": 2700
    },
    {
      "epoch": 3.418987341772152,
      "grad_norm": 1.3222393989562988,
      "learning_rate": 5e-05,
      "loss": 1.0788,
      "step": 2701
    },
    {
      "epoch": 3.420253164556962,
      "grad_norm": 1.3050851821899414,
      "learning_rate": 5e-05,
      "loss": 1.0313,
      "step": 2702
    },
    {
      "epoch": 3.421518987341772,
      "grad_norm": 1.357980489730835,
      "learning_rate": 5e-05,
      "loss": 1.1365,
      "step": 2703
    },
    {
      "epoch": 3.4227848101265823,
      "grad_norm": 1.3460286855697632,
      "learning_rate": 5e-05,
      "loss": 1.1295,
      "step": 2704
    },
    {
      "epoch": 3.4240506329113924,
      "grad_norm": 1.3072805404663086,
      "learning_rate": 5e-05,
      "loss": 1.0759,
      "step": 2705
    },
    {
      "epoch": 3.4253164556962026,
      "grad_norm": 1.331416368484497,
      "learning_rate": 5e-05,
      "loss": 1.1503,
      "step": 2706
    },
    {
      "epoch": 3.4265822784810127,
      "grad_norm": 1.3036856651306152,
      "learning_rate": 5e-05,
      "loss": 1.0602,
      "step": 2707
    },
    {
      "epoch": 3.427848101265823,
      "grad_norm": 1.283145546913147,
      "learning_rate": 5e-05,
      "loss": 1.0239,
      "step": 2708
    },
    {
      "epoch": 3.429113924050633,
      "grad_norm": 1.328653335571289,
      "learning_rate": 5e-05,
      "loss": 1.0754,
      "step": 2709
    },
    {
      "epoch": 3.430379746835443,
      "grad_norm": 1.343206524848938,
      "learning_rate": 5e-05,
      "loss": 1.0671,
      "step": 2710
    },
    {
      "epoch": 3.4316455696202532,
      "grad_norm": 1.3259729146957397,
      "learning_rate": 5e-05,
      "loss": 1.1106,
      "step": 2711
    },
    {
      "epoch": 3.4329113924050634,
      "grad_norm": 1.354021668434143,
      "learning_rate": 5e-05,
      "loss": 1.1305,
      "step": 2712
    },
    {
      "epoch": 3.4341772151898735,
      "grad_norm": 1.3217805624008179,
      "learning_rate": 5e-05,
      "loss": 1.0876,
      "step": 2713
    },
    {
      "epoch": 3.4354430379746836,
      "grad_norm": 1.3230376243591309,
      "learning_rate": 5e-05,
      "loss": 1.1055,
      "step": 2714
    },
    {
      "epoch": 3.4367088607594938,
      "grad_norm": 1.293029546737671,
      "learning_rate": 5e-05,
      "loss": 1.0426,
      "step": 2715
    },
    {
      "epoch": 3.437974683544304,
      "grad_norm": 1.3320777416229248,
      "learning_rate": 5e-05,
      "loss": 1.0397,
      "step": 2716
    },
    {
      "epoch": 3.439240506329114,
      "grad_norm": 1.3117502927780151,
      "learning_rate": 5e-05,
      "loss": 1.0541,
      "step": 2717
    },
    {
      "epoch": 3.440506329113924,
      "grad_norm": 1.3491578102111816,
      "learning_rate": 5e-05,
      "loss": 1.1212,
      "step": 2718
    },
    {
      "epoch": 3.4417721518987343,
      "grad_norm": 1.3360337018966675,
      "learning_rate": 5e-05,
      "loss": 1.072,
      "step": 2719
    },
    {
      "epoch": 3.4430379746835444,
      "grad_norm": 1.3184784650802612,
      "learning_rate": 5e-05,
      "loss": 1.0893,
      "step": 2720
    },
    {
      "epoch": 3.4443037974683546,
      "grad_norm": 1.3377466201782227,
      "learning_rate": 5e-05,
      "loss": 1.1005,
      "step": 2721
    },
    {
      "epoch": 3.4455696202531647,
      "grad_norm": 1.3219186067581177,
      "learning_rate": 5e-05,
      "loss": 1.0426,
      "step": 2722
    },
    {
      "epoch": 3.446835443037975,
      "grad_norm": 1.3029850721359253,
      "learning_rate": 5e-05,
      "loss": 1.1298,
      "step": 2723
    },
    {
      "epoch": 3.448101265822785,
      "grad_norm": 1.3429570198059082,
      "learning_rate": 5e-05,
      "loss": 1.107,
      "step": 2724
    },
    {
      "epoch": 3.449367088607595,
      "grad_norm": 1.3318451642990112,
      "learning_rate": 5e-05,
      "loss": 1.0925,
      "step": 2725
    },
    {
      "epoch": 3.4506329113924052,
      "grad_norm": 1.2694264650344849,
      "learning_rate": 5e-05,
      "loss": 1.0426,
      "step": 2726
    },
    {
      "epoch": 3.4518987341772154,
      "grad_norm": 1.3208904266357422,
      "learning_rate": 5e-05,
      "loss": 1.0909,
      "step": 2727
    },
    {
      "epoch": 3.453164556962025,
      "grad_norm": 1.3629308938980103,
      "learning_rate": 5e-05,
      "loss": 1.0908,
      "step": 2728
    },
    {
      "epoch": 3.454430379746835,
      "grad_norm": 1.3103681802749634,
      "learning_rate": 5e-05,
      "loss": 1.103,
      "step": 2729
    },
    {
      "epoch": 3.4556962025316453,
      "grad_norm": 1.3297501802444458,
      "learning_rate": 5e-05,
      "loss": 1.1094,
      "step": 2730
    },
    {
      "epoch": 3.4569620253164555,
      "grad_norm": 1.328183650970459,
      "learning_rate": 5e-05,
      "loss": 1.0476,
      "step": 2731
    },
    {
      "epoch": 3.4582278481012656,
      "grad_norm": 1.3400170803070068,
      "learning_rate": 5e-05,
      "loss": 1.1194,
      "step": 2732
    },
    {
      "epoch": 3.4594936708860757,
      "grad_norm": 1.3237872123718262,
      "learning_rate": 5e-05,
      "loss": 1.0615,
      "step": 2733
    },
    {
      "epoch": 3.460759493670886,
      "grad_norm": 1.3510633707046509,
      "learning_rate": 5e-05,
      "loss": 1.1072,
      "step": 2734
    },
    {
      "epoch": 3.462025316455696,
      "grad_norm": 1.2964329719543457,
      "learning_rate": 5e-05,
      "loss": 1.131,
      "step": 2735
    },
    {
      "epoch": 3.463291139240506,
      "grad_norm": 1.3348088264465332,
      "learning_rate": 5e-05,
      "loss": 1.1116,
      "step": 2736
    },
    {
      "epoch": 3.4645569620253163,
      "grad_norm": 1.3345260620117188,
      "learning_rate": 5e-05,
      "loss": 1.1289,
      "step": 2737
    },
    {
      "epoch": 3.4658227848101264,
      "grad_norm": 1.3018399477005005,
      "learning_rate": 5e-05,
      "loss": 1.0968,
      "step": 2738
    },
    {
      "epoch": 3.4670886075949365,
      "grad_norm": 1.3624413013458252,
      "learning_rate": 5e-05,
      "loss": 1.0974,
      "step": 2739
    },
    {
      "epoch": 3.4683544303797467,
      "grad_norm": 1.354160189628601,
      "learning_rate": 5e-05,
      "loss": 1.0963,
      "step": 2740
    },
    {
      "epoch": 3.469620253164557,
      "grad_norm": 1.2848361730575562,
      "learning_rate": 5e-05,
      "loss": 1.0047,
      "step": 2741
    },
    {
      "epoch": 3.470886075949367,
      "grad_norm": 1.3145543336868286,
      "learning_rate": 5e-05,
      "loss": 1.0636,
      "step": 2742
    },
    {
      "epoch": 3.472151898734177,
      "grad_norm": 1.3578436374664307,
      "learning_rate": 5e-05,
      "loss": 1.0964,
      "step": 2743
    },
    {
      "epoch": 3.473417721518987,
      "grad_norm": 1.323566198348999,
      "learning_rate": 5e-05,
      "loss": 1.0614,
      "step": 2744
    },
    {
      "epoch": 3.4746835443037973,
      "grad_norm": 1.3834129571914673,
      "learning_rate": 5e-05,
      "loss": 1.1175,
      "step": 2745
    },
    {
      "epoch": 3.4759493670886075,
      "grad_norm": 1.3501979112625122,
      "learning_rate": 5e-05,
      "loss": 1.137,
      "step": 2746
    },
    {
      "epoch": 3.4772151898734176,
      "grad_norm": 1.3204622268676758,
      "learning_rate": 5e-05,
      "loss": 1.0607,
      "step": 2747
    },
    {
      "epoch": 3.4784810126582277,
      "grad_norm": 1.306861162185669,
      "learning_rate": 5e-05,
      "loss": 1.098,
      "step": 2748
    },
    {
      "epoch": 3.479746835443038,
      "grad_norm": 1.2694790363311768,
      "learning_rate": 5e-05,
      "loss": 1.0854,
      "step": 2749
    },
    {
      "epoch": 3.481012658227848,
      "grad_norm": 1.3481284379959106,
      "learning_rate": 5e-05,
      "loss": 1.1178,
      "step": 2750
    },
    {
      "epoch": 3.482278481012658,
      "grad_norm": 1.2935057878494263,
      "learning_rate": 5e-05,
      "loss": 1.076,
      "step": 2751
    },
    {
      "epoch": 3.4835443037974683,
      "grad_norm": 1.344171404838562,
      "learning_rate": 5e-05,
      "loss": 1.1179,
      "step": 2752
    },
    {
      "epoch": 3.4848101265822784,
      "grad_norm": 1.248071551322937,
      "learning_rate": 5e-05,
      "loss": 1.0175,
      "step": 2753
    },
    {
      "epoch": 3.4860759493670885,
      "grad_norm": 1.2941129207611084,
      "learning_rate": 5e-05,
      "loss": 1.0301,
      "step": 2754
    },
    {
      "epoch": 3.4873417721518987,
      "grad_norm": 1.3193837404251099,
      "learning_rate": 5e-05,
      "loss": 1.0364,
      "step": 2755
    },
    {
      "epoch": 3.488607594936709,
      "grad_norm": 1.372890830039978,
      "learning_rate": 5e-05,
      "loss": 1.1164,
      "step": 2756
    },
    {
      "epoch": 3.489873417721519,
      "grad_norm": 1.3147709369659424,
      "learning_rate": 5e-05,
      "loss": 1.0808,
      "step": 2757
    },
    {
      "epoch": 3.491139240506329,
      "grad_norm": 1.3032090663909912,
      "learning_rate": 5e-05,
      "loss": 1.0866,
      "step": 2758
    },
    {
      "epoch": 3.492405063291139,
      "grad_norm": 1.3539679050445557,
      "learning_rate": 5e-05,
      "loss": 1.0821,
      "step": 2759
    },
    {
      "epoch": 3.4936708860759493,
      "grad_norm": 1.3291192054748535,
      "learning_rate": 5e-05,
      "loss": 1.0697,
      "step": 2760
    },
    {
      "epoch": 3.4949367088607595,
      "grad_norm": 1.3072209358215332,
      "learning_rate": 5e-05,
      "loss": 1.0875,
      "step": 2761
    },
    {
      "epoch": 3.4962025316455696,
      "grad_norm": 1.3088082075119019,
      "learning_rate": 5e-05,
      "loss": 1.0574,
      "step": 2762
    },
    {
      "epoch": 3.4974683544303797,
      "grad_norm": 1.3128582239151,
      "learning_rate": 5e-05,
      "loss": 1.0786,
      "step": 2763
    },
    {
      "epoch": 3.49873417721519,
      "grad_norm": 1.3184814453125,
      "learning_rate": 5e-05,
      "loss": 1.0544,
      "step": 2764
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.3391813039779663,
      "learning_rate": 5e-05,
      "loss": 1.0766,
      "step": 2765
    },
    {
      "epoch": 3.5,
      "eval_loss": 1.9693291187286377,
      "eval_runtime": 862.7537,
      "eval_samples_per_second": 104.178,
      "eval_steps_per_second": 0.815,
      "step": 2765
    },
    {
      "epoch": 3.50126582278481,
      "grad_norm": 1.362690806388855,
      "learning_rate": 5e-05,
      "loss": 1.1543,
      "step": 2766
    },
    {
      "epoch": 3.5025316455696203,
      "grad_norm": 1.3400272130966187,
      "learning_rate": 5e-05,
      "loss": 1.101,
      "step": 2767
    },
    {
      "epoch": 3.5037974683544304,
      "grad_norm": 1.3146922588348389,
      "learning_rate": 5e-05,
      "loss": 1.0725,
      "step": 2768
    },
    {
      "epoch": 3.5050632911392405,
      "grad_norm": 1.360204815864563,
      "learning_rate": 5e-05,
      "loss": 1.1341,
      "step": 2769
    },
    {
      "epoch": 3.5063291139240507,
      "grad_norm": 1.2625036239624023,
      "learning_rate": 5e-05,
      "loss": 1.0338,
      "step": 2770
    },
    {
      "epoch": 3.507594936708861,
      "grad_norm": 1.328963041305542,
      "learning_rate": 5e-05,
      "loss": 1.0754,
      "step": 2771
    },
    {
      "epoch": 3.508860759493671,
      "grad_norm": 1.3596141338348389,
      "learning_rate": 5e-05,
      "loss": 1.1266,
      "step": 2772
    },
    {
      "epoch": 3.510126582278481,
      "grad_norm": 1.3052774667739868,
      "learning_rate": 5e-05,
      "loss": 1.0402,
      "step": 2773
    },
    {
      "epoch": 3.511392405063291,
      "grad_norm": 1.3263952732086182,
      "learning_rate": 5e-05,
      "loss": 1.0482,
      "step": 2774
    },
    {
      "epoch": 3.5126582278481013,
      "grad_norm": 1.325411319732666,
      "learning_rate": 5e-05,
      "loss": 1.0699,
      "step": 2775
    },
    {
      "epoch": 3.5139240506329115,
      "grad_norm": 1.3396058082580566,
      "learning_rate": 5e-05,
      "loss": 1.1188,
      "step": 2776
    },
    {
      "epoch": 3.5151898734177216,
      "grad_norm": 1.342905879020691,
      "learning_rate": 5e-05,
      "loss": 1.1343,
      "step": 2777
    },
    {
      "epoch": 3.5164556962025317,
      "grad_norm": 1.3545342683792114,
      "learning_rate": 5e-05,
      "loss": 1.0345,
      "step": 2778
    },
    {
      "epoch": 3.517721518987342,
      "grad_norm": 1.3165587186813354,
      "learning_rate": 5e-05,
      "loss": 1.0784,
      "step": 2779
    },
    {
      "epoch": 3.518987341772152,
      "grad_norm": 1.3334441184997559,
      "learning_rate": 5e-05,
      "loss": 1.1173,
      "step": 2780
    },
    {
      "epoch": 3.520253164556962,
      "grad_norm": 1.3538812398910522,
      "learning_rate": 5e-05,
      "loss": 1.0729,
      "step": 2781
    },
    {
      "epoch": 3.5215189873417723,
      "grad_norm": 1.3258531093597412,
      "learning_rate": 5e-05,
      "loss": 1.0308,
      "step": 2782
    },
    {
      "epoch": 3.5227848101265824,
      "grad_norm": 1.3036704063415527,
      "learning_rate": 5e-05,
      "loss": 1.0189,
      "step": 2783
    },
    {
      "epoch": 3.5240506329113925,
      "grad_norm": 1.331984043121338,
      "learning_rate": 5e-05,
      "loss": 1.1119,
      "step": 2784
    },
    {
      "epoch": 3.5253164556962027,
      "grad_norm": 1.3222788572311401,
      "learning_rate": 5e-05,
      "loss": 1.0779,
      "step": 2785
    },
    {
      "epoch": 3.526582278481013,
      "grad_norm": 1.340322732925415,
      "learning_rate": 5e-05,
      "loss": 1.0743,
      "step": 2786
    },
    {
      "epoch": 3.527848101265823,
      "grad_norm": 1.3473299741744995,
      "learning_rate": 5e-05,
      "loss": 1.1191,
      "step": 2787
    },
    {
      "epoch": 3.529113924050633,
      "grad_norm": 1.3215909004211426,
      "learning_rate": 5e-05,
      "loss": 1.0827,
      "step": 2788
    },
    {
      "epoch": 3.530379746835443,
      "grad_norm": 1.2921444177627563,
      "learning_rate": 5e-05,
      "loss": 1.0289,
      "step": 2789
    },
    {
      "epoch": 3.5316455696202533,
      "grad_norm": 1.305281162261963,
      "learning_rate": 5e-05,
      "loss": 1.0653,
      "step": 2790
    },
    {
      "epoch": 3.5329113924050635,
      "grad_norm": 1.317169427871704,
      "learning_rate": 5e-05,
      "loss": 1.034,
      "step": 2791
    },
    {
      "epoch": 3.5341772151898736,
      "grad_norm": 1.3434932231903076,
      "learning_rate": 5e-05,
      "loss": 1.1241,
      "step": 2792
    },
    {
      "epoch": 3.5354430379746837,
      "grad_norm": 1.333956003189087,
      "learning_rate": 5e-05,
      "loss": 1.1197,
      "step": 2793
    },
    {
      "epoch": 3.536708860759494,
      "grad_norm": 1.3448271751403809,
      "learning_rate": 5e-05,
      "loss": 1.1047,
      "step": 2794
    },
    {
      "epoch": 3.537974683544304,
      "grad_norm": 1.3334547281265259,
      "learning_rate": 5e-05,
      "loss": 1.0205,
      "step": 2795
    },
    {
      "epoch": 3.539240506329114,
      "grad_norm": 1.3254201412200928,
      "learning_rate": 5e-05,
      "loss": 1.0776,
      "step": 2796
    },
    {
      "epoch": 3.5405063291139243,
      "grad_norm": 1.3145036697387695,
      "learning_rate": 5e-05,
      "loss": 1.0054,
      "step": 2797
    },
    {
      "epoch": 3.5417721518987344,
      "grad_norm": 1.2936198711395264,
      "learning_rate": 5e-05,
      "loss": 1.0018,
      "step": 2798
    },
    {
      "epoch": 3.5430379746835445,
      "grad_norm": 1.3497672080993652,
      "learning_rate": 5e-05,
      "loss": 1.0544,
      "step": 2799
    },
    {
      "epoch": 3.5443037974683547,
      "grad_norm": 1.3328275680541992,
      "learning_rate": 5e-05,
      "loss": 1.1399,
      "step": 2800
    },
    {
      "epoch": 3.545569620253165,
      "grad_norm": 1.2939459085464478,
      "learning_rate": 5e-05,
      "loss": 1.0363,
      "step": 2801
    },
    {
      "epoch": 3.546835443037975,
      "grad_norm": 1.3439254760742188,
      "learning_rate": 5e-05,
      "loss": 1.0662,
      "step": 2802
    },
    {
      "epoch": 3.548101265822785,
      "grad_norm": 1.327785849571228,
      "learning_rate": 5e-05,
      "loss": 1.1006,
      "step": 2803
    },
    {
      "epoch": 3.549367088607595,
      "grad_norm": 1.310529112815857,
      "learning_rate": 5e-05,
      "loss": 1.0778,
      "step": 2804
    },
    {
      "epoch": 3.5506329113924053,
      "grad_norm": 1.366159439086914,
      "learning_rate": 5e-05,
      "loss": 1.1056,
      "step": 2805
    },
    {
      "epoch": 3.5518987341772155,
      "grad_norm": 1.2925776243209839,
      "learning_rate": 5e-05,
      "loss": 1.0015,
      "step": 2806
    },
    {
      "epoch": 3.553164556962025,
      "grad_norm": 1.3293826580047607,
      "learning_rate": 5e-05,
      "loss": 1.0789,
      "step": 2807
    },
    {
      "epoch": 3.5544303797468353,
      "grad_norm": 1.3943697214126587,
      "learning_rate": 5e-05,
      "loss": 1.1691,
      "step": 2808
    },
    {
      "epoch": 3.5556962025316454,
      "grad_norm": 1.3165359497070312,
      "learning_rate": 5e-05,
      "loss": 1.0308,
      "step": 2809
    },
    {
      "epoch": 3.5569620253164556,
      "grad_norm": 1.287817358970642,
      "learning_rate": 5e-05,
      "loss": 1.0865,
      "step": 2810
    },
    {
      "epoch": 3.5582278481012657,
      "grad_norm": 1.3607112169265747,
      "learning_rate": 5e-05,
      "loss": 1.061,
      "step": 2811
    },
    {
      "epoch": 3.559493670886076,
      "grad_norm": 1.4127728939056396,
      "learning_rate": 5e-05,
      "loss": 1.0728,
      "step": 2812
    },
    {
      "epoch": 3.560759493670886,
      "grad_norm": 1.3090940713882446,
      "learning_rate": 5e-05,
      "loss": 1.035,
      "step": 2813
    },
    {
      "epoch": 3.562025316455696,
      "grad_norm": 1.330284595489502,
      "learning_rate": 5e-05,
      "loss": 1.0918,
      "step": 2814
    },
    {
      "epoch": 3.5632911392405062,
      "grad_norm": 1.332881212234497,
      "learning_rate": 5e-05,
      "loss": 1.1341,
      "step": 2815
    },
    {
      "epoch": 3.5645569620253164,
      "grad_norm": 1.2970510721206665,
      "learning_rate": 5e-05,
      "loss": 1.0172,
      "step": 2816
    },
    {
      "epoch": 3.5658227848101265,
      "grad_norm": 1.3193215131759644,
      "learning_rate": 5e-05,
      "loss": 1.0417,
      "step": 2817
    },
    {
      "epoch": 3.5670886075949366,
      "grad_norm": 1.3137155771255493,
      "learning_rate": 5e-05,
      "loss": 1.0909,
      "step": 2818
    },
    {
      "epoch": 3.5683544303797468,
      "grad_norm": 1.3515639305114746,
      "learning_rate": 5e-05,
      "loss": 1.1009,
      "step": 2819
    },
    {
      "epoch": 3.569620253164557,
      "grad_norm": 1.3203011751174927,
      "learning_rate": 5e-05,
      "loss": 1.0717,
      "step": 2820
    },
    {
      "epoch": 3.570886075949367,
      "grad_norm": 1.3011811971664429,
      "learning_rate": 5e-05,
      "loss": 1.0474,
      "step": 2821
    },
    {
      "epoch": 3.572151898734177,
      "grad_norm": 1.3229080438613892,
      "learning_rate": 5e-05,
      "loss": 1.0965,
      "step": 2822
    },
    {
      "epoch": 3.5734177215189873,
      "grad_norm": 1.3022265434265137,
      "learning_rate": 5e-05,
      "loss": 1.0494,
      "step": 2823
    },
    {
      "epoch": 3.5746835443037974,
      "grad_norm": 1.316681981086731,
      "learning_rate": 5e-05,
      "loss": 1.0186,
      "step": 2824
    },
    {
      "epoch": 3.5759493670886076,
      "grad_norm": 1.3416997194290161,
      "learning_rate": 5e-05,
      "loss": 1.0763,
      "step": 2825
    },
    {
      "epoch": 3.5772151898734177,
      "grad_norm": 1.271544098854065,
      "learning_rate": 5e-05,
      "loss": 1.0477,
      "step": 2826
    },
    {
      "epoch": 3.578481012658228,
      "grad_norm": 1.3199650049209595,
      "learning_rate": 5e-05,
      "loss": 1.0548,
      "step": 2827
    },
    {
      "epoch": 3.579746835443038,
      "grad_norm": 1.3733197450637817,
      "learning_rate": 5e-05,
      "loss": 1.1622,
      "step": 2828
    },
    {
      "epoch": 3.581012658227848,
      "grad_norm": 1.3162041902542114,
      "learning_rate": 5e-05,
      "loss": 1.099,
      "step": 2829
    },
    {
      "epoch": 3.5822784810126582,
      "grad_norm": 1.3405265808105469,
      "learning_rate": 5e-05,
      "loss": 1.0562,
      "step": 2830
    },
    {
      "epoch": 3.5835443037974684,
      "grad_norm": 1.3384406566619873,
      "learning_rate": 5e-05,
      "loss": 1.0958,
      "step": 2831
    },
    {
      "epoch": 3.5848101265822785,
      "grad_norm": 1.3033075332641602,
      "learning_rate": 5e-05,
      "loss": 1.0952,
      "step": 2832
    },
    {
      "epoch": 3.5860759493670886,
      "grad_norm": 1.3606349229812622,
      "learning_rate": 5e-05,
      "loss": 1.1362,
      "step": 2833
    },
    {
      "epoch": 3.5873417721518988,
      "grad_norm": 1.2900569438934326,
      "learning_rate": 5e-05,
      "loss": 1.0098,
      "step": 2834
    },
    {
      "epoch": 3.588607594936709,
      "grad_norm": 1.3713473081588745,
      "learning_rate": 5e-05,
      "loss": 1.1246,
      "step": 2835
    },
    {
      "epoch": 3.589873417721519,
      "grad_norm": 1.3367198705673218,
      "learning_rate": 5e-05,
      "loss": 1.1203,
      "step": 2836
    },
    {
      "epoch": 3.591139240506329,
      "grad_norm": 1.3340895175933838,
      "learning_rate": 5e-05,
      "loss": 1.0581,
      "step": 2837
    },
    {
      "epoch": 3.5924050632911393,
      "grad_norm": 1.2783112525939941,
      "learning_rate": 5e-05,
      "loss": 1.0041,
      "step": 2838
    },
    {
      "epoch": 3.5936708860759494,
      "grad_norm": 1.3701170682907104,
      "learning_rate": 5e-05,
      "loss": 1.0916,
      "step": 2839
    },
    {
      "epoch": 3.5949367088607596,
      "grad_norm": 1.2903683185577393,
      "learning_rate": 5e-05,
      "loss": 1.0285,
      "step": 2840
    },
    {
      "epoch": 3.5962025316455697,
      "grad_norm": 1.3769190311431885,
      "learning_rate": 5e-05,
      "loss": 1.1076,
      "step": 2841
    },
    {
      "epoch": 3.59746835443038,
      "grad_norm": 1.3637962341308594,
      "learning_rate": 5e-05,
      "loss": 1.1315,
      "step": 2842
    },
    {
      "epoch": 3.59873417721519,
      "grad_norm": 1.322097659111023,
      "learning_rate": 5e-05,
      "loss": 1.0351,
      "step": 2843
    },
    {
      "epoch": 3.6,
      "grad_norm": 1.3164057731628418,
      "learning_rate": 5e-05,
      "loss": 1.059,
      "step": 2844
    },
    {
      "epoch": 3.6012658227848102,
      "grad_norm": 1.3529620170593262,
      "learning_rate": 5e-05,
      "loss": 1.0459,
      "step": 2845
    },
    {
      "epoch": 3.6025316455696204,
      "grad_norm": 1.296122670173645,
      "learning_rate": 5e-05,
      "loss": 1.0628,
      "step": 2846
    },
    {
      "epoch": 3.6037974683544305,
      "grad_norm": 1.2983859777450562,
      "learning_rate": 5e-05,
      "loss": 1.0755,
      "step": 2847
    },
    {
      "epoch": 3.6050632911392406,
      "grad_norm": 1.4140442609786987,
      "learning_rate": 5e-05,
      "loss": 1.0501,
      "step": 2848
    },
    {
      "epoch": 3.6063291139240508,
      "grad_norm": 1.3046449422836304,
      "learning_rate": 5e-05,
      "loss": 1.0813,
      "step": 2849
    },
    {
      "epoch": 3.607594936708861,
      "grad_norm": 1.3112211227416992,
      "learning_rate": 5e-05,
      "loss": 1.0641,
      "step": 2850
    },
    {
      "epoch": 3.608860759493671,
      "grad_norm": 1.3673213720321655,
      "learning_rate": 5e-05,
      "loss": 1.167,
      "step": 2851
    },
    {
      "epoch": 3.610126582278481,
      "grad_norm": 1.325336217880249,
      "learning_rate": 5e-05,
      "loss": 1.1072,
      "step": 2852
    },
    {
      "epoch": 3.6113924050632913,
      "grad_norm": 1.3220075368881226,
      "learning_rate": 5e-05,
      "loss": 1.0158,
      "step": 2853
    },
    {
      "epoch": 3.6126582278481014,
      "grad_norm": 1.3323118686676025,
      "learning_rate": 5e-05,
      "loss": 1.117,
      "step": 2854
    },
    {
      "epoch": 3.6139240506329116,
      "grad_norm": 1.2672450542449951,
      "learning_rate": 5e-05,
      "loss": 1.0502,
      "step": 2855
    },
    {
      "epoch": 3.6151898734177212,
      "grad_norm": 1.2889206409454346,
      "learning_rate": 5e-05,
      "loss": 0.9773,
      "step": 2856
    },
    {
      "epoch": 3.6164556962025314,
      "grad_norm": 1.3343766927719116,
      "learning_rate": 5e-05,
      "loss": 1.0816,
      "step": 2857
    },
    {
      "epoch": 3.6177215189873415,
      "grad_norm": 1.3957983255386353,
      "learning_rate": 5e-05,
      "loss": 1.1229,
      "step": 2858
    },
    {
      "epoch": 3.6189873417721516,
      "grad_norm": 1.271881103515625,
      "learning_rate": 5e-05,
      "loss": 0.9661,
      "step": 2859
    },
    {
      "epoch": 3.620253164556962,
      "grad_norm": 1.341633915901184,
      "learning_rate": 5e-05,
      "loss": 1.0833,
      "step": 2860
    },
    {
      "epoch": 3.621518987341772,
      "grad_norm": 1.387040138244629,
      "learning_rate": 5e-05,
      "loss": 1.1116,
      "step": 2861
    },
    {
      "epoch": 3.622784810126582,
      "grad_norm": 1.2891583442687988,
      "learning_rate": 5e-05,
      "loss": 1.0313,
      "step": 2862
    },
    {
      "epoch": 3.624050632911392,
      "grad_norm": 1.2755831480026245,
      "learning_rate": 5e-05,
      "loss": 1.0356,
      "step": 2863
    },
    {
      "epoch": 3.6253164556962023,
      "grad_norm": 1.3473340272903442,
      "learning_rate": 5e-05,
      "loss": 1.0804,
      "step": 2864
    },
    {
      "epoch": 3.6265822784810124,
      "grad_norm": 1.3102046251296997,
      "learning_rate": 5e-05,
      "loss": 1.0609,
      "step": 2865
    },
    {
      "epoch": 3.6278481012658226,
      "grad_norm": 1.3262430429458618,
      "learning_rate": 5e-05,
      "loss": 1.0349,
      "step": 2866
    },
    {
      "epoch": 3.6291139240506327,
      "grad_norm": 1.3714369535446167,
      "learning_rate": 5e-05,
      "loss": 1.0949,
      "step": 2867
    },
    {
      "epoch": 3.630379746835443,
      "grad_norm": 1.3249791860580444,
      "learning_rate": 5e-05,
      "loss": 1.1057,
      "step": 2868
    },
    {
      "epoch": 3.631645569620253,
      "grad_norm": 1.2660080194473267,
      "learning_rate": 5e-05,
      "loss": 1.0253,
      "step": 2869
    },
    {
      "epoch": 3.632911392405063,
      "grad_norm": 1.2709156274795532,
      "learning_rate": 5e-05,
      "loss": 1.0393,
      "step": 2870
    },
    {
      "epoch": 3.6341772151898732,
      "grad_norm": 1.3166881799697876,
      "learning_rate": 5e-05,
      "loss": 1.0942,
      "step": 2871
    },
    {
      "epoch": 3.6354430379746834,
      "grad_norm": 1.350098967552185,
      "learning_rate": 5e-05,
      "loss": 1.1386,
      "step": 2872
    },
    {
      "epoch": 3.6367088607594935,
      "grad_norm": 1.2842423915863037,
      "learning_rate": 5e-05,
      "loss": 1.0571,
      "step": 2873
    },
    {
      "epoch": 3.6379746835443036,
      "grad_norm": 1.3058379888534546,
      "learning_rate": 5e-05,
      "loss": 1.0149,
      "step": 2874
    },
    {
      "epoch": 3.6392405063291138,
      "grad_norm": 1.3491759300231934,
      "learning_rate": 5e-05,
      "loss": 1.0933,
      "step": 2875
    },
    {
      "epoch": 3.640506329113924,
      "grad_norm": 1.3287030458450317,
      "learning_rate": 5e-05,
      "loss": 1.0922,
      "step": 2876
    },
    {
      "epoch": 3.641772151898734,
      "grad_norm": 1.347760558128357,
      "learning_rate": 5e-05,
      "loss": 1.1023,
      "step": 2877
    },
    {
      "epoch": 3.643037974683544,
      "grad_norm": 1.3038973808288574,
      "learning_rate": 5e-05,
      "loss": 1.0599,
      "step": 2878
    },
    {
      "epoch": 3.6443037974683543,
      "grad_norm": 1.3310596942901611,
      "learning_rate": 5e-05,
      "loss": 1.1347,
      "step": 2879
    },
    {
      "epoch": 3.6455696202531644,
      "grad_norm": 1.354387640953064,
      "learning_rate": 5e-05,
      "loss": 1.0946,
      "step": 2880
    },
    {
      "epoch": 3.6468354430379746,
      "grad_norm": 1.3717093467712402,
      "learning_rate": 5e-05,
      "loss": 1.081,
      "step": 2881
    },
    {
      "epoch": 3.6481012658227847,
      "grad_norm": 1.3424512147903442,
      "learning_rate": 5e-05,
      "loss": 1.0693,
      "step": 2882
    },
    {
      "epoch": 3.649367088607595,
      "grad_norm": 1.3479740619659424,
      "learning_rate": 5e-05,
      "loss": 1.0575,
      "step": 2883
    },
    {
      "epoch": 3.650632911392405,
      "grad_norm": 1.3385365009307861,
      "learning_rate": 5e-05,
      "loss": 1.0726,
      "step": 2884
    },
    {
      "epoch": 3.651898734177215,
      "grad_norm": 1.299080729484558,
      "learning_rate": 5e-05,
      "loss": 1.0055,
      "step": 2885
    },
    {
      "epoch": 3.6531645569620252,
      "grad_norm": 1.3519582748413086,
      "learning_rate": 5e-05,
      "loss": 1.0709,
      "step": 2886
    },
    {
      "epoch": 3.6544303797468354,
      "grad_norm": 1.3115456104278564,
      "learning_rate": 5e-05,
      "loss": 1.1114,
      "step": 2887
    },
    {
      "epoch": 3.6556962025316455,
      "grad_norm": 1.3063874244689941,
      "learning_rate": 5e-05,
      "loss": 1.0068,
      "step": 2888
    },
    {
      "epoch": 3.6569620253164556,
      "grad_norm": 1.2894896268844604,
      "learning_rate": 5e-05,
      "loss": 1.0085,
      "step": 2889
    },
    {
      "epoch": 3.6582278481012658,
      "grad_norm": 1.3388534784317017,
      "learning_rate": 5e-05,
      "loss": 1.0246,
      "step": 2890
    },
    {
      "epoch": 3.659493670886076,
      "grad_norm": 1.3627005815505981,
      "learning_rate": 5e-05,
      "loss": 1.0479,
      "step": 2891
    },
    {
      "epoch": 3.660759493670886,
      "grad_norm": 1.3448939323425293,
      "learning_rate": 5e-05,
      "loss": 1.053,
      "step": 2892
    },
    {
      "epoch": 3.662025316455696,
      "grad_norm": 1.272117257118225,
      "learning_rate": 5e-05,
      "loss": 1.0138,
      "step": 2893
    },
    {
      "epoch": 3.6632911392405063,
      "grad_norm": 1.2894645929336548,
      "learning_rate": 5e-05,
      "loss": 1.0482,
      "step": 2894
    },
    {
      "epoch": 3.6645569620253164,
      "grad_norm": 1.304237961769104,
      "learning_rate": 5e-05,
      "loss": 1.0016,
      "step": 2895
    },
    {
      "epoch": 3.6658227848101266,
      "grad_norm": 1.300720453262329,
      "learning_rate": 5e-05,
      "loss": 0.994,
      "step": 2896
    },
    {
      "epoch": 3.6670886075949367,
      "grad_norm": 1.3731986284255981,
      "learning_rate": 5e-05,
      "loss": 1.1112,
      "step": 2897
    },
    {
      "epoch": 3.668354430379747,
      "grad_norm": 1.3045414686203003,
      "learning_rate": 5e-05,
      "loss": 1.0587,
      "step": 2898
    },
    {
      "epoch": 3.669620253164557,
      "grad_norm": 1.3461393117904663,
      "learning_rate": 5e-05,
      "loss": 1.094,
      "step": 2899
    },
    {
      "epoch": 3.670886075949367,
      "grad_norm": 1.3246064186096191,
      "learning_rate": 5e-05,
      "loss": 1.0789,
      "step": 2900
    },
    {
      "epoch": 3.6721518987341772,
      "grad_norm": 1.3518680334091187,
      "learning_rate": 5e-05,
      "loss": 1.0907,
      "step": 2901
    },
    {
      "epoch": 3.6734177215189874,
      "grad_norm": 1.354811429977417,
      "learning_rate": 5e-05,
      "loss": 1.0996,
      "step": 2902
    },
    {
      "epoch": 3.6746835443037975,
      "grad_norm": 1.324412226676941,
      "learning_rate": 5e-05,
      "loss": 1.0466,
      "step": 2903
    },
    {
      "epoch": 3.6759493670886076,
      "grad_norm": 1.3135184049606323,
      "learning_rate": 5e-05,
      "loss": 1.0566,
      "step": 2904
    },
    {
      "epoch": 3.6772151898734178,
      "grad_norm": 1.3438845872879028,
      "learning_rate": 5e-05,
      "loss": 1.0885,
      "step": 2905
    },
    {
      "epoch": 3.678481012658228,
      "grad_norm": 1.3146600723266602,
      "learning_rate": 5e-05,
      "loss": 1.0586,
      "step": 2906
    },
    {
      "epoch": 3.679746835443038,
      "grad_norm": 1.3332767486572266,
      "learning_rate": 5e-05,
      "loss": 1.0689,
      "step": 2907
    },
    {
      "epoch": 3.681012658227848,
      "grad_norm": 1.339795470237732,
      "learning_rate": 5e-05,
      "loss": 1.0741,
      "step": 2908
    },
    {
      "epoch": 3.6822784810126583,
      "grad_norm": 1.323806881904602,
      "learning_rate": 5e-05,
      "loss": 1.0551,
      "step": 2909
    },
    {
      "epoch": 3.6835443037974684,
      "grad_norm": 1.3270822763442993,
      "learning_rate": 5e-05,
      "loss": 0.9857,
      "step": 2910
    },
    {
      "epoch": 3.6848101265822786,
      "grad_norm": 1.3212684392929077,
      "learning_rate": 5e-05,
      "loss": 1.0828,
      "step": 2911
    },
    {
      "epoch": 3.6860759493670887,
      "grad_norm": 1.3393374681472778,
      "learning_rate": 5e-05,
      "loss": 1.0665,
      "step": 2912
    },
    {
      "epoch": 3.687341772151899,
      "grad_norm": 1.3278067111968994,
      "learning_rate": 5e-05,
      "loss": 1.0902,
      "step": 2913
    },
    {
      "epoch": 3.688607594936709,
      "grad_norm": 1.2919037342071533,
      "learning_rate": 5e-05,
      "loss": 1.0683,
      "step": 2914
    },
    {
      "epoch": 3.689873417721519,
      "grad_norm": 1.3340163230895996,
      "learning_rate": 5e-05,
      "loss": 1.0907,
      "step": 2915
    },
    {
      "epoch": 3.6911392405063292,
      "grad_norm": 1.3082114458084106,
      "learning_rate": 5e-05,
      "loss": 1.0861,
      "step": 2916
    },
    {
      "epoch": 3.6924050632911394,
      "grad_norm": 1.3377569913864136,
      "learning_rate": 5e-05,
      "loss": 1.0349,
      "step": 2917
    },
    {
      "epoch": 3.6936708860759495,
      "grad_norm": 1.3331329822540283,
      "learning_rate": 5e-05,
      "loss": 1.0748,
      "step": 2918
    },
    {
      "epoch": 3.6949367088607596,
      "grad_norm": 1.3236700296401978,
      "learning_rate": 5e-05,
      "loss": 1.0523,
      "step": 2919
    },
    {
      "epoch": 3.6962025316455698,
      "grad_norm": 1.277099609375,
      "learning_rate": 5e-05,
      "loss": 1.0155,
      "step": 2920
    },
    {
      "epoch": 3.69746835443038,
      "grad_norm": 1.2962281703948975,
      "learning_rate": 5e-05,
      "loss": 0.9795,
      "step": 2921
    },
    {
      "epoch": 3.69873417721519,
      "grad_norm": 1.3379534482955933,
      "learning_rate": 5e-05,
      "loss": 1.055,
      "step": 2922
    },
    {
      "epoch": 3.7,
      "grad_norm": 1.3177084922790527,
      "learning_rate": 5e-05,
      "loss": 1.0333,
      "step": 2923
    },
    {
      "epoch": 3.7012658227848103,
      "grad_norm": 1.4097615480422974,
      "learning_rate": 5e-05,
      "loss": 1.1404,
      "step": 2924
    },
    {
      "epoch": 3.7025316455696204,
      "grad_norm": 1.285481572151184,
      "learning_rate": 5e-05,
      "loss": 1.0223,
      "step": 2925
    },
    {
      "epoch": 3.7037974683544306,
      "grad_norm": 1.3129652738571167,
      "learning_rate": 5e-05,
      "loss": 1.1099,
      "step": 2926
    },
    {
      "epoch": 3.7050632911392407,
      "grad_norm": 1.3341753482818604,
      "learning_rate": 5e-05,
      "loss": 1.0269,
      "step": 2927
    },
    {
      "epoch": 3.706329113924051,
      "grad_norm": 1.3691017627716064,
      "learning_rate": 5e-05,
      "loss": 1.0753,
      "step": 2928
    },
    {
      "epoch": 3.707594936708861,
      "grad_norm": 1.3153128623962402,
      "learning_rate": 5e-05,
      "loss": 1.0505,
      "step": 2929
    },
    {
      "epoch": 3.708860759493671,
      "grad_norm": 1.3164994716644287,
      "learning_rate": 5e-05,
      "loss": 1.089,
      "step": 2930
    },
    {
      "epoch": 3.7101265822784812,
      "grad_norm": 1.3358222246170044,
      "learning_rate": 5e-05,
      "loss": 1.0092,
      "step": 2931
    },
    {
      "epoch": 3.7113924050632914,
      "grad_norm": 1.30526602268219,
      "learning_rate": 5e-05,
      "loss": 1.0076,
      "step": 2932
    },
    {
      "epoch": 3.7126582278481015,
      "grad_norm": 1.278452754020691,
      "learning_rate": 5e-05,
      "loss": 1.0004,
      "step": 2933
    },
    {
      "epoch": 3.7139240506329116,
      "grad_norm": 1.3327876329421997,
      "learning_rate": 5e-05,
      "loss": 1.0392,
      "step": 2934
    },
    {
      "epoch": 3.7151898734177218,
      "grad_norm": 1.3172094821929932,
      "learning_rate": 5e-05,
      "loss": 1.015,
      "step": 2935
    },
    {
      "epoch": 3.716455696202532,
      "grad_norm": 1.3020515441894531,
      "learning_rate": 5e-05,
      "loss": 1.0411,
      "step": 2936
    },
    {
      "epoch": 3.717721518987342,
      "grad_norm": 1.3120473623275757,
      "learning_rate": 5e-05,
      "loss": 1.0475,
      "step": 2937
    },
    {
      "epoch": 3.7189873417721517,
      "grad_norm": 1.266387701034546,
      "learning_rate": 5e-05,
      "loss": 0.9968,
      "step": 2938
    },
    {
      "epoch": 3.720253164556962,
      "grad_norm": 1.338681697845459,
      "learning_rate": 5e-05,
      "loss": 1.0882,
      "step": 2939
    },
    {
      "epoch": 3.721518987341772,
      "grad_norm": 1.2724517583847046,
      "learning_rate": 5e-05,
      "loss": 1.0426,
      "step": 2940
    },
    {
      "epoch": 3.722784810126582,
      "grad_norm": 1.3396894931793213,
      "learning_rate": 5e-05,
      "loss": 1.0955,
      "step": 2941
    },
    {
      "epoch": 3.7240506329113923,
      "grad_norm": 1.2708128690719604,
      "learning_rate": 5e-05,
      "loss": 1.0129,
      "step": 2942
    },
    {
      "epoch": 3.7253164556962024,
      "grad_norm": 1.282861351966858,
      "learning_rate": 5e-05,
      "loss": 0.9689,
      "step": 2943
    },
    {
      "epoch": 3.7265822784810125,
      "grad_norm": 1.2971140146255493,
      "learning_rate": 5e-05,
      "loss": 1.0585,
      "step": 2944
    },
    {
      "epoch": 3.7278481012658227,
      "grad_norm": 1.3422293663024902,
      "learning_rate": 5e-05,
      "loss": 1.0703,
      "step": 2945
    },
    {
      "epoch": 3.729113924050633,
      "grad_norm": 1.2687106132507324,
      "learning_rate": 5e-05,
      "loss": 0.9692,
      "step": 2946
    },
    {
      "epoch": 3.730379746835443,
      "grad_norm": 1.3354244232177734,
      "learning_rate": 5e-05,
      "loss": 1.0549,
      "step": 2947
    },
    {
      "epoch": 3.731645569620253,
      "grad_norm": 1.333009123802185,
      "learning_rate": 5e-05,
      "loss": 1.1122,
      "step": 2948
    },
    {
      "epoch": 3.732911392405063,
      "grad_norm": 1.333903193473816,
      "learning_rate": 5e-05,
      "loss": 1.0407,
      "step": 2949
    },
    {
      "epoch": 3.7341772151898733,
      "grad_norm": 1.3217896223068237,
      "learning_rate": 5e-05,
      "loss": 1.0708,
      "step": 2950
    },
    {
      "epoch": 3.7354430379746835,
      "grad_norm": 1.3305727243423462,
      "learning_rate": 5e-05,
      "loss": 1.0665,
      "step": 2951
    },
    {
      "epoch": 3.7367088607594936,
      "grad_norm": 1.3352810144424438,
      "learning_rate": 5e-05,
      "loss": 1.094,
      "step": 2952
    },
    {
      "epoch": 3.7379746835443037,
      "grad_norm": 1.327659010887146,
      "learning_rate": 5e-05,
      "loss": 1.0608,
      "step": 2953
    },
    {
      "epoch": 3.739240506329114,
      "grad_norm": 1.3091111183166504,
      "learning_rate": 5e-05,
      "loss": 1.0908,
      "step": 2954
    },
    {
      "epoch": 3.740506329113924,
      "grad_norm": 1.3897351026535034,
      "learning_rate": 5e-05,
      "loss": 1.1191,
      "step": 2955
    },
    {
      "epoch": 3.741772151898734,
      "grad_norm": 1.365341305732727,
      "learning_rate": 5e-05,
      "loss": 1.0401,
      "step": 2956
    },
    {
      "epoch": 3.7430379746835443,
      "grad_norm": 1.3430813550949097,
      "learning_rate": 5e-05,
      "loss": 1.1009,
      "step": 2957
    },
    {
      "epoch": 3.7443037974683544,
      "grad_norm": 1.345131754875183,
      "learning_rate": 5e-05,
      "loss": 1.0419,
      "step": 2958
    },
    {
      "epoch": 3.7455696202531645,
      "grad_norm": 1.2960976362228394,
      "learning_rate": 5e-05,
      "loss": 1.057,
      "step": 2959
    },
    {
      "epoch": 3.7468354430379747,
      "grad_norm": 1.3316115140914917,
      "learning_rate": 5e-05,
      "loss": 1.0554,
      "step": 2960
    },
    {
      "epoch": 3.748101265822785,
      "grad_norm": 1.293915867805481,
      "learning_rate": 5e-05,
      "loss": 1.0548,
      "step": 2961
    },
    {
      "epoch": 3.749367088607595,
      "grad_norm": 1.2718695402145386,
      "learning_rate": 5e-05,
      "loss": 0.9523,
      "step": 2962
    },
    {
      "epoch": 3.750632911392405,
      "grad_norm": 1.2665504217147827,
      "learning_rate": 5e-05,
      "loss": 1.0271,
      "step": 2963
    },
    {
      "epoch": 3.751898734177215,
      "grad_norm": 1.360694169998169,
      "learning_rate": 5e-05,
      "loss": 1.076,
      "step": 2964
    },
    {
      "epoch": 3.7531645569620253,
      "grad_norm": 1.339540719985962,
      "learning_rate": 5e-05,
      "loss": 1.1137,
      "step": 2965
    },
    {
      "epoch": 3.7544303797468355,
      "grad_norm": 1.3285953998565674,
      "learning_rate": 5e-05,
      "loss": 1.0484,
      "step": 2966
    },
    {
      "epoch": 3.7556962025316456,
      "grad_norm": 1.3193882703781128,
      "learning_rate": 5e-05,
      "loss": 1.021,
      "step": 2967
    },
    {
      "epoch": 3.7569620253164557,
      "grad_norm": 1.3063362836837769,
      "learning_rate": 5e-05,
      "loss": 0.99,
      "step": 2968
    },
    {
      "epoch": 3.758227848101266,
      "grad_norm": 1.326197862625122,
      "learning_rate": 5e-05,
      "loss": 1.0272,
      "step": 2969
    },
    {
      "epoch": 3.759493670886076,
      "grad_norm": 1.3376028537750244,
      "learning_rate": 5e-05,
      "loss": 1.036,
      "step": 2970
    },
    {
      "epoch": 3.760759493670886,
      "grad_norm": 1.3245618343353271,
      "learning_rate": 5e-05,
      "loss": 1.0718,
      "step": 2971
    },
    {
      "epoch": 3.7620253164556963,
      "grad_norm": 1.3647990226745605,
      "learning_rate": 5e-05,
      "loss": 1.0388,
      "step": 2972
    },
    {
      "epoch": 3.7632911392405064,
      "grad_norm": 1.3358008861541748,
      "learning_rate": 5e-05,
      "loss": 1.0476,
      "step": 2973
    },
    {
      "epoch": 3.7645569620253165,
      "grad_norm": 1.2749866247177124,
      "learning_rate": 5e-05,
      "loss": 1.0076,
      "step": 2974
    },
    {
      "epoch": 3.7658227848101267,
      "grad_norm": 1.3232125043869019,
      "learning_rate": 5e-05,
      "loss": 1.0452,
      "step": 2975
    },
    {
      "epoch": 3.767088607594937,
      "grad_norm": 1.3256901502609253,
      "learning_rate": 5e-05,
      "loss": 1.0496,
      "step": 2976
    },
    {
      "epoch": 3.768354430379747,
      "grad_norm": 1.3122905492782593,
      "learning_rate": 5e-05,
      "loss": 1.1185,
      "step": 2977
    },
    {
      "epoch": 3.769620253164557,
      "grad_norm": 1.340766191482544,
      "learning_rate": 5e-05,
      "loss": 1.0759,
      "step": 2978
    },
    {
      "epoch": 3.770886075949367,
      "grad_norm": 1.3155611753463745,
      "learning_rate": 5e-05,
      "loss": 1.1549,
      "step": 2979
    },
    {
      "epoch": 3.7721518987341773,
      "grad_norm": 1.3166457414627075,
      "learning_rate": 5e-05,
      "loss": 1.0172,
      "step": 2980
    },
    {
      "epoch": 3.7734177215189875,
      "grad_norm": 1.2959396839141846,
      "learning_rate": 5e-05,
      "loss": 1.0393,
      "step": 2981
    },
    {
      "epoch": 3.7746835443037976,
      "grad_norm": 1.3324633836746216,
      "learning_rate": 5e-05,
      "loss": 1.064,
      "step": 2982
    },
    {
      "epoch": 3.7759493670886077,
      "grad_norm": 1.295090675354004,
      "learning_rate": 5e-05,
      "loss": 1.0448,
      "step": 2983
    },
    {
      "epoch": 3.777215189873418,
      "grad_norm": 1.3333858251571655,
      "learning_rate": 5e-05,
      "loss": 1.0696,
      "step": 2984
    },
    {
      "epoch": 3.778481012658228,
      "grad_norm": 1.319371223449707,
      "learning_rate": 5e-05,
      "loss": 1.0433,
      "step": 2985
    },
    {
      "epoch": 3.779746835443038,
      "grad_norm": 1.3396626710891724,
      "learning_rate": 5e-05,
      "loss": 1.0268,
      "step": 2986
    },
    {
      "epoch": 3.7810126582278483,
      "grad_norm": 1.3698192834854126,
      "learning_rate": 5e-05,
      "loss": 1.0492,
      "step": 2987
    },
    {
      "epoch": 3.782278481012658,
      "grad_norm": 1.3202970027923584,
      "learning_rate": 5e-05,
      "loss": 1.0736,
      "step": 2988
    },
    {
      "epoch": 3.783544303797468,
      "grad_norm": 1.367104411125183,
      "learning_rate": 5e-05,
      "loss": 1.1392,
      "step": 2989
    },
    {
      "epoch": 3.7848101265822782,
      "grad_norm": 1.3448103666305542,
      "learning_rate": 5e-05,
      "loss": 1.0892,
      "step": 2990
    },
    {
      "epoch": 3.7860759493670884,
      "grad_norm": 1.2966948747634888,
      "learning_rate": 5e-05,
      "loss": 0.9996,
      "step": 2991
    },
    {
      "epoch": 3.7873417721518985,
      "grad_norm": 1.3300522565841675,
      "learning_rate": 5e-05,
      "loss": 1.0794,
      "step": 2992
    },
    {
      "epoch": 3.7886075949367086,
      "grad_norm": 1.3129628896713257,
      "learning_rate": 5e-05,
      "loss": 1.0536,
      "step": 2993
    },
    {
      "epoch": 3.7898734177215188,
      "grad_norm": 1.2808283567428589,
      "learning_rate": 5e-05,
      "loss": 1.0329,
      "step": 2994
    },
    {
      "epoch": 3.791139240506329,
      "grad_norm": 1.2980304956436157,
      "learning_rate": 5e-05,
      "loss": 1.0264,
      "step": 2995
    },
    {
      "epoch": 3.792405063291139,
      "grad_norm": 1.3065463304519653,
      "learning_rate": 5e-05,
      "loss": 1.0088,
      "step": 2996
    },
    {
      "epoch": 3.793670886075949,
      "grad_norm": 1.3107757568359375,
      "learning_rate": 5e-05,
      "loss": 1.0344,
      "step": 2997
    },
    {
      "epoch": 3.7949367088607593,
      "grad_norm": 1.3770842552185059,
      "learning_rate": 5e-05,
      "loss": 1.1898,
      "step": 2998
    },
    {
      "epoch": 3.7962025316455694,
      "grad_norm": 1.2948111295700073,
      "learning_rate": 5e-05,
      "loss": 1.0408,
      "step": 2999
    },
    {
      "epoch": 3.7974683544303796,
      "grad_norm": 1.3508671522140503,
      "learning_rate": 5e-05,
      "loss": 1.0835,
      "step": 3000
    },
    {
      "epoch": 3.7987341772151897,
      "grad_norm": 1.3556467294692993,
      "learning_rate": 5e-05,
      "loss": 1.0345,
      "step": 3001
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.286191701889038,
      "learning_rate": 5e-05,
      "loss": 1.0163,
      "step": 3002
    },
    {
      "epoch": 3.80126582278481,
      "grad_norm": 1.326707363128662,
      "learning_rate": 5e-05,
      "loss": 1.0984,
      "step": 3003
    },
    {
      "epoch": 3.80253164556962,
      "grad_norm": 1.326526403427124,
      "learning_rate": 5e-05,
      "loss": 1.0624,
      "step": 3004
    },
    {
      "epoch": 3.8037974683544302,
      "grad_norm": 1.3332780599594116,
      "learning_rate": 5e-05,
      "loss": 1.0706,
      "step": 3005
    },
    {
      "epoch": 3.8050632911392404,
      "grad_norm": 1.2999978065490723,
      "learning_rate": 5e-05,
      "loss": 1.0224,
      "step": 3006
    },
    {
      "epoch": 3.8063291139240505,
      "grad_norm": 1.2889809608459473,
      "learning_rate": 5e-05,
      "loss": 1.048,
      "step": 3007
    },
    {
      "epoch": 3.8075949367088606,
      "grad_norm": 1.3278982639312744,
      "learning_rate": 5e-05,
      "loss": 1.0158,
      "step": 3008
    },
    {
      "epoch": 3.8088607594936708,
      "grad_norm": 1.3786824941635132,
      "learning_rate": 5e-05,
      "loss": 1.1306,
      "step": 3009
    },
    {
      "epoch": 3.810126582278481,
      "grad_norm": 1.3190696239471436,
      "learning_rate": 5e-05,
      "loss": 1.0043,
      "step": 3010
    },
    {
      "epoch": 3.811392405063291,
      "grad_norm": 1.3794301748275757,
      "learning_rate": 5e-05,
      "loss": 1.1065,
      "step": 3011
    },
    {
      "epoch": 3.812658227848101,
      "grad_norm": 1.3914772272109985,
      "learning_rate": 5e-05,
      "loss": 1.1271,
      "step": 3012
    },
    {
      "epoch": 3.8139240506329113,
      "grad_norm": 1.3615831136703491,
      "learning_rate": 5e-05,
      "loss": 1.0581,
      "step": 3013
    },
    {
      "epoch": 3.8151898734177214,
      "grad_norm": 1.350468635559082,
      "learning_rate": 5e-05,
      "loss": 1.0979,
      "step": 3014
    },
    {
      "epoch": 3.8164556962025316,
      "grad_norm": 1.3259094953536987,
      "learning_rate": 5e-05,
      "loss": 1.018,
      "step": 3015
    },
    {
      "epoch": 3.8177215189873417,
      "grad_norm": 1.3264180421829224,
      "learning_rate": 5e-05,
      "loss": 1.081,
      "step": 3016
    },
    {
      "epoch": 3.818987341772152,
      "grad_norm": 1.3093785047531128,
      "learning_rate": 5e-05,
      "loss": 1.0465,
      "step": 3017
    },
    {
      "epoch": 3.820253164556962,
      "grad_norm": 1.286240816116333,
      "learning_rate": 5e-05,
      "loss": 0.9724,
      "step": 3018
    },
    {
      "epoch": 3.821518987341772,
      "grad_norm": 1.3134095668792725,
      "learning_rate": 5e-05,
      "loss": 1.0379,
      "step": 3019
    },
    {
      "epoch": 3.8227848101265822,
      "grad_norm": 1.3208292722702026,
      "learning_rate": 5e-05,
      "loss": 1.0154,
      "step": 3020
    },
    {
      "epoch": 3.8240506329113924,
      "grad_norm": 1.2848485708236694,
      "learning_rate": 5e-05,
      "loss": 0.9673,
      "step": 3021
    },
    {
      "epoch": 3.8253164556962025,
      "grad_norm": 1.3638299703598022,
      "learning_rate": 5e-05,
      "loss": 1.1181,
      "step": 3022
    },
    {
      "epoch": 3.8265822784810126,
      "grad_norm": 1.3073029518127441,
      "learning_rate": 5e-05,
      "loss": 1.0161,
      "step": 3023
    },
    {
      "epoch": 3.8278481012658228,
      "grad_norm": 1.3562164306640625,
      "learning_rate": 5e-05,
      "loss": 1.0369,
      "step": 3024
    },
    {
      "epoch": 3.829113924050633,
      "grad_norm": 1.3625308275222778,
      "learning_rate": 5e-05,
      "loss": 1.0316,
      "step": 3025
    },
    {
      "epoch": 3.830379746835443,
      "grad_norm": 1.2736914157867432,
      "learning_rate": 5e-05,
      "loss": 1.0622,
      "step": 3026
    },
    {
      "epoch": 3.831645569620253,
      "grad_norm": 1.3195805549621582,
      "learning_rate": 5e-05,
      "loss": 1.0174,
      "step": 3027
    },
    {
      "epoch": 3.8329113924050633,
      "grad_norm": 1.2698943614959717,
      "learning_rate": 5e-05,
      "loss": 0.9468,
      "step": 3028
    },
    {
      "epoch": 3.8341772151898734,
      "grad_norm": 1.3560166358947754,
      "learning_rate": 5e-05,
      "loss": 1.0641,
      "step": 3029
    },
    {
      "epoch": 3.8354430379746836,
      "grad_norm": 1.3449132442474365,
      "learning_rate": 5e-05,
      "loss": 1.1045,
      "step": 3030
    },
    {
      "epoch": 3.8367088607594937,
      "grad_norm": 1.3042479753494263,
      "learning_rate": 5e-05,
      "loss": 0.971,
      "step": 3031
    },
    {
      "epoch": 3.837974683544304,
      "grad_norm": 1.3028545379638672,
      "learning_rate": 5e-05,
      "loss": 1.0922,
      "step": 3032
    },
    {
      "epoch": 3.839240506329114,
      "grad_norm": 1.2952255010604858,
      "learning_rate": 5e-05,
      "loss": 1.0576,
      "step": 3033
    },
    {
      "epoch": 3.840506329113924,
      "grad_norm": 1.3303370475769043,
      "learning_rate": 5e-05,
      "loss": 1.0142,
      "step": 3034
    },
    {
      "epoch": 3.8417721518987342,
      "grad_norm": 1.3123273849487305,
      "learning_rate": 5e-05,
      "loss": 1.0139,
      "step": 3035
    },
    {
      "epoch": 3.8430379746835444,
      "grad_norm": 1.303086757659912,
      "learning_rate": 5e-05,
      "loss": 1.0818,
      "step": 3036
    },
    {
      "epoch": 3.8443037974683545,
      "grad_norm": 1.36104154586792,
      "learning_rate": 5e-05,
      "loss": 1.0173,
      "step": 3037
    },
    {
      "epoch": 3.8455696202531646,
      "grad_norm": 1.2977272272109985,
      "learning_rate": 5e-05,
      "loss": 1.007,
      "step": 3038
    },
    {
      "epoch": 3.8468354430379748,
      "grad_norm": 1.3413563966751099,
      "learning_rate": 5e-05,
      "loss": 1.0639,
      "step": 3039
    },
    {
      "epoch": 3.848101265822785,
      "grad_norm": 1.3691327571868896,
      "learning_rate": 5e-05,
      "loss": 1.0739,
      "step": 3040
    },
    {
      "epoch": 3.849367088607595,
      "grad_norm": 1.299880027770996,
      "learning_rate": 5e-05,
      "loss": 1.0085,
      "step": 3041
    },
    {
      "epoch": 3.850632911392405,
      "grad_norm": 1.3296406269073486,
      "learning_rate": 5e-05,
      "loss": 0.9943,
      "step": 3042
    },
    {
      "epoch": 3.8518987341772153,
      "grad_norm": 1.3220713138580322,
      "learning_rate": 5e-05,
      "loss": 1.0073,
      "step": 3043
    },
    {
      "epoch": 3.8531645569620254,
      "grad_norm": 1.3068991899490356,
      "learning_rate": 5e-05,
      "loss": 1.0423,
      "step": 3044
    },
    {
      "epoch": 3.8544303797468356,
      "grad_norm": 1.3181053400039673,
      "learning_rate": 5e-05,
      "loss": 1.0288,
      "step": 3045
    },
    {
      "epoch": 3.8556962025316457,
      "grad_norm": 1.3208940029144287,
      "learning_rate": 5e-05,
      "loss": 1.0062,
      "step": 3046
    },
    {
      "epoch": 3.856962025316456,
      "grad_norm": 1.30864417552948,
      "learning_rate": 5e-05,
      "loss": 1.0454,
      "step": 3047
    },
    {
      "epoch": 3.858227848101266,
      "grad_norm": 1.2959563732147217,
      "learning_rate": 5e-05,
      "loss": 1.0782,
      "step": 3048
    },
    {
      "epoch": 3.859493670886076,
      "grad_norm": 1.350009799003601,
      "learning_rate": 5e-05,
      "loss": 1.1056,
      "step": 3049
    },
    {
      "epoch": 3.8607594936708862,
      "grad_norm": 1.3668198585510254,
      "learning_rate": 5e-05,
      "loss": 1.0564,
      "step": 3050
    },
    {
      "epoch": 3.8620253164556964,
      "grad_norm": 1.2728418111801147,
      "learning_rate": 5e-05,
      "loss": 1.0018,
      "step": 3051
    },
    {
      "epoch": 3.8632911392405065,
      "grad_norm": 1.3129563331604004,
      "learning_rate": 5e-05,
      "loss": 1.0092,
      "step": 3052
    },
    {
      "epoch": 3.8645569620253166,
      "grad_norm": 1.3218692541122437,
      "learning_rate": 5e-05,
      "loss": 1.0731,
      "step": 3053
    },
    {
      "epoch": 3.8658227848101268,
      "grad_norm": 1.3080259561538696,
      "learning_rate": 5e-05,
      "loss": 1.0246,
      "step": 3054
    },
    {
      "epoch": 3.867088607594937,
      "grad_norm": 1.3037952184677124,
      "learning_rate": 5e-05,
      "loss": 1.0564,
      "step": 3055
    },
    {
      "epoch": 3.868354430379747,
      "grad_norm": 1.3239095211029053,
      "learning_rate": 5e-05,
      "loss": 1.1152,
      "step": 3056
    },
    {
      "epoch": 3.869620253164557,
      "grad_norm": 1.361016869544983,
      "learning_rate": 5e-05,
      "loss": 1.0848,
      "step": 3057
    },
    {
      "epoch": 3.8708860759493673,
      "grad_norm": 1.3158279657363892,
      "learning_rate": 5e-05,
      "loss": 1.0299,
      "step": 3058
    },
    {
      "epoch": 3.8721518987341774,
      "grad_norm": 1.3136868476867676,
      "learning_rate": 5e-05,
      "loss": 1.0071,
      "step": 3059
    },
    {
      "epoch": 3.8734177215189876,
      "grad_norm": 1.2843445539474487,
      "learning_rate": 5e-05,
      "loss": 0.9863,
      "step": 3060
    },
    {
      "epoch": 3.8746835443037977,
      "grad_norm": 1.3820908069610596,
      "learning_rate": 5e-05,
      "loss": 1.1071,
      "step": 3061
    },
    {
      "epoch": 3.875949367088608,
      "grad_norm": 1.3501553535461426,
      "learning_rate": 5e-05,
      "loss": 1.0701,
      "step": 3062
    },
    {
      "epoch": 3.877215189873418,
      "grad_norm": 1.3683335781097412,
      "learning_rate": 5e-05,
      "loss": 1.0585,
      "step": 3063
    },
    {
      "epoch": 3.878481012658228,
      "grad_norm": 1.288498044013977,
      "learning_rate": 5e-05,
      "loss": 0.9866,
      "step": 3064
    },
    {
      "epoch": 3.879746835443038,
      "grad_norm": 1.3095506429672241,
      "learning_rate": 5e-05,
      "loss": 1.0175,
      "step": 3065
    },
    {
      "epoch": 3.8810126582278484,
      "grad_norm": 1.3259660005569458,
      "learning_rate": 5e-05,
      "loss": 1.0606,
      "step": 3066
    },
    {
      "epoch": 3.8822784810126585,
      "grad_norm": 1.3084743022918701,
      "learning_rate": 5e-05,
      "loss": 1.0671,
      "step": 3067
    },
    {
      "epoch": 3.8835443037974686,
      "grad_norm": 1.3449835777282715,
      "learning_rate": 5e-05,
      "loss": 1.068,
      "step": 3068
    },
    {
      "epoch": 3.8848101265822788,
      "grad_norm": 1.3143800497055054,
      "learning_rate": 5e-05,
      "loss": 1.0353,
      "step": 3069
    },
    {
      "epoch": 3.8860759493670884,
      "grad_norm": 1.317046880722046,
      "learning_rate": 5e-05,
      "loss": 1.0174,
      "step": 3070
    },
    {
      "epoch": 3.8873417721518986,
      "grad_norm": 1.3757890462875366,
      "learning_rate": 5e-05,
      "loss": 1.0837,
      "step": 3071
    },
    {
      "epoch": 3.8886075949367087,
      "grad_norm": 1.3468981981277466,
      "learning_rate": 5e-05,
      "loss": 0.9964,
      "step": 3072
    },
    {
      "epoch": 3.889873417721519,
      "grad_norm": 1.3138643503189087,
      "learning_rate": 5e-05,
      "loss": 1.0807,
      "step": 3073
    },
    {
      "epoch": 3.891139240506329,
      "grad_norm": 1.346008062362671,
      "learning_rate": 5e-05,
      "loss": 1.0775,
      "step": 3074
    },
    {
      "epoch": 3.892405063291139,
      "grad_norm": 1.3146449327468872,
      "learning_rate": 5e-05,
      "loss": 1.024,
      "step": 3075
    },
    {
      "epoch": 3.8936708860759492,
      "grad_norm": 1.278490424156189,
      "learning_rate": 5e-05,
      "loss": 1.0418,
      "step": 3076
    },
    {
      "epoch": 3.8949367088607594,
      "grad_norm": 1.3500198125839233,
      "learning_rate": 5e-05,
      "loss": 1.0424,
      "step": 3077
    },
    {
      "epoch": 3.8962025316455695,
      "grad_norm": 1.2965019941329956,
      "learning_rate": 5e-05,
      "loss": 0.9781,
      "step": 3078
    },
    {
      "epoch": 3.8974683544303796,
      "grad_norm": 1.2939292192459106,
      "learning_rate": 5e-05,
      "loss": 1.0395,
      "step": 3079
    },
    {
      "epoch": 3.8987341772151898,
      "grad_norm": 1.3210822343826294,
      "learning_rate": 5e-05,
      "loss": 1.0383,
      "step": 3080
    },
    {
      "epoch": 3.9,
      "grad_norm": 1.3262317180633545,
      "learning_rate": 5e-05,
      "loss": 1.0717,
      "step": 3081
    },
    {
      "epoch": 3.90126582278481,
      "grad_norm": 1.3146488666534424,
      "learning_rate": 5e-05,
      "loss": 1.0176,
      "step": 3082
    },
    {
      "epoch": 3.90253164556962,
      "grad_norm": 1.3401240110397339,
      "learning_rate": 5e-05,
      "loss": 1.0635,
      "step": 3083
    },
    {
      "epoch": 3.9037974683544303,
      "grad_norm": 1.3850151300430298,
      "learning_rate": 5e-05,
      "loss": 1.1298,
      "step": 3084
    },
    {
      "epoch": 3.9050632911392404,
      "grad_norm": 1.296579360961914,
      "learning_rate": 5e-05,
      "loss": 1.0275,
      "step": 3085
    },
    {
      "epoch": 3.9063291139240506,
      "grad_norm": 1.2960783243179321,
      "learning_rate": 5e-05,
      "loss": 1.0378,
      "step": 3086
    },
    {
      "epoch": 3.9075949367088607,
      "grad_norm": 1.3096976280212402,
      "learning_rate": 5e-05,
      "loss": 1.0451,
      "step": 3087
    },
    {
      "epoch": 3.908860759493671,
      "grad_norm": 1.3369518518447876,
      "learning_rate": 5e-05,
      "loss": 1.0558,
      "step": 3088
    },
    {
      "epoch": 3.910126582278481,
      "grad_norm": 1.2841054201126099,
      "learning_rate": 5e-05,
      "loss": 0.9878,
      "step": 3089
    },
    {
      "epoch": 3.911392405063291,
      "grad_norm": 1.3201686143875122,
      "learning_rate": 5e-05,
      "loss": 1.0144,
      "step": 3090
    },
    {
      "epoch": 3.9126582278481012,
      "grad_norm": 1.3571845293045044,
      "learning_rate": 5e-05,
      "loss": 1.0061,
      "step": 3091
    },
    {
      "epoch": 3.9139240506329114,
      "grad_norm": 1.3043239116668701,
      "learning_rate": 5e-05,
      "loss": 1.0559,
      "step": 3092
    },
    {
      "epoch": 3.9151898734177215,
      "grad_norm": 1.3194434642791748,
      "learning_rate": 5e-05,
      "loss": 0.9875,
      "step": 3093
    },
    {
      "epoch": 3.9164556962025316,
      "grad_norm": 1.3046491146087646,
      "learning_rate": 5e-05,
      "loss": 1.0726,
      "step": 3094
    },
    {
      "epoch": 3.9177215189873418,
      "grad_norm": 1.340335488319397,
      "learning_rate": 5e-05,
      "loss": 0.9973,
      "step": 3095
    },
    {
      "epoch": 3.918987341772152,
      "grad_norm": 1.363045334815979,
      "learning_rate": 5e-05,
      "loss": 1.0764,
      "step": 3096
    },
    {
      "epoch": 3.920253164556962,
      "grad_norm": 1.3922690153121948,
      "learning_rate": 5e-05,
      "loss": 1.0861,
      "step": 3097
    },
    {
      "epoch": 3.921518987341772,
      "grad_norm": 1.3547565937042236,
      "learning_rate": 5e-05,
      "loss": 1.0768,
      "step": 3098
    },
    {
      "epoch": 3.9227848101265823,
      "grad_norm": 1.3308911323547363,
      "learning_rate": 5e-05,
      "loss": 0.9805,
      "step": 3099
    },
    {
      "epoch": 3.9240506329113924,
      "grad_norm": 1.3143655061721802,
      "learning_rate": 5e-05,
      "loss": 0.9964,
      "step": 3100
    },
    {
      "epoch": 3.9253164556962026,
      "grad_norm": 1.3215553760528564,
      "learning_rate": 5e-05,
      "loss": 1.0806,
      "step": 3101
    },
    {
      "epoch": 3.9265822784810127,
      "grad_norm": 1.2966359853744507,
      "learning_rate": 5e-05,
      "loss": 1.0136,
      "step": 3102
    },
    {
      "epoch": 3.927848101265823,
      "grad_norm": 1.3682024478912354,
      "learning_rate": 5e-05,
      "loss": 1.0367,
      "step": 3103
    },
    {
      "epoch": 3.929113924050633,
      "grad_norm": 1.3685020208358765,
      "learning_rate": 5e-05,
      "loss": 1.0457,
      "step": 3104
    },
    {
      "epoch": 3.930379746835443,
      "grad_norm": 1.3004275560379028,
      "learning_rate": 5e-05,
      "loss": 1.0439,
      "step": 3105
    },
    {
      "epoch": 3.9316455696202532,
      "grad_norm": 1.3215395212173462,
      "learning_rate": 5e-05,
      "loss": 1.0602,
      "step": 3106
    },
    {
      "epoch": 3.9329113924050634,
      "grad_norm": 1.3836474418640137,
      "learning_rate": 5e-05,
      "loss": 1.0429,
      "step": 3107
    },
    {
      "epoch": 3.9341772151898735,
      "grad_norm": 1.286303162574768,
      "learning_rate": 5e-05,
      "loss": 1.0609,
      "step": 3108
    },
    {
      "epoch": 3.9354430379746836,
      "grad_norm": 1.265829086303711,
      "learning_rate": 5e-05,
      "loss": 0.9637,
      "step": 3109
    },
    {
      "epoch": 3.9367088607594938,
      "grad_norm": 1.2837326526641846,
      "learning_rate": 5e-05,
      "loss": 0.9944,
      "step": 3110
    },
    {
      "epoch": 3.937974683544304,
      "grad_norm": 1.2575325965881348,
      "learning_rate": 5e-05,
      "loss": 0.9378,
      "step": 3111
    },
    {
      "epoch": 3.939240506329114,
      "grad_norm": 1.3073962926864624,
      "learning_rate": 5e-05,
      "loss": 1.0559,
      "step": 3112
    },
    {
      "epoch": 3.940506329113924,
      "grad_norm": 1.3234268426895142,
      "learning_rate": 5e-05,
      "loss": 1.0039,
      "step": 3113
    },
    {
      "epoch": 3.9417721518987343,
      "grad_norm": 1.3158589601516724,
      "learning_rate": 5e-05,
      "loss": 1.0395,
      "step": 3114
    },
    {
      "epoch": 3.9430379746835444,
      "grad_norm": 1.3532017469406128,
      "learning_rate": 5e-05,
      "loss": 1.0678,
      "step": 3115
    },
    {
      "epoch": 3.9443037974683546,
      "grad_norm": 1.303654670715332,
      "learning_rate": 5e-05,
      "loss": 0.9984,
      "step": 3116
    },
    {
      "epoch": 3.9455696202531647,
      "grad_norm": 1.3158260583877563,
      "learning_rate": 5e-05,
      "loss": 0.9717,
      "step": 3117
    },
    {
      "epoch": 3.946835443037975,
      "grad_norm": 1.3024307489395142,
      "learning_rate": 5e-05,
      "loss": 1.029,
      "step": 3118
    },
    {
      "epoch": 3.9481012658227845,
      "grad_norm": 1.270399570465088,
      "learning_rate": 5e-05,
      "loss": 1.0222,
      "step": 3119
    },
    {
      "epoch": 3.9493670886075947,
      "grad_norm": 1.3507111072540283,
      "learning_rate": 5e-05,
      "loss": 1.0462,
      "step": 3120
    },
    {
      "epoch": 3.950632911392405,
      "grad_norm": 1.3046761751174927,
      "learning_rate": 5e-05,
      "loss": 1.0079,
      "step": 3121
    },
    {
      "epoch": 3.951898734177215,
      "grad_norm": 1.2970949411392212,
      "learning_rate": 5e-05,
      "loss": 1.0544,
      "step": 3122
    },
    {
      "epoch": 3.953164556962025,
      "grad_norm": 1.291520118713379,
      "learning_rate": 5e-05,
      "loss": 1.0306,
      "step": 3123
    },
    {
      "epoch": 3.954430379746835,
      "grad_norm": 1.3243564367294312,
      "learning_rate": 5e-05,
      "loss": 1.0119,
      "step": 3124
    },
    {
      "epoch": 3.9556962025316453,
      "grad_norm": 1.2809134721755981,
      "learning_rate": 5e-05,
      "loss": 1.0258,
      "step": 3125
    },
    {
      "epoch": 3.9569620253164555,
      "grad_norm": 1.3070266246795654,
      "learning_rate": 5e-05,
      "loss": 0.9981,
      "step": 3126
    },
    {
      "epoch": 3.9582278481012656,
      "grad_norm": 1.3144197463989258,
      "learning_rate": 5e-05,
      "loss": 0.9914,
      "step": 3127
    },
    {
      "epoch": 3.9594936708860757,
      "grad_norm": 1.264379620552063,
      "learning_rate": 5e-05,
      "loss": 0.9522,
      "step": 3128
    },
    {
      "epoch": 3.960759493670886,
      "grad_norm": 1.3237807750701904,
      "learning_rate": 5e-05,
      "loss": 1.0803,
      "step": 3129
    },
    {
      "epoch": 3.962025316455696,
      "grad_norm": 1.3087183237075806,
      "learning_rate": 5e-05,
      "loss": 1.0343,
      "step": 3130
    },
    {
      "epoch": 3.963291139240506,
      "grad_norm": 1.314368486404419,
      "learning_rate": 5e-05,
      "loss": 1.007,
      "step": 3131
    },
    {
      "epoch": 3.9645569620253163,
      "grad_norm": 1.3131121397018433,
      "learning_rate": 5e-05,
      "loss": 0.9532,
      "step": 3132
    },
    {
      "epoch": 3.9658227848101264,
      "grad_norm": 1.3604555130004883,
      "learning_rate": 5e-05,
      "loss": 1.092,
      "step": 3133
    },
    {
      "epoch": 3.9670886075949365,
      "grad_norm": 1.3375670909881592,
      "learning_rate": 5e-05,
      "loss": 1.0212,
      "step": 3134
    },
    {
      "epoch": 3.9683544303797467,
      "grad_norm": 1.3434556722640991,
      "learning_rate": 5e-05,
      "loss": 1.068,
      "step": 3135
    },
    {
      "epoch": 3.969620253164557,
      "grad_norm": 1.3175021409988403,
      "learning_rate": 5e-05,
      "loss": 0.9841,
      "step": 3136
    },
    {
      "epoch": 3.970886075949367,
      "grad_norm": 1.3569071292877197,
      "learning_rate": 5e-05,
      "loss": 1.0672,
      "step": 3137
    },
    {
      "epoch": 3.972151898734177,
      "grad_norm": 1.3637664318084717,
      "learning_rate": 5e-05,
      "loss": 1.0747,
      "step": 3138
    },
    {
      "epoch": 3.973417721518987,
      "grad_norm": 1.2966312170028687,
      "learning_rate": 5e-05,
      "loss": 0.9763,
      "step": 3139
    },
    {
      "epoch": 3.9746835443037973,
      "grad_norm": 1.380767583847046,
      "learning_rate": 5e-05,
      "loss": 1.0666,
      "step": 3140
    },
    {
      "epoch": 3.9759493670886075,
      "grad_norm": 1.2992894649505615,
      "learning_rate": 5e-05,
      "loss": 1.0295,
      "step": 3141
    },
    {
      "epoch": 3.9772151898734176,
      "grad_norm": 1.3405632972717285,
      "learning_rate": 5e-05,
      "loss": 1.0318,
      "step": 3142
    },
    {
      "epoch": 3.9784810126582277,
      "grad_norm": 1.3239126205444336,
      "learning_rate": 5e-05,
      "loss": 1.0693,
      "step": 3143
    },
    {
      "epoch": 3.979746835443038,
      "grad_norm": 1.3439713716506958,
      "learning_rate": 5e-05,
      "loss": 1.0684,
      "step": 3144
    },
    {
      "epoch": 3.981012658227848,
      "grad_norm": 1.3486207723617554,
      "learning_rate": 5e-05,
      "loss": 1.0645,
      "step": 3145
    },
    {
      "epoch": 3.982278481012658,
      "grad_norm": 1.2993626594543457,
      "learning_rate": 5e-05,
      "loss": 1.0091,
      "step": 3146
    },
    {
      "epoch": 3.9835443037974683,
      "grad_norm": 1.3285484313964844,
      "learning_rate": 5e-05,
      "loss": 1.074,
      "step": 3147
    },
    {
      "epoch": 3.9848101265822784,
      "grad_norm": 1.3718754053115845,
      "learning_rate": 5e-05,
      "loss": 0.9817,
      "step": 3148
    },
    {
      "epoch": 3.9860759493670885,
      "grad_norm": 1.4006211757659912,
      "learning_rate": 5e-05,
      "loss": 1.112,
      "step": 3149
    },
    {
      "epoch": 3.9873417721518987,
      "grad_norm": 1.3519216775894165,
      "learning_rate": 5e-05,
      "loss": 1.0193,
      "step": 3150
    },
    {
      "epoch": 3.988607594936709,
      "grad_norm": 1.300922155380249,
      "learning_rate": 5e-05,
      "loss": 1.0482,
      "step": 3151
    },
    {
      "epoch": 3.989873417721519,
      "grad_norm": 1.3844761848449707,
      "learning_rate": 5e-05,
      "loss": 1.0896,
      "step": 3152
    },
    {
      "epoch": 3.991139240506329,
      "grad_norm": 1.3414052724838257,
      "learning_rate": 5e-05,
      "loss": 1.0666,
      "step": 3153
    },
    {
      "epoch": 3.992405063291139,
      "grad_norm": 1.325451374053955,
      "learning_rate": 5e-05,
      "loss": 0.9832,
      "step": 3154
    },
    {
      "epoch": 3.9936708860759493,
      "grad_norm": 1.3343095779418945,
      "learning_rate": 5e-05,
      "loss": 1.0615,
      "step": 3155
    },
    {
      "epoch": 3.9949367088607595,
      "grad_norm": 1.3858792781829834,
      "learning_rate": 5e-05,
      "loss": 1.0461,
      "step": 3156
    },
    {
      "epoch": 3.9962025316455696,
      "grad_norm": 1.2874780893325806,
      "learning_rate": 5e-05,
      "loss": 1.0076,
      "step": 3157
    },
    {
      "epoch": 3.9974683544303797,
      "grad_norm": 1.3102537393569946,
      "learning_rate": 5e-05,
      "loss": 1.1006,
      "step": 3158
    },
    {
      "epoch": 3.99873417721519,
      "grad_norm": 1.3615059852600098,
      "learning_rate": 5e-05,
      "loss": 1.0427,
      "step": 3159
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.303972840309143,
      "learning_rate": 5e-05,
      "loss": 0.9488,
      "step": 3160
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9367564916610718,
      "eval_runtime": 163.5176,
      "eval_samples_per_second": 549.666,
      "eval_steps_per_second": 4.299,
      "step": 3160
    },
    {
      "epoch": 4.00126582278481,
      "grad_norm": 1.3076084852218628,
      "learning_rate": 5e-05,
      "loss": 0.9266,
      "step": 3161
    },
    {
      "epoch": 4.00253164556962,
      "grad_norm": 1.2713782787322998,
      "learning_rate": 5e-05,
      "loss": 0.9639,
      "step": 3162
    },
    {
      "epoch": 4.00379746835443,
      "grad_norm": 1.2555497884750366,
      "learning_rate": 5e-05,
      "loss": 0.9918,
      "step": 3163
    },
    {
      "epoch": 4.0050632911392405,
      "grad_norm": 1.2717732191085815,
      "learning_rate": 5e-05,
      "loss": 0.9574,
      "step": 3164
    },
    {
      "epoch": 4.006329113924051,
      "grad_norm": 1.3202571868896484,
      "learning_rate": 5e-05,
      "loss": 0.9827,
      "step": 3165
    },
    {
      "epoch": 4.007594936708861,
      "grad_norm": 1.288995623588562,
      "learning_rate": 5e-05,
      "loss": 0.964,
      "step": 3166
    },
    {
      "epoch": 4.008860759493671,
      "grad_norm": 1.2773513793945312,
      "learning_rate": 5e-05,
      "loss": 0.9576,
      "step": 3167
    },
    {
      "epoch": 4.010126582278481,
      "grad_norm": 1.2644785642623901,
      "learning_rate": 5e-05,
      "loss": 0.899,
      "step": 3168
    },
    {
      "epoch": 4.011392405063291,
      "grad_norm": 1.3532837629318237,
      "learning_rate": 5e-05,
      "loss": 1.006,
      "step": 3169
    },
    {
      "epoch": 4.012658227848101,
      "grad_norm": 1.2855464220046997,
      "learning_rate": 5e-05,
      "loss": 1.0118,
      "step": 3170
    },
    {
      "epoch": 4.0139240506329115,
      "grad_norm": 1.2515485286712646,
      "learning_rate": 5e-05,
      "loss": 0.9645,
      "step": 3171
    },
    {
      "epoch": 4.015189873417722,
      "grad_norm": 1.2921847105026245,
      "learning_rate": 5e-05,
      "loss": 0.9949,
      "step": 3172
    },
    {
      "epoch": 4.016455696202532,
      "grad_norm": 1.3258168697357178,
      "learning_rate": 5e-05,
      "loss": 1.026,
      "step": 3173
    },
    {
      "epoch": 4.017721518987342,
      "grad_norm": 1.3128012418746948,
      "learning_rate": 5e-05,
      "loss": 0.9716,
      "step": 3174
    },
    {
      "epoch": 4.018987341772152,
      "grad_norm": 1.2642040252685547,
      "learning_rate": 5e-05,
      "loss": 0.9449,
      "step": 3175
    },
    {
      "epoch": 4.020253164556962,
      "grad_norm": 1.2350050210952759,
      "learning_rate": 5e-05,
      "loss": 0.9405,
      "step": 3176
    },
    {
      "epoch": 4.021518987341772,
      "grad_norm": 1.3021975755691528,
      "learning_rate": 5e-05,
      "loss": 1.0017,
      "step": 3177
    },
    {
      "epoch": 4.022784810126582,
      "grad_norm": 1.2426081895828247,
      "learning_rate": 5e-05,
      "loss": 0.9128,
      "step": 3178
    },
    {
      "epoch": 4.0240506329113925,
      "grad_norm": 1.337669849395752,
      "learning_rate": 5e-05,
      "loss": 0.9461,
      "step": 3179
    },
    {
      "epoch": 4.025316455696203,
      "grad_norm": 1.2860543727874756,
      "learning_rate": 5e-05,
      "loss": 0.9336,
      "step": 3180
    },
    {
      "epoch": 4.026582278481013,
      "grad_norm": 1.3381632566452026,
      "learning_rate": 5e-05,
      "loss": 1.05,
      "step": 3181
    },
    {
      "epoch": 4.027848101265823,
      "grad_norm": 1.2964798212051392,
      "learning_rate": 5e-05,
      "loss": 0.9417,
      "step": 3182
    },
    {
      "epoch": 4.029113924050633,
      "grad_norm": 1.288173794746399,
      "learning_rate": 5e-05,
      "loss": 0.9161,
      "step": 3183
    },
    {
      "epoch": 4.030379746835443,
      "grad_norm": 1.2806001901626587,
      "learning_rate": 5e-05,
      "loss": 0.972,
      "step": 3184
    },
    {
      "epoch": 4.031645569620253,
      "grad_norm": 1.2823182344436646,
      "learning_rate": 5e-05,
      "loss": 0.9615,
      "step": 3185
    },
    {
      "epoch": 4.0329113924050635,
      "grad_norm": 1.3267234563827515,
      "learning_rate": 5e-05,
      "loss": 0.9575,
      "step": 3186
    },
    {
      "epoch": 4.034177215189874,
      "grad_norm": 1.3027775287628174,
      "learning_rate": 5e-05,
      "loss": 0.9555,
      "step": 3187
    },
    {
      "epoch": 4.035443037974684,
      "grad_norm": 1.3528634309768677,
      "learning_rate": 5e-05,
      "loss": 0.9907,
      "step": 3188
    },
    {
      "epoch": 4.036708860759494,
      "grad_norm": 1.276301383972168,
      "learning_rate": 5e-05,
      "loss": 0.9651,
      "step": 3189
    },
    {
      "epoch": 4.037974683544304,
      "grad_norm": 1.3116724491119385,
      "learning_rate": 5e-05,
      "loss": 0.9467,
      "step": 3190
    },
    {
      "epoch": 4.039240506329114,
      "grad_norm": 1.2747840881347656,
      "learning_rate": 5e-05,
      "loss": 0.9198,
      "step": 3191
    },
    {
      "epoch": 4.040506329113924,
      "grad_norm": 1.2717304229736328,
      "learning_rate": 5e-05,
      "loss": 0.8864,
      "step": 3192
    },
    {
      "epoch": 4.041772151898734,
      "grad_norm": 1.34334397315979,
      "learning_rate": 5e-05,
      "loss": 1.0118,
      "step": 3193
    },
    {
      "epoch": 4.0430379746835445,
      "grad_norm": 1.3137552738189697,
      "learning_rate": 5e-05,
      "loss": 0.9824,
      "step": 3194
    },
    {
      "epoch": 4.044303797468355,
      "grad_norm": 1.358941912651062,
      "learning_rate": 5e-05,
      "loss": 0.9442,
      "step": 3195
    },
    {
      "epoch": 4.045569620253165,
      "grad_norm": 1.261383295059204,
      "learning_rate": 5e-05,
      "loss": 0.9313,
      "step": 3196
    },
    {
      "epoch": 4.046835443037975,
      "grad_norm": 1.296220064163208,
      "learning_rate": 5e-05,
      "loss": 0.9514,
      "step": 3197
    },
    {
      "epoch": 4.048101265822785,
      "grad_norm": 1.327157735824585,
      "learning_rate": 5e-05,
      "loss": 0.9778,
      "step": 3198
    },
    {
      "epoch": 4.049367088607595,
      "grad_norm": 1.323136806488037,
      "learning_rate": 5e-05,
      "loss": 0.9784,
      "step": 3199
    },
    {
      "epoch": 4.050632911392405,
      "grad_norm": 1.3069449663162231,
      "learning_rate": 5e-05,
      "loss": 0.9805,
      "step": 3200
    },
    {
      "epoch": 4.0518987341772155,
      "grad_norm": 1.2656375169754028,
      "learning_rate": 5e-05,
      "loss": 0.9464,
      "step": 3201
    },
    {
      "epoch": 4.053164556962026,
      "grad_norm": 1.3016598224639893,
      "learning_rate": 5e-05,
      "loss": 1.0233,
      "step": 3202
    },
    {
      "epoch": 4.054430379746836,
      "grad_norm": 1.2899539470672607,
      "learning_rate": 5e-05,
      "loss": 0.9676,
      "step": 3203
    },
    {
      "epoch": 4.055696202531646,
      "grad_norm": 1.3159735202789307,
      "learning_rate": 5e-05,
      "loss": 0.9503,
      "step": 3204
    },
    {
      "epoch": 4.056962025316456,
      "grad_norm": 1.3094019889831543,
      "learning_rate": 5e-05,
      "loss": 0.9443,
      "step": 3205
    },
    {
      "epoch": 4.058227848101266,
      "grad_norm": 1.3422017097473145,
      "learning_rate": 5e-05,
      "loss": 1.017,
      "step": 3206
    },
    {
      "epoch": 4.059493670886076,
      "grad_norm": 1.3772350549697876,
      "learning_rate": 5e-05,
      "loss": 0.999,
      "step": 3207
    },
    {
      "epoch": 4.060759493670886,
      "grad_norm": 1.339079737663269,
      "learning_rate": 5e-05,
      "loss": 0.9956,
      "step": 3208
    },
    {
      "epoch": 4.0620253164556965,
      "grad_norm": 1.294019341468811,
      "learning_rate": 5e-05,
      "loss": 0.9393,
      "step": 3209
    },
    {
      "epoch": 4.063291139240507,
      "grad_norm": 1.3533176183700562,
      "learning_rate": 5e-05,
      "loss": 0.9871,
      "step": 3210
    },
    {
      "epoch": 4.064556962025317,
      "grad_norm": 1.3056252002716064,
      "learning_rate": 5e-05,
      "loss": 0.9943,
      "step": 3211
    },
    {
      "epoch": 4.065822784810127,
      "grad_norm": 1.327755093574524,
      "learning_rate": 5e-05,
      "loss": 0.9839,
      "step": 3212
    },
    {
      "epoch": 4.067088607594937,
      "grad_norm": 1.2704814672470093,
      "learning_rate": 5e-05,
      "loss": 0.9357,
      "step": 3213
    },
    {
      "epoch": 4.068354430379747,
      "grad_norm": 1.3207179307937622,
      "learning_rate": 5e-05,
      "loss": 0.9746,
      "step": 3214
    },
    {
      "epoch": 4.069620253164557,
      "grad_norm": 1.3043981790542603,
      "learning_rate": 5e-05,
      "loss": 1.0026,
      "step": 3215
    },
    {
      "epoch": 4.0708860759493675,
      "grad_norm": 1.3221186399459839,
      "learning_rate": 5e-05,
      "loss": 0.9589,
      "step": 3216
    },
    {
      "epoch": 4.072151898734178,
      "grad_norm": 1.3483505249023438,
      "learning_rate": 5e-05,
      "loss": 1.0515,
      "step": 3217
    },
    {
      "epoch": 4.073417721518988,
      "grad_norm": 1.2880324125289917,
      "learning_rate": 5e-05,
      "loss": 0.9529,
      "step": 3218
    },
    {
      "epoch": 4.074683544303798,
      "grad_norm": 1.289208173751831,
      "learning_rate": 5e-05,
      "loss": 0.9282,
      "step": 3219
    },
    {
      "epoch": 4.075949367088608,
      "grad_norm": 1.3006080389022827,
      "learning_rate": 5e-05,
      "loss": 0.9692,
      "step": 3220
    },
    {
      "epoch": 4.077215189873418,
      "grad_norm": 1.3379149436950684,
      "learning_rate": 5e-05,
      "loss": 0.9747,
      "step": 3221
    },
    {
      "epoch": 4.078481012658228,
      "grad_norm": 1.3636868000030518,
      "learning_rate": 5e-05,
      "loss": 0.9685,
      "step": 3222
    },
    {
      "epoch": 4.079746835443038,
      "grad_norm": 1.3286807537078857,
      "learning_rate": 5e-05,
      "loss": 0.9974,
      "step": 3223
    },
    {
      "epoch": 4.0810126582278485,
      "grad_norm": 1.3099489212036133,
      "learning_rate": 5e-05,
      "loss": 0.9507,
      "step": 3224
    },
    {
      "epoch": 4.082278481012658,
      "grad_norm": 1.316085696220398,
      "learning_rate": 5e-05,
      "loss": 0.9657,
      "step": 3225
    },
    {
      "epoch": 4.083544303797469,
      "grad_norm": 1.333800196647644,
      "learning_rate": 5e-05,
      "loss": 1.0098,
      "step": 3226
    },
    {
      "epoch": 4.084810126582278,
      "grad_norm": 1.3504281044006348,
      "learning_rate": 5e-05,
      "loss": 0.9925,
      "step": 3227
    },
    {
      "epoch": 4.086075949367088,
      "grad_norm": 1.3682358264923096,
      "learning_rate": 5e-05,
      "loss": 1.0142,
      "step": 3228
    },
    {
      "epoch": 4.087341772151898,
      "grad_norm": 1.333715796470642,
      "learning_rate": 5e-05,
      "loss": 0.9473,
      "step": 3229
    },
    {
      "epoch": 4.0886075949367084,
      "grad_norm": 1.3338630199432373,
      "learning_rate": 5e-05,
      "loss": 0.995,
      "step": 3230
    },
    {
      "epoch": 4.089873417721519,
      "grad_norm": 1.2831010818481445,
      "learning_rate": 5e-05,
      "loss": 0.9818,
      "step": 3231
    },
    {
      "epoch": 4.091139240506329,
      "grad_norm": 1.3522692918777466,
      "learning_rate": 5e-05,
      "loss": 0.988,
      "step": 3232
    },
    {
      "epoch": 4.092405063291139,
      "grad_norm": 1.307986855506897,
      "learning_rate": 5e-05,
      "loss": 0.9858,
      "step": 3233
    },
    {
      "epoch": 4.093670886075949,
      "grad_norm": 1.3092494010925293,
      "learning_rate": 5e-05,
      "loss": 0.9904,
      "step": 3234
    },
    {
      "epoch": 4.094936708860759,
      "grad_norm": 1.2728692293167114,
      "learning_rate": 5e-05,
      "loss": 0.9313,
      "step": 3235
    },
    {
      "epoch": 4.096202531645569,
      "grad_norm": 1.3250788450241089,
      "learning_rate": 5e-05,
      "loss": 1.0366,
      "step": 3236
    },
    {
      "epoch": 4.097468354430379,
      "grad_norm": 1.3099180459976196,
      "learning_rate": 5e-05,
      "loss": 0.9962,
      "step": 3237
    },
    {
      "epoch": 4.0987341772151895,
      "grad_norm": 1.346791386604309,
      "learning_rate": 5e-05,
      "loss": 0.9853,
      "step": 3238
    },
    {
      "epoch": 4.1,
      "grad_norm": 1.2960734367370605,
      "learning_rate": 5e-05,
      "loss": 0.9166,
      "step": 3239
    },
    {
      "epoch": 4.10126582278481,
      "grad_norm": 1.361985445022583,
      "learning_rate": 5e-05,
      "loss": 0.9986,
      "step": 3240
    },
    {
      "epoch": 4.10253164556962,
      "grad_norm": 1.3093643188476562,
      "learning_rate": 5e-05,
      "loss": 0.9356,
      "step": 3241
    },
    {
      "epoch": 4.10379746835443,
      "grad_norm": 1.3159818649291992,
      "learning_rate": 5e-05,
      "loss": 0.969,
      "step": 3242
    },
    {
      "epoch": 4.10506329113924,
      "grad_norm": 1.279549241065979,
      "learning_rate": 5e-05,
      "loss": 0.9289,
      "step": 3243
    },
    {
      "epoch": 4.10632911392405,
      "grad_norm": 1.3195892572402954,
      "learning_rate": 5e-05,
      "loss": 1.0365,
      "step": 3244
    },
    {
      "epoch": 4.1075949367088604,
      "grad_norm": 1.3084145784378052,
      "learning_rate": 5e-05,
      "loss": 1.0282,
      "step": 3245
    },
    {
      "epoch": 4.108860759493671,
      "grad_norm": 1.3030469417572021,
      "learning_rate": 5e-05,
      "loss": 0.9512,
      "step": 3246
    },
    {
      "epoch": 4.110126582278481,
      "grad_norm": 1.320663571357727,
      "learning_rate": 5e-05,
      "loss": 0.9612,
      "step": 3247
    },
    {
      "epoch": 4.111392405063291,
      "grad_norm": 1.2864766120910645,
      "learning_rate": 5e-05,
      "loss": 0.9555,
      "step": 3248
    },
    {
      "epoch": 4.112658227848101,
      "grad_norm": 1.316870927810669,
      "learning_rate": 5e-05,
      "loss": 0.9839,
      "step": 3249
    },
    {
      "epoch": 4.113924050632911,
      "grad_norm": 1.2746608257293701,
      "learning_rate": 5e-05,
      "loss": 1.0232,
      "step": 3250
    },
    {
      "epoch": 4.115189873417721,
      "grad_norm": 1.2467868328094482,
      "learning_rate": 5e-05,
      "loss": 0.9825,
      "step": 3251
    },
    {
      "epoch": 4.116455696202531,
      "grad_norm": 1.3513915538787842,
      "learning_rate": 5e-05,
      "loss": 1.0283,
      "step": 3252
    },
    {
      "epoch": 4.1177215189873415,
      "grad_norm": 1.3006746768951416,
      "learning_rate": 5e-05,
      "loss": 1.0311,
      "step": 3253
    },
    {
      "epoch": 4.118987341772152,
      "grad_norm": 1.3188623189926147,
      "learning_rate": 5e-05,
      "loss": 0.9175,
      "step": 3254
    },
    {
      "epoch": 4.120253164556962,
      "grad_norm": 1.3084309101104736,
      "learning_rate": 5e-05,
      "loss": 0.9842,
      "step": 3255
    },
    {
      "epoch": 4.121518987341772,
      "grad_norm": 1.3220739364624023,
      "learning_rate": 5e-05,
      "loss": 0.9219,
      "step": 3256
    },
    {
      "epoch": 4.122784810126582,
      "grad_norm": 1.3375163078308105,
      "learning_rate": 5e-05,
      "loss": 1.0118,
      "step": 3257
    },
    {
      "epoch": 4.124050632911392,
      "grad_norm": 1.3246525526046753,
      "learning_rate": 5e-05,
      "loss": 0.9674,
      "step": 3258
    },
    {
      "epoch": 4.125316455696202,
      "grad_norm": 1.369225263595581,
      "learning_rate": 5e-05,
      "loss": 1.0193,
      "step": 3259
    },
    {
      "epoch": 4.1265822784810124,
      "grad_norm": 1.3156241178512573,
      "learning_rate": 5e-05,
      "loss": 0.9349,
      "step": 3260
    },
    {
      "epoch": 4.127848101265823,
      "grad_norm": 1.3167288303375244,
      "learning_rate": 5e-05,
      "loss": 1.0151,
      "step": 3261
    },
    {
      "epoch": 4.129113924050633,
      "grad_norm": 1.3383352756500244,
      "learning_rate": 5e-05,
      "loss": 0.9666,
      "step": 3262
    },
    {
      "epoch": 4.130379746835443,
      "grad_norm": 1.2760199308395386,
      "learning_rate": 5e-05,
      "loss": 0.9047,
      "step": 3263
    },
    {
      "epoch": 4.131645569620253,
      "grad_norm": 1.329135775566101,
      "learning_rate": 5e-05,
      "loss": 0.9345,
      "step": 3264
    },
    {
      "epoch": 4.132911392405063,
      "grad_norm": 1.3184102773666382,
      "learning_rate": 5e-05,
      "loss": 1.0034,
      "step": 3265
    },
    {
      "epoch": 4.134177215189873,
      "grad_norm": 1.3498286008834839,
      "learning_rate": 5e-05,
      "loss": 0.9705,
      "step": 3266
    },
    {
      "epoch": 4.135443037974683,
      "grad_norm": 1.3230798244476318,
      "learning_rate": 5e-05,
      "loss": 1.0104,
      "step": 3267
    },
    {
      "epoch": 4.1367088607594935,
      "grad_norm": 1.3204338550567627,
      "learning_rate": 5e-05,
      "loss": 1.0077,
      "step": 3268
    },
    {
      "epoch": 4.137974683544304,
      "grad_norm": 1.3513076305389404,
      "learning_rate": 5e-05,
      "loss": 1.0101,
      "step": 3269
    },
    {
      "epoch": 4.139240506329114,
      "grad_norm": 1.3456660509109497,
      "learning_rate": 5e-05,
      "loss": 0.9922,
      "step": 3270
    },
    {
      "epoch": 4.140506329113924,
      "grad_norm": 1.338684320449829,
      "learning_rate": 5e-05,
      "loss": 1.0019,
      "step": 3271
    },
    {
      "epoch": 4.141772151898734,
      "grad_norm": 1.3318848609924316,
      "learning_rate": 5e-05,
      "loss": 0.9862,
      "step": 3272
    },
    {
      "epoch": 4.143037974683544,
      "grad_norm": 1.3455474376678467,
      "learning_rate": 5e-05,
      "loss": 1.0177,
      "step": 3273
    },
    {
      "epoch": 4.144303797468354,
      "grad_norm": 1.360800862312317,
      "learning_rate": 5e-05,
      "loss": 1.0338,
      "step": 3274
    },
    {
      "epoch": 4.1455696202531644,
      "grad_norm": 1.3161696195602417,
      "learning_rate": 5e-05,
      "loss": 0.9229,
      "step": 3275
    },
    {
      "epoch": 4.146835443037975,
      "grad_norm": 1.3772588968276978,
      "learning_rate": 5e-05,
      "loss": 1.0179,
      "step": 3276
    },
    {
      "epoch": 4.148101265822785,
      "grad_norm": 1.3452694416046143,
      "learning_rate": 5e-05,
      "loss": 1.0095,
      "step": 3277
    },
    {
      "epoch": 4.149367088607595,
      "grad_norm": 1.3471976518630981,
      "learning_rate": 5e-05,
      "loss": 1.0039,
      "step": 3278
    },
    {
      "epoch": 4.150632911392405,
      "grad_norm": 1.330378532409668,
      "learning_rate": 5e-05,
      "loss": 0.9508,
      "step": 3279
    },
    {
      "epoch": 4.151898734177215,
      "grad_norm": 1.288615345954895,
      "learning_rate": 5e-05,
      "loss": 0.8777,
      "step": 3280
    },
    {
      "epoch": 4.153164556962025,
      "grad_norm": 1.3598508834838867,
      "learning_rate": 5e-05,
      "loss": 1.0105,
      "step": 3281
    },
    {
      "epoch": 4.154430379746835,
      "grad_norm": 1.2966585159301758,
      "learning_rate": 5e-05,
      "loss": 0.9469,
      "step": 3282
    },
    {
      "epoch": 4.1556962025316455,
      "grad_norm": 1.3111321926116943,
      "learning_rate": 5e-05,
      "loss": 0.8774,
      "step": 3283
    },
    {
      "epoch": 4.156962025316456,
      "grad_norm": 1.330786943435669,
      "learning_rate": 5e-05,
      "loss": 0.9226,
      "step": 3284
    },
    {
      "epoch": 4.158227848101266,
      "grad_norm": 1.252934217453003,
      "learning_rate": 5e-05,
      "loss": 0.9053,
      "step": 3285
    },
    {
      "epoch": 4.159493670886076,
      "grad_norm": 1.3187779188156128,
      "learning_rate": 5e-05,
      "loss": 0.9585,
      "step": 3286
    },
    {
      "epoch": 4.160759493670886,
      "grad_norm": 1.2330988645553589,
      "learning_rate": 5e-05,
      "loss": 0.9005,
      "step": 3287
    },
    {
      "epoch": 4.162025316455696,
      "grad_norm": 1.3084245920181274,
      "learning_rate": 5e-05,
      "loss": 0.9819,
      "step": 3288
    },
    {
      "epoch": 4.163291139240506,
      "grad_norm": 1.3532778024673462,
      "learning_rate": 5e-05,
      "loss": 0.9675,
      "step": 3289
    },
    {
      "epoch": 4.1645569620253164,
      "grad_norm": 1.3214606046676636,
      "learning_rate": 5e-05,
      "loss": 0.9754,
      "step": 3290
    },
    {
      "epoch": 4.165822784810127,
      "grad_norm": 1.2900935411453247,
      "learning_rate": 5e-05,
      "loss": 0.9665,
      "step": 3291
    },
    {
      "epoch": 4.167088607594937,
      "grad_norm": 1.2897828817367554,
      "learning_rate": 5e-05,
      "loss": 0.927,
      "step": 3292
    },
    {
      "epoch": 4.168354430379747,
      "grad_norm": 1.2864774465560913,
      "learning_rate": 5e-05,
      "loss": 0.9123,
      "step": 3293
    },
    {
      "epoch": 4.169620253164557,
      "grad_norm": 1.3040778636932373,
      "learning_rate": 5e-05,
      "loss": 0.9795,
      "step": 3294
    },
    {
      "epoch": 4.170886075949367,
      "grad_norm": 1.30279541015625,
      "learning_rate": 5e-05,
      "loss": 0.9762,
      "step": 3295
    },
    {
      "epoch": 4.172151898734177,
      "grad_norm": 1.2888298034667969,
      "learning_rate": 5e-05,
      "loss": 0.9084,
      "step": 3296
    },
    {
      "epoch": 4.173417721518987,
      "grad_norm": 1.3586454391479492,
      "learning_rate": 5e-05,
      "loss": 1.0286,
      "step": 3297
    },
    {
      "epoch": 4.1746835443037975,
      "grad_norm": 1.3210785388946533,
      "learning_rate": 5e-05,
      "loss": 0.9943,
      "step": 3298
    },
    {
      "epoch": 4.175949367088608,
      "grad_norm": 1.2889043092727661,
      "learning_rate": 5e-05,
      "loss": 0.9345,
      "step": 3299
    },
    {
      "epoch": 4.177215189873418,
      "grad_norm": 1.261548399925232,
      "learning_rate": 5e-05,
      "loss": 0.8925,
      "step": 3300
    },
    {
      "epoch": 4.178481012658228,
      "grad_norm": 1.2638944387435913,
      "learning_rate": 5e-05,
      "loss": 0.8113,
      "step": 3301
    },
    {
      "epoch": 4.179746835443038,
      "grad_norm": 1.3361490964889526,
      "learning_rate": 5e-05,
      "loss": 0.9428,
      "step": 3302
    },
    {
      "epoch": 4.181012658227848,
      "grad_norm": 1.304917335510254,
      "learning_rate": 5e-05,
      "loss": 0.9556,
      "step": 3303
    },
    {
      "epoch": 4.182278481012658,
      "grad_norm": 1.349763035774231,
      "learning_rate": 5e-05,
      "loss": 0.921,
      "step": 3304
    },
    {
      "epoch": 4.1835443037974684,
      "grad_norm": 1.3418760299682617,
      "learning_rate": 5e-05,
      "loss": 0.9871,
      "step": 3305
    },
    {
      "epoch": 4.184810126582279,
      "grad_norm": 1.2861119508743286,
      "learning_rate": 5e-05,
      "loss": 0.8913,
      "step": 3306
    },
    {
      "epoch": 4.186075949367089,
      "grad_norm": 1.328338384628296,
      "learning_rate": 5e-05,
      "loss": 0.9822,
      "step": 3307
    },
    {
      "epoch": 4.187341772151899,
      "grad_norm": 1.2913591861724854,
      "learning_rate": 5e-05,
      "loss": 0.9227,
      "step": 3308
    },
    {
      "epoch": 4.188607594936709,
      "grad_norm": 1.3420268297195435,
      "learning_rate": 5e-05,
      "loss": 0.9489,
      "step": 3309
    },
    {
      "epoch": 4.189873417721519,
      "grad_norm": 1.3054924011230469,
      "learning_rate": 5e-05,
      "loss": 0.9698,
      "step": 3310
    },
    {
      "epoch": 4.191139240506329,
      "grad_norm": 1.3298490047454834,
      "learning_rate": 5e-05,
      "loss": 1.0074,
      "step": 3311
    },
    {
      "epoch": 4.192405063291139,
      "grad_norm": 1.3753472566604614,
      "learning_rate": 5e-05,
      "loss": 0.9925,
      "step": 3312
    },
    {
      "epoch": 4.1936708860759495,
      "grad_norm": 1.2624844312667847,
      "learning_rate": 5e-05,
      "loss": 0.8805,
      "step": 3313
    },
    {
      "epoch": 4.19493670886076,
      "grad_norm": 1.2734969854354858,
      "learning_rate": 5e-05,
      "loss": 0.9506,
      "step": 3314
    },
    {
      "epoch": 4.19620253164557,
      "grad_norm": 1.3634469509124756,
      "learning_rate": 5e-05,
      "loss": 1.0147,
      "step": 3315
    },
    {
      "epoch": 4.19746835443038,
      "grad_norm": 1.3033592700958252,
      "learning_rate": 5e-05,
      "loss": 0.9496,
      "step": 3316
    },
    {
      "epoch": 4.19873417721519,
      "grad_norm": 1.357121467590332,
      "learning_rate": 5e-05,
      "loss": 0.992,
      "step": 3317
    },
    {
      "epoch": 4.2,
      "grad_norm": 1.3224434852600098,
      "learning_rate": 5e-05,
      "loss": 0.9717,
      "step": 3318
    },
    {
      "epoch": 4.20126582278481,
      "grad_norm": 1.295693278312683,
      "learning_rate": 5e-05,
      "loss": 0.9585,
      "step": 3319
    },
    {
      "epoch": 4.2025316455696204,
      "grad_norm": 1.3782575130462646,
      "learning_rate": 5e-05,
      "loss": 1.0229,
      "step": 3320
    },
    {
      "epoch": 4.203797468354431,
      "grad_norm": 1.3228641748428345,
      "learning_rate": 5e-05,
      "loss": 0.9922,
      "step": 3321
    },
    {
      "epoch": 4.205063291139241,
      "grad_norm": 1.3178821802139282,
      "learning_rate": 5e-05,
      "loss": 0.9781,
      "step": 3322
    },
    {
      "epoch": 4.206329113924051,
      "grad_norm": 1.3386563062667847,
      "learning_rate": 5e-05,
      "loss": 1.0048,
      "step": 3323
    },
    {
      "epoch": 4.207594936708861,
      "grad_norm": 1.2962366342544556,
      "learning_rate": 5e-05,
      "loss": 0.914,
      "step": 3324
    },
    {
      "epoch": 4.208860759493671,
      "grad_norm": 1.379634976387024,
      "learning_rate": 5e-05,
      "loss": 1.0214,
      "step": 3325
    },
    {
      "epoch": 4.210126582278481,
      "grad_norm": 1.3485008478164673,
      "learning_rate": 5e-05,
      "loss": 0.9967,
      "step": 3326
    },
    {
      "epoch": 4.211392405063291,
      "grad_norm": 1.3369712829589844,
      "learning_rate": 5e-05,
      "loss": 0.9759,
      "step": 3327
    },
    {
      "epoch": 4.2126582278481015,
      "grad_norm": 1.3211627006530762,
      "learning_rate": 5e-05,
      "loss": 0.9775,
      "step": 3328
    },
    {
      "epoch": 4.213924050632912,
      "grad_norm": 1.3339706659317017,
      "learning_rate": 5e-05,
      "loss": 0.9754,
      "step": 3329
    },
    {
      "epoch": 4.215189873417722,
      "grad_norm": 1.3242961168289185,
      "learning_rate": 5e-05,
      "loss": 0.9309,
      "step": 3330
    },
    {
      "epoch": 4.216455696202532,
      "grad_norm": 1.3019945621490479,
      "learning_rate": 5e-05,
      "loss": 0.9073,
      "step": 3331
    },
    {
      "epoch": 4.217721518987342,
      "grad_norm": 1.3074896335601807,
      "learning_rate": 5e-05,
      "loss": 0.9631,
      "step": 3332
    },
    {
      "epoch": 4.218987341772152,
      "grad_norm": 1.3330879211425781,
      "learning_rate": 5e-05,
      "loss": 0.9547,
      "step": 3333
    },
    {
      "epoch": 4.220253164556962,
      "grad_norm": 1.3151811361312866,
      "learning_rate": 5e-05,
      "loss": 0.9429,
      "step": 3334
    },
    {
      "epoch": 4.2215189873417724,
      "grad_norm": 1.328361988067627,
      "learning_rate": 5e-05,
      "loss": 0.9726,
      "step": 3335
    },
    {
      "epoch": 4.222784810126583,
      "grad_norm": 1.3137142658233643,
      "learning_rate": 5e-05,
      "loss": 0.9877,
      "step": 3336
    },
    {
      "epoch": 4.224050632911393,
      "grad_norm": 1.24952232837677,
      "learning_rate": 5e-05,
      "loss": 0.8804,
      "step": 3337
    },
    {
      "epoch": 4.225316455696203,
      "grad_norm": 1.3327347040176392,
      "learning_rate": 5e-05,
      "loss": 0.948,
      "step": 3338
    },
    {
      "epoch": 4.226582278481013,
      "grad_norm": 1.355431079864502,
      "learning_rate": 5e-05,
      "loss": 0.9918,
      "step": 3339
    },
    {
      "epoch": 4.227848101265823,
      "grad_norm": 1.400633454322815,
      "learning_rate": 5e-05,
      "loss": 1.0147,
      "step": 3340
    },
    {
      "epoch": 4.229113924050633,
      "grad_norm": 1.3509752750396729,
      "learning_rate": 5e-05,
      "loss": 0.9412,
      "step": 3341
    },
    {
      "epoch": 4.230379746835443,
      "grad_norm": 1.2867759466171265,
      "learning_rate": 5e-05,
      "loss": 0.8891,
      "step": 3342
    },
    {
      "epoch": 4.2316455696202535,
      "grad_norm": 1.319328784942627,
      "learning_rate": 5e-05,
      "loss": 0.9898,
      "step": 3343
    },
    {
      "epoch": 4.232911392405064,
      "grad_norm": 1.3142932653427124,
      "learning_rate": 5e-05,
      "loss": 0.96,
      "step": 3344
    },
    {
      "epoch": 4.234177215189874,
      "grad_norm": 1.289325475692749,
      "learning_rate": 5e-05,
      "loss": 0.9191,
      "step": 3345
    },
    {
      "epoch": 4.235443037974684,
      "grad_norm": 1.298516035079956,
      "learning_rate": 5e-05,
      "loss": 0.8983,
      "step": 3346
    },
    {
      "epoch": 4.236708860759494,
      "grad_norm": 1.3130265474319458,
      "learning_rate": 5e-05,
      "loss": 0.926,
      "step": 3347
    },
    {
      "epoch": 4.237974683544304,
      "grad_norm": 1.271082878112793,
      "learning_rate": 5e-05,
      "loss": 0.9699,
      "step": 3348
    },
    {
      "epoch": 4.239240506329114,
      "grad_norm": 1.3640371561050415,
      "learning_rate": 5e-05,
      "loss": 0.9854,
      "step": 3349
    },
    {
      "epoch": 4.2405063291139244,
      "grad_norm": 1.344606637954712,
      "learning_rate": 5e-05,
      "loss": 0.9372,
      "step": 3350
    },
    {
      "epoch": 4.241772151898735,
      "grad_norm": 1.3128360509872437,
      "learning_rate": 5e-05,
      "loss": 0.929,
      "step": 3351
    },
    {
      "epoch": 4.243037974683545,
      "grad_norm": 1.3038386106491089,
      "learning_rate": 5e-05,
      "loss": 0.9564,
      "step": 3352
    },
    {
      "epoch": 4.244303797468355,
      "grad_norm": 1.289951205253601,
      "learning_rate": 5e-05,
      "loss": 0.8962,
      "step": 3353
    },
    {
      "epoch": 4.245569620253165,
      "grad_norm": 1.3356273174285889,
      "learning_rate": 5e-05,
      "loss": 0.934,
      "step": 3354
    },
    {
      "epoch": 4.246835443037975,
      "grad_norm": 1.2591795921325684,
      "learning_rate": 5e-05,
      "loss": 0.9418,
      "step": 3355
    },
    {
      "epoch": 4.248101265822784,
      "grad_norm": 1.3222352266311646,
      "learning_rate": 5e-05,
      "loss": 0.9807,
      "step": 3356
    },
    {
      "epoch": 4.249367088607595,
      "grad_norm": 1.3132938146591187,
      "learning_rate": 5e-05,
      "loss": 0.9527,
      "step": 3357
    },
    {
      "epoch": 4.250632911392405,
      "grad_norm": 1.3343992233276367,
      "learning_rate": 5e-05,
      "loss": 0.9932,
      "step": 3358
    },
    {
      "epoch": 4.251898734177216,
      "grad_norm": 1.3543423414230347,
      "learning_rate": 5e-05,
      "loss": 0.965,
      "step": 3359
    },
    {
      "epoch": 4.253164556962025,
      "grad_norm": 1.369807481765747,
      "learning_rate": 5e-05,
      "loss": 0.9559,
      "step": 3360
    },
    {
      "epoch": 4.254430379746836,
      "grad_norm": 1.3211697340011597,
      "learning_rate": 5e-05,
      "loss": 0.9561,
      "step": 3361
    },
    {
      "epoch": 4.255696202531645,
      "grad_norm": 1.3159210681915283,
      "learning_rate": 5e-05,
      "loss": 0.9872,
      "step": 3362
    },
    {
      "epoch": 4.256962025316455,
      "grad_norm": 1.313340663909912,
      "learning_rate": 5e-05,
      "loss": 0.9318,
      "step": 3363
    },
    {
      "epoch": 4.258227848101265,
      "grad_norm": 1.2951993942260742,
      "learning_rate": 5e-05,
      "loss": 0.9045,
      "step": 3364
    },
    {
      "epoch": 4.2594936708860756,
      "grad_norm": 1.3275883197784424,
      "learning_rate": 5e-05,
      "loss": 0.9788,
      "step": 3365
    },
    {
      "epoch": 4.260759493670886,
      "grad_norm": 1.3373295068740845,
      "learning_rate": 5e-05,
      "loss": 0.9893,
      "step": 3366
    },
    {
      "epoch": 4.262025316455696,
      "grad_norm": 1.3444130420684814,
      "learning_rate": 5e-05,
      "loss": 1.0154,
      "step": 3367
    },
    {
      "epoch": 4.263291139240506,
      "grad_norm": 1.3823407888412476,
      "learning_rate": 5e-05,
      "loss": 0.9502,
      "step": 3368
    },
    {
      "epoch": 4.264556962025316,
      "grad_norm": 1.3459059000015259,
      "learning_rate": 5e-05,
      "loss": 0.9339,
      "step": 3369
    },
    {
      "epoch": 4.265822784810126,
      "grad_norm": 1.3237357139587402,
      "learning_rate": 5e-05,
      "loss": 0.9933,
      "step": 3370
    },
    {
      "epoch": 4.267088607594936,
      "grad_norm": 1.3224613666534424,
      "learning_rate": 5e-05,
      "loss": 1.0034,
      "step": 3371
    },
    {
      "epoch": 4.2683544303797465,
      "grad_norm": 1.3266987800598145,
      "learning_rate": 5e-05,
      "loss": 0.9432,
      "step": 3372
    },
    {
      "epoch": 4.269620253164557,
      "grad_norm": 1.325775146484375,
      "learning_rate": 5e-05,
      "loss": 0.9762,
      "step": 3373
    },
    {
      "epoch": 4.270886075949367,
      "grad_norm": 1.3028500080108643,
      "learning_rate": 5e-05,
      "loss": 0.9818,
      "step": 3374
    },
    {
      "epoch": 4.272151898734177,
      "grad_norm": 1.3279401063919067,
      "learning_rate": 5e-05,
      "loss": 0.9459,
      "step": 3375
    },
    {
      "epoch": 4.273417721518987,
      "grad_norm": 1.4058881998062134,
      "learning_rate": 5e-05,
      "loss": 0.9973,
      "step": 3376
    },
    {
      "epoch": 4.274683544303797,
      "grad_norm": 1.3487513065338135,
      "learning_rate": 5e-05,
      "loss": 1.0006,
      "step": 3377
    },
    {
      "epoch": 4.275949367088607,
      "grad_norm": 1.3713451623916626,
      "learning_rate": 5e-05,
      "loss": 0.9811,
      "step": 3378
    },
    {
      "epoch": 4.277215189873417,
      "grad_norm": 1.2556499242782593,
      "learning_rate": 5e-05,
      "loss": 0.9147,
      "step": 3379
    },
    {
      "epoch": 4.2784810126582276,
      "grad_norm": 1.3286471366882324,
      "learning_rate": 5e-05,
      "loss": 0.9389,
      "step": 3380
    },
    {
      "epoch": 4.279746835443038,
      "grad_norm": 1.3102880716323853,
      "learning_rate": 5e-05,
      "loss": 0.9893,
      "step": 3381
    },
    {
      "epoch": 4.281012658227848,
      "grad_norm": 1.326899528503418,
      "learning_rate": 5e-05,
      "loss": 0.9443,
      "step": 3382
    },
    {
      "epoch": 4.282278481012658,
      "grad_norm": 1.299869179725647,
      "learning_rate": 5e-05,
      "loss": 0.9018,
      "step": 3383
    },
    {
      "epoch": 4.283544303797468,
      "grad_norm": 1.3104866743087769,
      "learning_rate": 5e-05,
      "loss": 0.9308,
      "step": 3384
    },
    {
      "epoch": 4.284810126582278,
      "grad_norm": 1.2861297130584717,
      "learning_rate": 5e-05,
      "loss": 0.9693,
      "step": 3385
    },
    {
      "epoch": 4.286075949367088,
      "grad_norm": 1.278273105621338,
      "learning_rate": 5e-05,
      "loss": 0.9416,
      "step": 3386
    },
    {
      "epoch": 4.2873417721518985,
      "grad_norm": 1.3484570980072021,
      "learning_rate": 5e-05,
      "loss": 0.9693,
      "step": 3387
    },
    {
      "epoch": 4.288607594936709,
      "grad_norm": 1.3304444551467896,
      "learning_rate": 5e-05,
      "loss": 0.9538,
      "step": 3388
    },
    {
      "epoch": 4.289873417721519,
      "grad_norm": 1.3424572944641113,
      "learning_rate": 5e-05,
      "loss": 0.9929,
      "step": 3389
    },
    {
      "epoch": 4.291139240506329,
      "grad_norm": 1.325148105621338,
      "learning_rate": 5e-05,
      "loss": 0.9786,
      "step": 3390
    },
    {
      "epoch": 4.292405063291139,
      "grad_norm": 1.3189829587936401,
      "learning_rate": 5e-05,
      "loss": 0.956,
      "step": 3391
    },
    {
      "epoch": 4.293670886075949,
      "grad_norm": 1.2967240810394287,
      "learning_rate": 5e-05,
      "loss": 0.9307,
      "step": 3392
    },
    {
      "epoch": 4.294936708860759,
      "grad_norm": 1.3333534002304077,
      "learning_rate": 5e-05,
      "loss": 0.9577,
      "step": 3393
    },
    {
      "epoch": 4.296202531645569,
      "grad_norm": 1.3320534229278564,
      "learning_rate": 5e-05,
      "loss": 1.0053,
      "step": 3394
    },
    {
      "epoch": 4.2974683544303796,
      "grad_norm": 1.353606939315796,
      "learning_rate": 5e-05,
      "loss": 0.9469,
      "step": 3395
    },
    {
      "epoch": 4.29873417721519,
      "grad_norm": 1.2987070083618164,
      "learning_rate": 5e-05,
      "loss": 0.9131,
      "step": 3396
    },
    {
      "epoch": 4.3,
      "grad_norm": 1.2625702619552612,
      "learning_rate": 5e-05,
      "loss": 0.94,
      "step": 3397
    },
    {
      "epoch": 4.30126582278481,
      "grad_norm": 1.2701302766799927,
      "learning_rate": 5e-05,
      "loss": 0.9608,
      "step": 3398
    },
    {
      "epoch": 4.30253164556962,
      "grad_norm": 1.3349955081939697,
      "learning_rate": 5e-05,
      "loss": 0.9988,
      "step": 3399
    },
    {
      "epoch": 4.30379746835443,
      "grad_norm": 1.330049991607666,
      "learning_rate": 5e-05,
      "loss": 0.9493,
      "step": 3400
    },
    {
      "epoch": 4.30506329113924,
      "grad_norm": 1.330247402191162,
      "learning_rate": 5e-05,
      "loss": 0.9589,
      "step": 3401
    },
    {
      "epoch": 4.3063291139240505,
      "grad_norm": 1.297577977180481,
      "learning_rate": 5e-05,
      "loss": 0.9132,
      "step": 3402
    },
    {
      "epoch": 4.307594936708861,
      "grad_norm": 1.3414844274520874,
      "learning_rate": 5e-05,
      "loss": 0.966,
      "step": 3403
    },
    {
      "epoch": 4.308860759493671,
      "grad_norm": 1.3238698244094849,
      "learning_rate": 5e-05,
      "loss": 0.938,
      "step": 3404
    },
    {
      "epoch": 4.310126582278481,
      "grad_norm": 1.3107832670211792,
      "learning_rate": 5e-05,
      "loss": 0.9904,
      "step": 3405
    },
    {
      "epoch": 4.311392405063291,
      "grad_norm": 1.335050344467163,
      "learning_rate": 5e-05,
      "loss": 0.9637,
      "step": 3406
    },
    {
      "epoch": 4.312658227848101,
      "grad_norm": 1.324289321899414,
      "learning_rate": 5e-05,
      "loss": 0.9471,
      "step": 3407
    },
    {
      "epoch": 4.313924050632911,
      "grad_norm": 1.2956171035766602,
      "learning_rate": 5e-05,
      "loss": 0.9484,
      "step": 3408
    },
    {
      "epoch": 4.315189873417721,
      "grad_norm": 1.3575359582901,
      "learning_rate": 5e-05,
      "loss": 0.9548,
      "step": 3409
    },
    {
      "epoch": 4.3164556962025316,
      "grad_norm": 1.26581871509552,
      "learning_rate": 5e-05,
      "loss": 0.901,
      "step": 3410
    },
    {
      "epoch": 4.317721518987342,
      "grad_norm": 1.3277950286865234,
      "learning_rate": 5e-05,
      "loss": 0.9564,
      "step": 3411
    },
    {
      "epoch": 4.318987341772152,
      "grad_norm": 1.294472336769104,
      "learning_rate": 5e-05,
      "loss": 0.9094,
      "step": 3412
    },
    {
      "epoch": 4.320253164556962,
      "grad_norm": 1.3467706441879272,
      "learning_rate": 5e-05,
      "loss": 0.9646,
      "step": 3413
    },
    {
      "epoch": 4.321518987341772,
      "grad_norm": 1.3382508754730225,
      "learning_rate": 5e-05,
      "loss": 0.9511,
      "step": 3414
    },
    {
      "epoch": 4.322784810126582,
      "grad_norm": 1.333024263381958,
      "learning_rate": 5e-05,
      "loss": 0.9591,
      "step": 3415
    },
    {
      "epoch": 4.324050632911392,
      "grad_norm": 1.3288005590438843,
      "learning_rate": 5e-05,
      "loss": 0.9418,
      "step": 3416
    },
    {
      "epoch": 4.3253164556962025,
      "grad_norm": 1.2783808708190918,
      "learning_rate": 5e-05,
      "loss": 0.9094,
      "step": 3417
    },
    {
      "epoch": 4.326582278481013,
      "grad_norm": 1.3436384201049805,
      "learning_rate": 5e-05,
      "loss": 0.9627,
      "step": 3418
    },
    {
      "epoch": 4.327848101265823,
      "grad_norm": 1.3764597177505493,
      "learning_rate": 5e-05,
      "loss": 1.0695,
      "step": 3419
    },
    {
      "epoch": 4.329113924050633,
      "grad_norm": 1.3453091382980347,
      "learning_rate": 5e-05,
      "loss": 0.8905,
      "step": 3420
    },
    {
      "epoch": 4.330379746835443,
      "grad_norm": 1.3522117137908936,
      "learning_rate": 5e-05,
      "loss": 0.9759,
      "step": 3421
    },
    {
      "epoch": 4.331645569620253,
      "grad_norm": 1.3410791158676147,
      "learning_rate": 5e-05,
      "loss": 0.9758,
      "step": 3422
    },
    {
      "epoch": 4.332911392405063,
      "grad_norm": 1.320671558380127,
      "learning_rate": 5e-05,
      "loss": 0.9521,
      "step": 3423
    },
    {
      "epoch": 4.334177215189873,
      "grad_norm": 1.3104236125946045,
      "learning_rate": 5e-05,
      "loss": 0.976,
      "step": 3424
    },
    {
      "epoch": 4.3354430379746836,
      "grad_norm": 1.3406634330749512,
      "learning_rate": 5e-05,
      "loss": 0.9537,
      "step": 3425
    },
    {
      "epoch": 4.336708860759494,
      "grad_norm": 1.3248634338378906,
      "learning_rate": 5e-05,
      "loss": 0.9707,
      "step": 3426
    },
    {
      "epoch": 4.337974683544304,
      "grad_norm": 1.314673900604248,
      "learning_rate": 5e-05,
      "loss": 0.9429,
      "step": 3427
    },
    {
      "epoch": 4.339240506329114,
      "grad_norm": 1.315314531326294,
      "learning_rate": 5e-05,
      "loss": 0.9738,
      "step": 3428
    },
    {
      "epoch": 4.340506329113924,
      "grad_norm": 1.3147534132003784,
      "learning_rate": 5e-05,
      "loss": 0.9743,
      "step": 3429
    },
    {
      "epoch": 4.341772151898734,
      "grad_norm": 1.2925270795822144,
      "learning_rate": 5e-05,
      "loss": 0.9575,
      "step": 3430
    },
    {
      "epoch": 4.343037974683544,
      "grad_norm": 1.327010989189148,
      "learning_rate": 5e-05,
      "loss": 0.9323,
      "step": 3431
    },
    {
      "epoch": 4.3443037974683545,
      "grad_norm": 1.283538579940796,
      "learning_rate": 5e-05,
      "loss": 0.9139,
      "step": 3432
    },
    {
      "epoch": 4.345569620253165,
      "grad_norm": 1.2986392974853516,
      "learning_rate": 5e-05,
      "loss": 0.9626,
      "step": 3433
    },
    {
      "epoch": 4.346835443037975,
      "grad_norm": 1.2832649946212769,
      "learning_rate": 5e-05,
      "loss": 0.9223,
      "step": 3434
    },
    {
      "epoch": 4.348101265822785,
      "grad_norm": 1.3259347677230835,
      "learning_rate": 5e-05,
      "loss": 0.9352,
      "step": 3435
    },
    {
      "epoch": 4.349367088607595,
      "grad_norm": 1.3121116161346436,
      "learning_rate": 5e-05,
      "loss": 0.9243,
      "step": 3436
    },
    {
      "epoch": 4.350632911392405,
      "grad_norm": 1.3635176420211792,
      "learning_rate": 5e-05,
      "loss": 1.0063,
      "step": 3437
    },
    {
      "epoch": 4.351898734177215,
      "grad_norm": 1.2770527601242065,
      "learning_rate": 5e-05,
      "loss": 0.8956,
      "step": 3438
    },
    {
      "epoch": 4.353164556962025,
      "grad_norm": 1.3576370477676392,
      "learning_rate": 5e-05,
      "loss": 0.958,
      "step": 3439
    },
    {
      "epoch": 4.3544303797468356,
      "grad_norm": 1.299277424812317,
      "learning_rate": 5e-05,
      "loss": 0.909,
      "step": 3440
    },
    {
      "epoch": 4.355696202531646,
      "grad_norm": 1.2785080671310425,
      "learning_rate": 5e-05,
      "loss": 0.9326,
      "step": 3441
    },
    {
      "epoch": 4.356962025316456,
      "grad_norm": 1.342045783996582,
      "learning_rate": 5e-05,
      "loss": 0.925,
      "step": 3442
    },
    {
      "epoch": 4.358227848101266,
      "grad_norm": 1.369415044784546,
      "learning_rate": 5e-05,
      "loss": 0.9864,
      "step": 3443
    },
    {
      "epoch": 4.359493670886076,
      "grad_norm": 1.3670800924301147,
      "learning_rate": 5e-05,
      "loss": 0.9595,
      "step": 3444
    },
    {
      "epoch": 4.360759493670886,
      "grad_norm": 1.3152929544448853,
      "learning_rate": 5e-05,
      "loss": 0.9148,
      "step": 3445
    },
    {
      "epoch": 4.362025316455696,
      "grad_norm": 1.2942715883255005,
      "learning_rate": 5e-05,
      "loss": 0.9905,
      "step": 3446
    },
    {
      "epoch": 4.3632911392405065,
      "grad_norm": 1.3205405473709106,
      "learning_rate": 5e-05,
      "loss": 0.9424,
      "step": 3447
    },
    {
      "epoch": 4.364556962025317,
      "grad_norm": 1.3082252740859985,
      "learning_rate": 5e-05,
      "loss": 0.8972,
      "step": 3448
    },
    {
      "epoch": 4.365822784810127,
      "grad_norm": 1.3242238759994507,
      "learning_rate": 5e-05,
      "loss": 0.9379,
      "step": 3449
    },
    {
      "epoch": 4.367088607594937,
      "grad_norm": 1.372334599494934,
      "learning_rate": 5e-05,
      "loss": 0.9283,
      "step": 3450
    },
    {
      "epoch": 4.368354430379747,
      "grad_norm": 1.371157169342041,
      "learning_rate": 5e-05,
      "loss": 1.0143,
      "step": 3451
    },
    {
      "epoch": 4.369620253164557,
      "grad_norm": 1.2884769439697266,
      "learning_rate": 5e-05,
      "loss": 0.9171,
      "step": 3452
    },
    {
      "epoch": 4.370886075949367,
      "grad_norm": 1.333617091178894,
      "learning_rate": 5e-05,
      "loss": 0.9816,
      "step": 3453
    },
    {
      "epoch": 4.372151898734177,
      "grad_norm": 1.312712550163269,
      "learning_rate": 5e-05,
      "loss": 0.8938,
      "step": 3454
    },
    {
      "epoch": 4.3734177215189876,
      "grad_norm": 1.3471651077270508,
      "learning_rate": 5e-05,
      "loss": 0.9616,
      "step": 3455
    },
    {
      "epoch": 4.374683544303798,
      "grad_norm": 1.3219960927963257,
      "learning_rate": 5e-05,
      "loss": 0.9517,
      "step": 3456
    },
    {
      "epoch": 4.375949367088608,
      "grad_norm": 1.2938666343688965,
      "learning_rate": 5e-05,
      "loss": 0.8896,
      "step": 3457
    },
    {
      "epoch": 4.377215189873418,
      "grad_norm": 1.3287363052368164,
      "learning_rate": 5e-05,
      "loss": 0.9696,
      "step": 3458
    },
    {
      "epoch": 4.378481012658228,
      "grad_norm": 1.330979824066162,
      "learning_rate": 5e-05,
      "loss": 0.9469,
      "step": 3459
    },
    {
      "epoch": 4.379746835443038,
      "grad_norm": 1.3218052387237549,
      "learning_rate": 5e-05,
      "loss": 0.9303,
      "step": 3460
    },
    {
      "epoch": 4.381012658227848,
      "grad_norm": 1.3110705614089966,
      "learning_rate": 5e-05,
      "loss": 0.8726,
      "step": 3461
    },
    {
      "epoch": 4.3822784810126585,
      "grad_norm": 1.330057144165039,
      "learning_rate": 5e-05,
      "loss": 0.9886,
      "step": 3462
    },
    {
      "epoch": 4.383544303797469,
      "grad_norm": 1.3288308382034302,
      "learning_rate": 5e-05,
      "loss": 0.9481,
      "step": 3463
    },
    {
      "epoch": 4.384810126582279,
      "grad_norm": 1.3028264045715332,
      "learning_rate": 5e-05,
      "loss": 0.9217,
      "step": 3464
    },
    {
      "epoch": 4.386075949367089,
      "grad_norm": 1.3159937858581543,
      "learning_rate": 5e-05,
      "loss": 0.9094,
      "step": 3465
    },
    {
      "epoch": 4.387341772151899,
      "grad_norm": 1.3855538368225098,
      "learning_rate": 5e-05,
      "loss": 0.9574,
      "step": 3466
    },
    {
      "epoch": 4.388607594936709,
      "grad_norm": 1.360450267791748,
      "learning_rate": 5e-05,
      "loss": 1.0203,
      "step": 3467
    },
    {
      "epoch": 4.389873417721519,
      "grad_norm": 1.2897261381149292,
      "learning_rate": 5e-05,
      "loss": 0.9828,
      "step": 3468
    },
    {
      "epoch": 4.391139240506329,
      "grad_norm": 1.3595993518829346,
      "learning_rate": 5e-05,
      "loss": 0.9714,
      "step": 3469
    },
    {
      "epoch": 4.3924050632911396,
      "grad_norm": 1.2882121801376343,
      "learning_rate": 5e-05,
      "loss": 0.8859,
      "step": 3470
    },
    {
      "epoch": 4.39367088607595,
      "grad_norm": 1.2838630676269531,
      "learning_rate": 5e-05,
      "loss": 0.8673,
      "step": 3471
    },
    {
      "epoch": 4.39493670886076,
      "grad_norm": 1.4256539344787598,
      "learning_rate": 5e-05,
      "loss": 1.1175,
      "step": 3472
    },
    {
      "epoch": 4.39620253164557,
      "grad_norm": 1.3269906044006348,
      "learning_rate": 5e-05,
      "loss": 0.9266,
      "step": 3473
    },
    {
      "epoch": 4.39746835443038,
      "grad_norm": 1.3350838422775269,
      "learning_rate": 5e-05,
      "loss": 0.9379,
      "step": 3474
    },
    {
      "epoch": 4.39873417721519,
      "grad_norm": 1.3180556297302246,
      "learning_rate": 5e-05,
      "loss": 0.9352,
      "step": 3475
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.3198491334915161,
      "learning_rate": 5e-05,
      "loss": 0.9723,
      "step": 3476
    },
    {
      "epoch": 4.4012658227848105,
      "grad_norm": 1.3433663845062256,
      "learning_rate": 5e-05,
      "loss": 0.9597,
      "step": 3477
    },
    {
      "epoch": 4.402531645569621,
      "grad_norm": 1.351061463356018,
      "learning_rate": 5e-05,
      "loss": 0.9962,
      "step": 3478
    },
    {
      "epoch": 4.403797468354431,
      "grad_norm": 1.3842792510986328,
      "learning_rate": 5e-05,
      "loss": 1.0003,
      "step": 3479
    },
    {
      "epoch": 4.405063291139241,
      "grad_norm": 1.3407539129257202,
      "learning_rate": 5e-05,
      "loss": 0.9618,
      "step": 3480
    },
    {
      "epoch": 4.406329113924051,
      "grad_norm": 1.3118584156036377,
      "learning_rate": 5e-05,
      "loss": 0.9056,
      "step": 3481
    },
    {
      "epoch": 4.407594936708861,
      "grad_norm": 1.3242597579956055,
      "learning_rate": 5e-05,
      "loss": 0.9359,
      "step": 3482
    },
    {
      "epoch": 4.408860759493671,
      "grad_norm": 1.3106369972229004,
      "learning_rate": 5e-05,
      "loss": 0.9611,
      "step": 3483
    },
    {
      "epoch": 4.410126582278481,
      "grad_norm": 1.313405990600586,
      "learning_rate": 5e-05,
      "loss": 0.9615,
      "step": 3484
    },
    {
      "epoch": 4.4113924050632916,
      "grad_norm": 1.297929286956787,
      "learning_rate": 5e-05,
      "loss": 0.9397,
      "step": 3485
    },
    {
      "epoch": 4.412658227848102,
      "grad_norm": 1.331817626953125,
      "learning_rate": 5e-05,
      "loss": 0.9246,
      "step": 3486
    },
    {
      "epoch": 4.413924050632911,
      "grad_norm": 1.3270467519760132,
      "learning_rate": 5e-05,
      "loss": 0.947,
      "step": 3487
    },
    {
      "epoch": 4.415189873417722,
      "grad_norm": 1.2939013242721558,
      "learning_rate": 5e-05,
      "loss": 0.9146,
      "step": 3488
    },
    {
      "epoch": 4.416455696202531,
      "grad_norm": 1.3216540813446045,
      "learning_rate": 5e-05,
      "loss": 0.9059,
      "step": 3489
    },
    {
      "epoch": 4.417721518987342,
      "grad_norm": 1.328074336051941,
      "learning_rate": 5e-05,
      "loss": 0.8963,
      "step": 3490
    },
    {
      "epoch": 4.4189873417721515,
      "grad_norm": 1.3449909687042236,
      "learning_rate": 5e-05,
      "loss": 1.0151,
      "step": 3491
    },
    {
      "epoch": 4.4202531645569625,
      "grad_norm": 1.2812445163726807,
      "learning_rate": 5e-05,
      "loss": 0.9027,
      "step": 3492
    },
    {
      "epoch": 4.421518987341772,
      "grad_norm": 1.3447492122650146,
      "learning_rate": 5e-05,
      "loss": 0.9908,
      "step": 3493
    },
    {
      "epoch": 4.422784810126582,
      "grad_norm": 1.2989543676376343,
      "learning_rate": 5e-05,
      "loss": 0.9596,
      "step": 3494
    },
    {
      "epoch": 4.424050632911392,
      "grad_norm": 1.3033803701400757,
      "learning_rate": 5e-05,
      "loss": 0.941,
      "step": 3495
    },
    {
      "epoch": 4.425316455696202,
      "grad_norm": 1.2746782302856445,
      "learning_rate": 5e-05,
      "loss": 0.8969,
      "step": 3496
    },
    {
      "epoch": 4.426582278481012,
      "grad_norm": 1.375286340713501,
      "learning_rate": 5e-05,
      "loss": 1.021,
      "step": 3497
    },
    {
      "epoch": 4.427848101265822,
      "grad_norm": 1.3183035850524902,
      "learning_rate": 5e-05,
      "loss": 0.9052,
      "step": 3498
    },
    {
      "epoch": 4.4291139240506325,
      "grad_norm": 1.388962984085083,
      "learning_rate": 5e-05,
      "loss": 0.9521,
      "step": 3499
    },
    {
      "epoch": 4.430379746835443,
      "grad_norm": 1.3448989391326904,
      "learning_rate": 5e-05,
      "loss": 0.9433,
      "step": 3500
    },
    {
      "epoch": 4.431645569620253,
      "grad_norm": 1.2928472757339478,
      "learning_rate": 5e-05,
      "loss": 0.9012,
      "step": 3501
    },
    {
      "epoch": 4.432911392405063,
      "grad_norm": 1.339390754699707,
      "learning_rate": 5e-05,
      "loss": 0.9352,
      "step": 3502
    },
    {
      "epoch": 4.434177215189873,
      "grad_norm": 1.318813681602478,
      "learning_rate": 5e-05,
      "loss": 0.9272,
      "step": 3503
    },
    {
      "epoch": 4.435443037974683,
      "grad_norm": 1.3830974102020264,
      "learning_rate": 5e-05,
      "loss": 1.008,
      "step": 3504
    },
    {
      "epoch": 4.436708860759493,
      "grad_norm": 1.382646918296814,
      "learning_rate": 5e-05,
      "loss": 0.9291,
      "step": 3505
    },
    {
      "epoch": 4.4379746835443035,
      "grad_norm": 1.3221780061721802,
      "learning_rate": 5e-05,
      "loss": 0.9115,
      "step": 3506
    },
    {
      "epoch": 4.439240506329114,
      "grad_norm": 1.3231468200683594,
      "learning_rate": 5e-05,
      "loss": 0.9228,
      "step": 3507
    },
    {
      "epoch": 4.440506329113924,
      "grad_norm": 1.3802295923233032,
      "learning_rate": 5e-05,
      "loss": 0.9763,
      "step": 3508
    },
    {
      "epoch": 4.441772151898734,
      "grad_norm": 1.3745728731155396,
      "learning_rate": 5e-05,
      "loss": 0.9486,
      "step": 3509
    },
    {
      "epoch": 4.443037974683544,
      "grad_norm": 1.3610504865646362,
      "learning_rate": 5e-05,
      "loss": 0.9733,
      "step": 3510
    },
    {
      "epoch": 4.444303797468354,
      "grad_norm": 1.2876585721969604,
      "learning_rate": 5e-05,
      "loss": 0.923,
      "step": 3511
    },
    {
      "epoch": 4.445569620253164,
      "grad_norm": 1.3037790060043335,
      "learning_rate": 5e-05,
      "loss": 0.919,
      "step": 3512
    },
    {
      "epoch": 4.446835443037974,
      "grad_norm": 1.3421515226364136,
      "learning_rate": 5e-05,
      "loss": 0.9476,
      "step": 3513
    },
    {
      "epoch": 4.4481012658227845,
      "grad_norm": 1.3167487382888794,
      "learning_rate": 5e-05,
      "loss": 0.8985,
      "step": 3514
    },
    {
      "epoch": 4.449367088607595,
      "grad_norm": 1.2935006618499756,
      "learning_rate": 5e-05,
      "loss": 0.9406,
      "step": 3515
    },
    {
      "epoch": 4.450632911392405,
      "grad_norm": 1.3628625869750977,
      "learning_rate": 5e-05,
      "loss": 0.9764,
      "step": 3516
    },
    {
      "epoch": 4.451898734177215,
      "grad_norm": 1.3425309658050537,
      "learning_rate": 5e-05,
      "loss": 0.9892,
      "step": 3517
    },
    {
      "epoch": 4.453164556962025,
      "grad_norm": 1.3266876935958862,
      "learning_rate": 5e-05,
      "loss": 0.9632,
      "step": 3518
    },
    {
      "epoch": 4.454430379746835,
      "grad_norm": 1.330684781074524,
      "learning_rate": 5e-05,
      "loss": 0.942,
      "step": 3519
    },
    {
      "epoch": 4.455696202531645,
      "grad_norm": 1.3344193696975708,
      "learning_rate": 5e-05,
      "loss": 0.9372,
      "step": 3520
    },
    {
      "epoch": 4.4569620253164555,
      "grad_norm": 1.353291630744934,
      "learning_rate": 5e-05,
      "loss": 0.9764,
      "step": 3521
    },
    {
      "epoch": 4.458227848101266,
      "grad_norm": 1.3058052062988281,
      "learning_rate": 5e-05,
      "loss": 0.9284,
      "step": 3522
    },
    {
      "epoch": 4.459493670886076,
      "grad_norm": 1.3622653484344482,
      "learning_rate": 5e-05,
      "loss": 0.9681,
      "step": 3523
    },
    {
      "epoch": 4.460759493670886,
      "grad_norm": 1.3024241924285889,
      "learning_rate": 5e-05,
      "loss": 0.9814,
      "step": 3524
    },
    {
      "epoch": 4.462025316455696,
      "grad_norm": 1.354928970336914,
      "learning_rate": 5e-05,
      "loss": 0.9795,
      "step": 3525
    },
    {
      "epoch": 4.463291139240506,
      "grad_norm": 1.2662746906280518,
      "learning_rate": 5e-05,
      "loss": 0.9207,
      "step": 3526
    },
    {
      "epoch": 4.464556962025316,
      "grad_norm": 1.322357416152954,
      "learning_rate": 5e-05,
      "loss": 0.9515,
      "step": 3527
    },
    {
      "epoch": 4.465822784810126,
      "grad_norm": 1.3434287309646606,
      "learning_rate": 5e-05,
      "loss": 0.9482,
      "step": 3528
    },
    {
      "epoch": 4.4670886075949365,
      "grad_norm": 1.3588948249816895,
      "learning_rate": 5e-05,
      "loss": 0.9399,
      "step": 3529
    },
    {
      "epoch": 4.468354430379747,
      "grad_norm": 1.3108187913894653,
      "learning_rate": 5e-05,
      "loss": 0.8824,
      "step": 3530
    },
    {
      "epoch": 4.469620253164557,
      "grad_norm": 1.3164739608764648,
      "learning_rate": 5e-05,
      "loss": 0.9182,
      "step": 3531
    },
    {
      "epoch": 4.470886075949367,
      "grad_norm": 1.3211596012115479,
      "learning_rate": 5e-05,
      "loss": 0.9213,
      "step": 3532
    },
    {
      "epoch": 4.472151898734177,
      "grad_norm": 1.3616955280303955,
      "learning_rate": 5e-05,
      "loss": 1.0164,
      "step": 3533
    },
    {
      "epoch": 4.473417721518987,
      "grad_norm": 1.298161268234253,
      "learning_rate": 5e-05,
      "loss": 0.9373,
      "step": 3534
    },
    {
      "epoch": 4.474683544303797,
      "grad_norm": 1.2669001817703247,
      "learning_rate": 5e-05,
      "loss": 0.9053,
      "step": 3535
    },
    {
      "epoch": 4.4759493670886075,
      "grad_norm": 1.360085368156433,
      "learning_rate": 5e-05,
      "loss": 0.9452,
      "step": 3536
    },
    {
      "epoch": 4.477215189873418,
      "grad_norm": 1.3240165710449219,
      "learning_rate": 5e-05,
      "loss": 0.9268,
      "step": 3537
    },
    {
      "epoch": 4.478481012658228,
      "grad_norm": 1.3343533277511597,
      "learning_rate": 5e-05,
      "loss": 1.0029,
      "step": 3538
    },
    {
      "epoch": 4.479746835443038,
      "grad_norm": 1.3430993556976318,
      "learning_rate": 5e-05,
      "loss": 0.9362,
      "step": 3539
    },
    {
      "epoch": 4.481012658227848,
      "grad_norm": 1.2974004745483398,
      "learning_rate": 5e-05,
      "loss": 0.9243,
      "step": 3540
    },
    {
      "epoch": 4.482278481012658,
      "grad_norm": 1.322472333908081,
      "learning_rate": 5e-05,
      "loss": 0.9202,
      "step": 3541
    },
    {
      "epoch": 4.483544303797468,
      "grad_norm": 1.3209049701690674,
      "learning_rate": 5e-05,
      "loss": 0.9076,
      "step": 3542
    },
    {
      "epoch": 4.484810126582278,
      "grad_norm": 1.3658242225646973,
      "learning_rate": 5e-05,
      "loss": 1.0199,
      "step": 3543
    },
    {
      "epoch": 4.4860759493670885,
      "grad_norm": 1.2961483001708984,
      "learning_rate": 5e-05,
      "loss": 0.9098,
      "step": 3544
    },
    {
      "epoch": 4.487341772151899,
      "grad_norm": 1.3080055713653564,
      "learning_rate": 5e-05,
      "loss": 0.8362,
      "step": 3545
    },
    {
      "epoch": 4.488607594936709,
      "grad_norm": 1.2753419876098633,
      "learning_rate": 5e-05,
      "loss": 0.8943,
      "step": 3546
    },
    {
      "epoch": 4.489873417721519,
      "grad_norm": 1.3323304653167725,
      "learning_rate": 5e-05,
      "loss": 0.9523,
      "step": 3547
    },
    {
      "epoch": 4.491139240506329,
      "grad_norm": 1.3173000812530518,
      "learning_rate": 5e-05,
      "loss": 0.9575,
      "step": 3548
    },
    {
      "epoch": 4.492405063291139,
      "grad_norm": 1.271756649017334,
      "learning_rate": 5e-05,
      "loss": 0.8729,
      "step": 3549
    },
    {
      "epoch": 4.493670886075949,
      "grad_norm": 1.3260290622711182,
      "learning_rate": 5e-05,
      "loss": 0.9569,
      "step": 3550
    },
    {
      "epoch": 4.4949367088607595,
      "grad_norm": 1.3232921361923218,
      "learning_rate": 5e-05,
      "loss": 0.945,
      "step": 3551
    },
    {
      "epoch": 4.49620253164557,
      "grad_norm": 1.3279935121536255,
      "learning_rate": 5e-05,
      "loss": 0.9425,
      "step": 3552
    },
    {
      "epoch": 4.49746835443038,
      "grad_norm": 1.2713232040405273,
      "learning_rate": 5e-05,
      "loss": 0.8928,
      "step": 3553
    },
    {
      "epoch": 4.49873417721519,
      "grad_norm": 1.3121073246002197,
      "learning_rate": 5e-05,
      "loss": 0.9747,
      "step": 3554
    },
    {
      "epoch": 4.5,
      "grad_norm": 1.3681520223617554,
      "learning_rate": 5e-05,
      "loss": 0.9594,
      "step": 3555
    },
    {
      "epoch": 4.5,
      "eval_loss": 1.9753851890563965,
      "eval_runtime": 1397.5624,
      "eval_samples_per_second": 64.312,
      "eval_steps_per_second": 0.503,
      "step": 3555
    },
    {
      "epoch": 4.50126582278481,
      "grad_norm": 1.3514312505722046,
      "learning_rate": 5e-05,
      "loss": 0.936,
      "step": 3556
    },
    {
      "epoch": 4.50253164556962,
      "grad_norm": 1.359605312347412,
      "learning_rate": 5e-05,
      "loss": 1.0124,
      "step": 3557
    },
    {
      "epoch": 4.50379746835443,
      "grad_norm": 1.264113426208496,
      "learning_rate": 5e-05,
      "loss": 0.827,
      "step": 3558
    },
    {
      "epoch": 4.5050632911392405,
      "grad_norm": 1.403586745262146,
      "learning_rate": 5e-05,
      "loss": 0.9781,
      "step": 3559
    },
    {
      "epoch": 4.506329113924051,
      "grad_norm": 1.320946216583252,
      "learning_rate": 5e-05,
      "loss": 0.9657,
      "step": 3560
    },
    {
      "epoch": 4.507594936708861,
      "grad_norm": 1.3615691661834717,
      "learning_rate": 5e-05,
      "loss": 0.9219,
      "step": 3561
    },
    {
      "epoch": 4.508860759493671,
      "grad_norm": 1.272261142730713,
      "learning_rate": 5e-05,
      "loss": 0.9469,
      "step": 3562
    },
    {
      "epoch": 4.510126582278481,
      "grad_norm": 1.2870076894760132,
      "learning_rate": 5e-05,
      "loss": 0.8403,
      "step": 3563
    },
    {
      "epoch": 4.511392405063291,
      "grad_norm": 1.384524941444397,
      "learning_rate": 5e-05,
      "loss": 0.9994,
      "step": 3564
    },
    {
      "epoch": 4.512658227848101,
      "grad_norm": 1.3517894744873047,
      "learning_rate": 5e-05,
      "loss": 0.9848,
      "step": 3565
    },
    {
      "epoch": 4.5139240506329115,
      "grad_norm": 1.295089602470398,
      "learning_rate": 5e-05,
      "loss": 0.9014,
      "step": 3566
    },
    {
      "epoch": 4.515189873417722,
      "grad_norm": 1.3721058368682861,
      "learning_rate": 5e-05,
      "loss": 1.0171,
      "step": 3567
    },
    {
      "epoch": 4.516455696202532,
      "grad_norm": 1.3291919231414795,
      "learning_rate": 5e-05,
      "loss": 0.9114,
      "step": 3568
    },
    {
      "epoch": 4.517721518987342,
      "grad_norm": 1.3779977560043335,
      "learning_rate": 5e-05,
      "loss": 0.9865,
      "step": 3569
    },
    {
      "epoch": 4.518987341772152,
      "grad_norm": 1.2966855764389038,
      "learning_rate": 5e-05,
      "loss": 0.8815,
      "step": 3570
    },
    {
      "epoch": 4.520253164556962,
      "grad_norm": 1.283504843711853,
      "learning_rate": 5e-05,
      "loss": 0.9152,
      "step": 3571
    },
    {
      "epoch": 4.521518987341772,
      "grad_norm": 1.3366222381591797,
      "learning_rate": 5e-05,
      "loss": 0.9548,
      "step": 3572
    },
    {
      "epoch": 4.522784810126582,
      "grad_norm": 1.381908655166626,
      "learning_rate": 5e-05,
      "loss": 0.9338,
      "step": 3573
    },
    {
      "epoch": 4.5240506329113925,
      "grad_norm": 1.3709874153137207,
      "learning_rate": 5e-05,
      "loss": 1.0111,
      "step": 3574
    },
    {
      "epoch": 4.525316455696203,
      "grad_norm": 1.3243505954742432,
      "learning_rate": 5e-05,
      "loss": 0.9426,
      "step": 3575
    },
    {
      "epoch": 4.526582278481013,
      "grad_norm": 1.3516263961791992,
      "learning_rate": 5e-05,
      "loss": 0.9428,
      "step": 3576
    },
    {
      "epoch": 4.527848101265823,
      "grad_norm": 1.354858160018921,
      "learning_rate": 5e-05,
      "loss": 1.0062,
      "step": 3577
    },
    {
      "epoch": 4.529113924050633,
      "grad_norm": 1.3216758966445923,
      "learning_rate": 5e-05,
      "loss": 0.9527,
      "step": 3578
    },
    {
      "epoch": 4.530379746835443,
      "grad_norm": 1.3661904335021973,
      "learning_rate": 5e-05,
      "loss": 0.9579,
      "step": 3579
    },
    {
      "epoch": 4.531645569620253,
      "grad_norm": 1.3661800622940063,
      "learning_rate": 5e-05,
      "loss": 0.9744,
      "step": 3580
    },
    {
      "epoch": 4.5329113924050635,
      "grad_norm": 1.3437308073043823,
      "learning_rate": 5e-05,
      "loss": 0.9096,
      "step": 3581
    },
    {
      "epoch": 4.534177215189874,
      "grad_norm": 1.3478710651397705,
      "learning_rate": 5e-05,
      "loss": 0.93,
      "step": 3582
    },
    {
      "epoch": 4.535443037974684,
      "grad_norm": 1.3504682779312134,
      "learning_rate": 5e-05,
      "loss": 0.9794,
      "step": 3583
    },
    {
      "epoch": 4.536708860759494,
      "grad_norm": 1.3081482648849487,
      "learning_rate": 5e-05,
      "loss": 0.9043,
      "step": 3584
    },
    {
      "epoch": 4.537974683544304,
      "grad_norm": 1.3236373662948608,
      "learning_rate": 5e-05,
      "loss": 0.9287,
      "step": 3585
    },
    {
      "epoch": 4.539240506329114,
      "grad_norm": 1.335802435874939,
      "learning_rate": 5e-05,
      "loss": 0.9784,
      "step": 3586
    },
    {
      "epoch": 4.540506329113924,
      "grad_norm": 1.3685914278030396,
      "learning_rate": 5e-05,
      "loss": 0.9866,
      "step": 3587
    },
    {
      "epoch": 4.541772151898734,
      "grad_norm": 1.2997324466705322,
      "learning_rate": 5e-05,
      "loss": 0.9528,
      "step": 3588
    },
    {
      "epoch": 4.5430379746835445,
      "grad_norm": 1.3199270963668823,
      "learning_rate": 5e-05,
      "loss": 0.9463,
      "step": 3589
    },
    {
      "epoch": 4.544303797468355,
      "grad_norm": 1.3286689519882202,
      "learning_rate": 5e-05,
      "loss": 0.9151,
      "step": 3590
    },
    {
      "epoch": 4.545569620253165,
      "grad_norm": 1.2806859016418457,
      "learning_rate": 5e-05,
      "loss": 0.9311,
      "step": 3591
    },
    {
      "epoch": 4.546835443037975,
      "grad_norm": 1.2727220058441162,
      "learning_rate": 5e-05,
      "loss": 0.926,
      "step": 3592
    },
    {
      "epoch": 4.548101265822785,
      "grad_norm": 1.3449718952178955,
      "learning_rate": 5e-05,
      "loss": 0.919,
      "step": 3593
    },
    {
      "epoch": 4.549367088607595,
      "grad_norm": 1.3593753576278687,
      "learning_rate": 5e-05,
      "loss": 0.9435,
      "step": 3594
    },
    {
      "epoch": 4.550632911392405,
      "grad_norm": 1.2920745611190796,
      "learning_rate": 5e-05,
      "loss": 0.9272,
      "step": 3595
    },
    {
      "epoch": 4.5518987341772155,
      "grad_norm": 1.3570688962936401,
      "learning_rate": 5e-05,
      "loss": 0.9133,
      "step": 3596
    },
    {
      "epoch": 4.553164556962026,
      "grad_norm": 1.3007986545562744,
      "learning_rate": 5e-05,
      "loss": 0.8755,
      "step": 3597
    },
    {
      "epoch": 4.554430379746836,
      "grad_norm": 1.3156092166900635,
      "learning_rate": 5e-05,
      "loss": 0.9046,
      "step": 3598
    },
    {
      "epoch": 4.555696202531646,
      "grad_norm": 1.322023868560791,
      "learning_rate": 5e-05,
      "loss": 0.9183,
      "step": 3599
    },
    {
      "epoch": 4.556962025316456,
      "grad_norm": 1.3050733804702759,
      "learning_rate": 5e-05,
      "loss": 0.8803,
      "step": 3600
    },
    {
      "epoch": 4.558227848101266,
      "grad_norm": 1.3627142906188965,
      "learning_rate": 5e-05,
      "loss": 1.0043,
      "step": 3601
    },
    {
      "epoch": 4.559493670886076,
      "grad_norm": 1.3282594680786133,
      "learning_rate": 5e-05,
      "loss": 0.9207,
      "step": 3602
    },
    {
      "epoch": 4.560759493670886,
      "grad_norm": 1.2931402921676636,
      "learning_rate": 5e-05,
      "loss": 0.868,
      "step": 3603
    },
    {
      "epoch": 4.5620253164556965,
      "grad_norm": 1.302730679512024,
      "learning_rate": 5e-05,
      "loss": 0.8914,
      "step": 3604
    },
    {
      "epoch": 4.563291139240507,
      "grad_norm": 1.367087483406067,
      "learning_rate": 5e-05,
      "loss": 0.96,
      "step": 3605
    },
    {
      "epoch": 4.564556962025317,
      "grad_norm": 1.3552453517913818,
      "learning_rate": 5e-05,
      "loss": 0.9336,
      "step": 3606
    },
    {
      "epoch": 4.565822784810127,
      "grad_norm": 1.3098571300506592,
      "learning_rate": 5e-05,
      "loss": 0.8895,
      "step": 3607
    },
    {
      "epoch": 4.567088607594937,
      "grad_norm": 1.3101822137832642,
      "learning_rate": 5e-05,
      "loss": 0.9331,
      "step": 3608
    },
    {
      "epoch": 4.568354430379747,
      "grad_norm": 1.2888193130493164,
      "learning_rate": 5e-05,
      "loss": 0.8609,
      "step": 3609
    },
    {
      "epoch": 4.569620253164557,
      "grad_norm": 1.3719720840454102,
      "learning_rate": 5e-05,
      "loss": 0.9743,
      "step": 3610
    },
    {
      "epoch": 4.5708860759493675,
      "grad_norm": 1.3505805730819702,
      "learning_rate": 5e-05,
      "loss": 0.9246,
      "step": 3611
    },
    {
      "epoch": 4.572151898734178,
      "grad_norm": 1.3324434757232666,
      "learning_rate": 5e-05,
      "loss": 0.9883,
      "step": 3612
    },
    {
      "epoch": 4.573417721518988,
      "grad_norm": 1.315902590751648,
      "learning_rate": 5e-05,
      "loss": 0.9817,
      "step": 3613
    },
    {
      "epoch": 4.574683544303797,
      "grad_norm": 1.3377089500427246,
      "learning_rate": 5e-05,
      "loss": 0.9434,
      "step": 3614
    },
    {
      "epoch": 4.575949367088608,
      "grad_norm": 1.3405650854110718,
      "learning_rate": 5e-05,
      "loss": 0.957,
      "step": 3615
    },
    {
      "epoch": 4.577215189873417,
      "grad_norm": 1.3343987464904785,
      "learning_rate": 5e-05,
      "loss": 0.9534,
      "step": 3616
    },
    {
      "epoch": 4.578481012658228,
      "grad_norm": 1.3852810859680176,
      "learning_rate": 5e-05,
      "loss": 0.9451,
      "step": 3617
    },
    {
      "epoch": 4.5797468354430375,
      "grad_norm": 1.3316316604614258,
      "learning_rate": 5e-05,
      "loss": 0.9622,
      "step": 3618
    },
    {
      "epoch": 4.5810126582278485,
      "grad_norm": 1.3334734439849854,
      "learning_rate": 5e-05,
      "loss": 0.913,
      "step": 3619
    },
    {
      "epoch": 4.582278481012658,
      "grad_norm": 1.3349647521972656,
      "learning_rate": 5e-05,
      "loss": 0.9617,
      "step": 3620
    },
    {
      "epoch": 4.583544303797469,
      "grad_norm": 1.3281420469284058,
      "learning_rate": 5e-05,
      "loss": 0.926,
      "step": 3621
    },
    {
      "epoch": 4.584810126582278,
      "grad_norm": 1.2902075052261353,
      "learning_rate": 5e-05,
      "loss": 0.8317,
      "step": 3622
    },
    {
      "epoch": 4.586075949367089,
      "grad_norm": 1.28587007522583,
      "learning_rate": 5e-05,
      "loss": 0.9164,
      "step": 3623
    },
    {
      "epoch": 4.587341772151898,
      "grad_norm": 1.340301513671875,
      "learning_rate": 5e-05,
      "loss": 0.9539,
      "step": 3624
    },
    {
      "epoch": 4.588607594936709,
      "grad_norm": 1.3266642093658447,
      "learning_rate": 5e-05,
      "loss": 0.8998,
      "step": 3625
    },
    {
      "epoch": 4.589873417721519,
      "grad_norm": 1.338781714439392,
      "learning_rate": 5e-05,
      "loss": 0.9149,
      "step": 3626
    },
    {
      "epoch": 4.59113924050633,
      "grad_norm": 1.2545987367630005,
      "learning_rate": 5e-05,
      "loss": 0.8659,
      "step": 3627
    },
    {
      "epoch": 4.592405063291139,
      "grad_norm": 1.344861626625061,
      "learning_rate": 5e-05,
      "loss": 0.9353,
      "step": 3628
    },
    {
      "epoch": 4.59367088607595,
      "grad_norm": 1.378147840499878,
      "learning_rate": 5e-05,
      "loss": 0.9481,
      "step": 3629
    },
    {
      "epoch": 4.594936708860759,
      "grad_norm": 1.3402198553085327,
      "learning_rate": 5e-05,
      "loss": 0.9753,
      "step": 3630
    },
    {
      "epoch": 4.596202531645569,
      "grad_norm": 1.3175325393676758,
      "learning_rate": 5e-05,
      "loss": 0.9522,
      "step": 3631
    },
    {
      "epoch": 4.597468354430379,
      "grad_norm": 1.3293168544769287,
      "learning_rate": 5e-05,
      "loss": 0.9311,
      "step": 3632
    },
    {
      "epoch": 4.5987341772151895,
      "grad_norm": 1.3252369165420532,
      "learning_rate": 5e-05,
      "loss": 0.9454,
      "step": 3633
    },
    {
      "epoch": 4.6,
      "grad_norm": 1.315178632736206,
      "learning_rate": 5e-05,
      "loss": 0.8968,
      "step": 3634
    },
    {
      "epoch": 4.60126582278481,
      "grad_norm": 1.2972376346588135,
      "learning_rate": 5e-05,
      "loss": 0.8946,
      "step": 3635
    },
    {
      "epoch": 4.60253164556962,
      "grad_norm": 1.3318513631820679,
      "learning_rate": 5e-05,
      "loss": 0.9433,
      "step": 3636
    },
    {
      "epoch": 4.60379746835443,
      "grad_norm": 1.3516919612884521,
      "learning_rate": 5e-05,
      "loss": 0.9711,
      "step": 3637
    },
    {
      "epoch": 4.60506329113924,
      "grad_norm": 1.3010280132293701,
      "learning_rate": 5e-05,
      "loss": 0.9074,
      "step": 3638
    },
    {
      "epoch": 4.60632911392405,
      "grad_norm": 1.3809059858322144,
      "learning_rate": 5e-05,
      "loss": 0.9453,
      "step": 3639
    },
    {
      "epoch": 4.6075949367088604,
      "grad_norm": 1.3544574975967407,
      "learning_rate": 5e-05,
      "loss": 0.9706,
      "step": 3640
    },
    {
      "epoch": 4.608860759493671,
      "grad_norm": 1.4072632789611816,
      "learning_rate": 5e-05,
      "loss": 0.9262,
      "step": 3641
    },
    {
      "epoch": 4.610126582278481,
      "grad_norm": 1.3113815784454346,
      "learning_rate": 5e-05,
      "loss": 0.9235,
      "step": 3642
    },
    {
      "epoch": 4.611392405063291,
      "grad_norm": 1.3524222373962402,
      "learning_rate": 5e-05,
      "loss": 0.9083,
      "step": 3643
    },
    {
      "epoch": 4.612658227848101,
      "grad_norm": 1.3593908548355103,
      "learning_rate": 5e-05,
      "loss": 0.9298,
      "step": 3644
    },
    {
      "epoch": 4.613924050632911,
      "grad_norm": 1.3033828735351562,
      "learning_rate": 5e-05,
      "loss": 0.9089,
      "step": 3645
    },
    {
      "epoch": 4.615189873417721,
      "grad_norm": 1.3527635335922241,
      "learning_rate": 5e-05,
      "loss": 0.8931,
      "step": 3646
    },
    {
      "epoch": 4.616455696202531,
      "grad_norm": 1.3639577627182007,
      "learning_rate": 5e-05,
      "loss": 0.9739,
      "step": 3647
    },
    {
      "epoch": 4.6177215189873415,
      "grad_norm": 1.2991540431976318,
      "learning_rate": 5e-05,
      "loss": 0.9652,
      "step": 3648
    },
    {
      "epoch": 4.618987341772152,
      "grad_norm": 1.3272454738616943,
      "learning_rate": 5e-05,
      "loss": 0.8828,
      "step": 3649
    },
    {
      "epoch": 4.620253164556962,
      "grad_norm": 1.295967936515808,
      "learning_rate": 5e-05,
      "loss": 0.8881,
      "step": 3650
    },
    {
      "epoch": 4.621518987341772,
      "grad_norm": 1.3160394430160522,
      "learning_rate": 5e-05,
      "loss": 0.9264,
      "step": 3651
    },
    {
      "epoch": 4.622784810126582,
      "grad_norm": 1.3035829067230225,
      "learning_rate": 5e-05,
      "loss": 0.9043,
      "step": 3652
    },
    {
      "epoch": 4.624050632911392,
      "grad_norm": 1.3012874126434326,
      "learning_rate": 5e-05,
      "loss": 0.8904,
      "step": 3653
    },
    {
      "epoch": 4.625316455696202,
      "grad_norm": 1.3340351581573486,
      "learning_rate": 5e-05,
      "loss": 0.9207,
      "step": 3654
    },
    {
      "epoch": 4.6265822784810124,
      "grad_norm": 1.2978897094726562,
      "learning_rate": 5e-05,
      "loss": 0.9284,
      "step": 3655
    },
    {
      "epoch": 4.627848101265823,
      "grad_norm": 1.3047256469726562,
      "learning_rate": 5e-05,
      "loss": 0.9517,
      "step": 3656
    },
    {
      "epoch": 4.629113924050633,
      "grad_norm": 1.2774457931518555,
      "learning_rate": 5e-05,
      "loss": 0.9161,
      "step": 3657
    },
    {
      "epoch": 4.630379746835443,
      "grad_norm": 1.2909653186798096,
      "learning_rate": 5e-05,
      "loss": 0.9082,
      "step": 3658
    },
    {
      "epoch": 4.631645569620253,
      "grad_norm": 1.3671140670776367,
      "learning_rate": 5e-05,
      "loss": 0.9867,
      "step": 3659
    },
    {
      "epoch": 4.632911392405063,
      "grad_norm": 1.3227084875106812,
      "learning_rate": 5e-05,
      "loss": 0.8612,
      "step": 3660
    },
    {
      "epoch": 4.634177215189873,
      "grad_norm": 1.2831811904907227,
      "learning_rate": 5e-05,
      "loss": 0.9699,
      "step": 3661
    },
    {
      "epoch": 4.635443037974683,
      "grad_norm": 1.355985164642334,
      "learning_rate": 5e-05,
      "loss": 0.9991,
      "step": 3662
    },
    {
      "epoch": 4.6367088607594935,
      "grad_norm": 1.3820468187332153,
      "learning_rate": 5e-05,
      "loss": 0.919,
      "step": 3663
    },
    {
      "epoch": 4.637974683544304,
      "grad_norm": 1.3360908031463623,
      "learning_rate": 5e-05,
      "loss": 0.9701,
      "step": 3664
    },
    {
      "epoch": 4.639240506329114,
      "grad_norm": 1.3388251066207886,
      "learning_rate": 5e-05,
      "loss": 0.9363,
      "step": 3665
    },
    {
      "epoch": 4.640506329113924,
      "grad_norm": 1.306294322013855,
      "learning_rate": 5e-05,
      "loss": 0.8823,
      "step": 3666
    },
    {
      "epoch": 4.641772151898734,
      "grad_norm": 1.3127658367156982,
      "learning_rate": 5e-05,
      "loss": 0.9078,
      "step": 3667
    },
    {
      "epoch": 4.643037974683544,
      "grad_norm": 1.3104140758514404,
      "learning_rate": 5e-05,
      "loss": 0.9284,
      "step": 3668
    },
    {
      "epoch": 4.644303797468354,
      "grad_norm": 1.306630253791809,
      "learning_rate": 5e-05,
      "loss": 0.9097,
      "step": 3669
    },
    {
      "epoch": 4.6455696202531644,
      "grad_norm": 1.319704294204712,
      "learning_rate": 5e-05,
      "loss": 0.9168,
      "step": 3670
    },
    {
      "epoch": 4.646835443037975,
      "grad_norm": 1.2973871231079102,
      "learning_rate": 5e-05,
      "loss": 0.9101,
      "step": 3671
    },
    {
      "epoch": 4.648101265822785,
      "grad_norm": 1.3601303100585938,
      "learning_rate": 5e-05,
      "loss": 1.0059,
      "step": 3672
    },
    {
      "epoch": 4.649367088607595,
      "grad_norm": 1.296189785003662,
      "learning_rate": 5e-05,
      "loss": 0.9369,
      "step": 3673
    },
    {
      "epoch": 4.650632911392405,
      "grad_norm": 1.2739540338516235,
      "learning_rate": 5e-05,
      "loss": 0.8538,
      "step": 3674
    },
    {
      "epoch": 4.651898734177215,
      "grad_norm": 1.3537288904190063,
      "learning_rate": 5e-05,
      "loss": 0.9745,
      "step": 3675
    },
    {
      "epoch": 4.653164556962025,
      "grad_norm": 1.342882752418518,
      "learning_rate": 5e-05,
      "loss": 0.9111,
      "step": 3676
    },
    {
      "epoch": 4.654430379746835,
      "grad_norm": 1.2671177387237549,
      "learning_rate": 5e-05,
      "loss": 0.8856,
      "step": 3677
    },
    {
      "epoch": 4.6556962025316455,
      "grad_norm": 1.3216239213943481,
      "learning_rate": 5e-05,
      "loss": 0.9314,
      "step": 3678
    },
    {
      "epoch": 4.656962025316456,
      "grad_norm": 1.3517839908599854,
      "learning_rate": 5e-05,
      "loss": 0.9652,
      "step": 3679
    },
    {
      "epoch": 4.658227848101266,
      "grad_norm": 1.3386976718902588,
      "learning_rate": 5e-05,
      "loss": 0.9262,
      "step": 3680
    },
    {
      "epoch": 4.659493670886076,
      "grad_norm": 1.3375895023345947,
      "learning_rate": 5e-05,
      "loss": 0.9433,
      "step": 3681
    },
    {
      "epoch": 4.660759493670886,
      "grad_norm": 1.3487766981124878,
      "learning_rate": 5e-05,
      "loss": 0.9436,
      "step": 3682
    },
    {
      "epoch": 4.662025316455696,
      "grad_norm": 1.3837482929229736,
      "learning_rate": 5e-05,
      "loss": 0.9604,
      "step": 3683
    },
    {
      "epoch": 4.663291139240506,
      "grad_norm": 1.3363908529281616,
      "learning_rate": 5e-05,
      "loss": 0.9691,
      "step": 3684
    },
    {
      "epoch": 4.6645569620253164,
      "grad_norm": 1.3103374242782593,
      "learning_rate": 5e-05,
      "loss": 0.93,
      "step": 3685
    },
    {
      "epoch": 4.665822784810127,
      "grad_norm": 1.3568955659866333,
      "learning_rate": 5e-05,
      "loss": 0.9326,
      "step": 3686
    },
    {
      "epoch": 4.667088607594937,
      "grad_norm": 1.3288904428482056,
      "learning_rate": 5e-05,
      "loss": 0.9088,
      "step": 3687
    },
    {
      "epoch": 4.668354430379747,
      "grad_norm": 1.27958083152771,
      "learning_rate": 5e-05,
      "loss": 0.9171,
      "step": 3688
    },
    {
      "epoch": 4.669620253164557,
      "grad_norm": 1.3191864490509033,
      "learning_rate": 5e-05,
      "loss": 0.931,
      "step": 3689
    },
    {
      "epoch": 4.670886075949367,
      "grad_norm": 1.318569302558899,
      "learning_rate": 5e-05,
      "loss": 0.9516,
      "step": 3690
    },
    {
      "epoch": 4.672151898734177,
      "grad_norm": 1.3817239999771118,
      "learning_rate": 5e-05,
      "loss": 0.9825,
      "step": 3691
    },
    {
      "epoch": 4.673417721518987,
      "grad_norm": 1.3352066278457642,
      "learning_rate": 5e-05,
      "loss": 0.9231,
      "step": 3692
    },
    {
      "epoch": 4.6746835443037975,
      "grad_norm": 1.3708573579788208,
      "learning_rate": 5e-05,
      "loss": 0.9531,
      "step": 3693
    },
    {
      "epoch": 4.675949367088608,
      "grad_norm": 1.3604285717010498,
      "learning_rate": 5e-05,
      "loss": 0.9828,
      "step": 3694
    },
    {
      "epoch": 4.677215189873418,
      "grad_norm": 1.2757967710494995,
      "learning_rate": 5e-05,
      "loss": 0.8641,
      "step": 3695
    },
    {
      "epoch": 4.678481012658228,
      "grad_norm": 1.2699958086013794,
      "learning_rate": 5e-05,
      "loss": 0.8818,
      "step": 3696
    },
    {
      "epoch": 4.679746835443038,
      "grad_norm": 1.4120103120803833,
      "learning_rate": 5e-05,
      "loss": 1.0165,
      "step": 3697
    },
    {
      "epoch": 4.681012658227848,
      "grad_norm": 1.3719018697738647,
      "learning_rate": 5e-05,
      "loss": 0.9823,
      "step": 3698
    },
    {
      "epoch": 4.682278481012658,
      "grad_norm": 1.3428425788879395,
      "learning_rate": 5e-05,
      "loss": 0.8943,
      "step": 3699
    },
    {
      "epoch": 4.6835443037974684,
      "grad_norm": 1.304444670677185,
      "learning_rate": 5e-05,
      "loss": 0.97,
      "step": 3700
    },
    {
      "epoch": 4.684810126582279,
      "grad_norm": 1.3529123067855835,
      "learning_rate": 5e-05,
      "loss": 0.9377,
      "step": 3701
    },
    {
      "epoch": 4.686075949367089,
      "grad_norm": 1.3824458122253418,
      "learning_rate": 5e-05,
      "loss": 0.9936,
      "step": 3702
    },
    {
      "epoch": 4.687341772151899,
      "grad_norm": 1.3404062986373901,
      "learning_rate": 5e-05,
      "loss": 0.9152,
      "step": 3703
    },
    {
      "epoch": 4.688607594936709,
      "grad_norm": 1.3151148557662964,
      "learning_rate": 5e-05,
      "loss": 0.8919,
      "step": 3704
    },
    {
      "epoch": 4.689873417721519,
      "grad_norm": 1.3349556922912598,
      "learning_rate": 5e-05,
      "loss": 0.9466,
      "step": 3705
    },
    {
      "epoch": 4.691139240506329,
      "grad_norm": 1.3484647274017334,
      "learning_rate": 5e-05,
      "loss": 0.9402,
      "step": 3706
    },
    {
      "epoch": 4.692405063291139,
      "grad_norm": 1.3307701349258423,
      "learning_rate": 5e-05,
      "loss": 0.9337,
      "step": 3707
    },
    {
      "epoch": 4.6936708860759495,
      "grad_norm": 1.349299669265747,
      "learning_rate": 5e-05,
      "loss": 0.9615,
      "step": 3708
    },
    {
      "epoch": 4.69493670886076,
      "grad_norm": 1.3283681869506836,
      "learning_rate": 5e-05,
      "loss": 0.9531,
      "step": 3709
    },
    {
      "epoch": 4.69620253164557,
      "grad_norm": 1.3563127517700195,
      "learning_rate": 5e-05,
      "loss": 0.9035,
      "step": 3710
    },
    {
      "epoch": 4.69746835443038,
      "grad_norm": 1.4039005041122437,
      "learning_rate": 5e-05,
      "loss": 0.9718,
      "step": 3711
    },
    {
      "epoch": 4.69873417721519,
      "grad_norm": 1.285884976387024,
      "learning_rate": 5e-05,
      "loss": 0.9408,
      "step": 3712
    },
    {
      "epoch": 4.7,
      "grad_norm": 1.331236720085144,
      "learning_rate": 5e-05,
      "loss": 0.9445,
      "step": 3713
    },
    {
      "epoch": 4.70126582278481,
      "grad_norm": 1.3125277757644653,
      "learning_rate": 5e-05,
      "loss": 0.8695,
      "step": 3714
    },
    {
      "epoch": 4.7025316455696204,
      "grad_norm": 1.3553547859191895,
      "learning_rate": 5e-05,
      "loss": 0.9284,
      "step": 3715
    },
    {
      "epoch": 4.703797468354431,
      "grad_norm": 1.2810273170471191,
      "learning_rate": 5e-05,
      "loss": 0.8335,
      "step": 3716
    },
    {
      "epoch": 4.705063291139241,
      "grad_norm": 1.3289341926574707,
      "learning_rate": 5e-05,
      "loss": 0.8794,
      "step": 3717
    },
    {
      "epoch": 4.706329113924051,
      "grad_norm": 1.310655117034912,
      "learning_rate": 5e-05,
      "loss": 0.9524,
      "step": 3718
    },
    {
      "epoch": 4.707594936708861,
      "grad_norm": 1.3395309448242188,
      "learning_rate": 5e-05,
      "loss": 0.964,
      "step": 3719
    },
    {
      "epoch": 4.708860759493671,
      "grad_norm": 1.2602663040161133,
      "learning_rate": 5e-05,
      "loss": 0.843,
      "step": 3720
    },
    {
      "epoch": 4.710126582278481,
      "grad_norm": 1.3107924461364746,
      "learning_rate": 5e-05,
      "loss": 0.9158,
      "step": 3721
    },
    {
      "epoch": 4.711392405063291,
      "grad_norm": 1.335655927658081,
      "learning_rate": 5e-05,
      "loss": 0.9373,
      "step": 3722
    },
    {
      "epoch": 4.7126582278481015,
      "grad_norm": 1.3433091640472412,
      "learning_rate": 5e-05,
      "loss": 0.9806,
      "step": 3723
    },
    {
      "epoch": 4.713924050632912,
      "grad_norm": 1.2958261966705322,
      "learning_rate": 5e-05,
      "loss": 0.8938,
      "step": 3724
    },
    {
      "epoch": 4.715189873417722,
      "grad_norm": 1.2928698062896729,
      "learning_rate": 5e-05,
      "loss": 0.8847,
      "step": 3725
    },
    {
      "epoch": 4.716455696202532,
      "grad_norm": 1.3464751243591309,
      "learning_rate": 5e-05,
      "loss": 0.9481,
      "step": 3726
    },
    {
      "epoch": 4.717721518987342,
      "grad_norm": 1.3182629346847534,
      "learning_rate": 5e-05,
      "loss": 0.8994,
      "step": 3727
    },
    {
      "epoch": 4.718987341772152,
      "grad_norm": 1.395998239517212,
      "learning_rate": 5e-05,
      "loss": 1.0115,
      "step": 3728
    },
    {
      "epoch": 4.720253164556962,
      "grad_norm": 1.3937468528747559,
      "learning_rate": 5e-05,
      "loss": 0.9242,
      "step": 3729
    },
    {
      "epoch": 4.7215189873417724,
      "grad_norm": 1.3006925582885742,
      "learning_rate": 5e-05,
      "loss": 0.9201,
      "step": 3730
    },
    {
      "epoch": 4.722784810126583,
      "grad_norm": 1.2768036127090454,
      "learning_rate": 5e-05,
      "loss": 0.92,
      "step": 3731
    },
    {
      "epoch": 4.724050632911393,
      "grad_norm": 1.2976841926574707,
      "learning_rate": 5e-05,
      "loss": 0.8823,
      "step": 3732
    },
    {
      "epoch": 4.725316455696203,
      "grad_norm": 1.365051031112671,
      "learning_rate": 5e-05,
      "loss": 0.9454,
      "step": 3733
    },
    {
      "epoch": 4.726582278481013,
      "grad_norm": 1.288523554801941,
      "learning_rate": 5e-05,
      "loss": 0.8666,
      "step": 3734
    },
    {
      "epoch": 4.727848101265823,
      "grad_norm": 1.328910231590271,
      "learning_rate": 5e-05,
      "loss": 0.9324,
      "step": 3735
    },
    {
      "epoch": 4.729113924050633,
      "grad_norm": 1.261741042137146,
      "learning_rate": 5e-05,
      "loss": 0.8944,
      "step": 3736
    },
    {
      "epoch": 4.730379746835443,
      "grad_norm": 1.3639905452728271,
      "learning_rate": 5e-05,
      "loss": 0.974,
      "step": 3737
    },
    {
      "epoch": 4.7316455696202535,
      "grad_norm": 1.3241289854049683,
      "learning_rate": 5e-05,
      "loss": 0.9346,
      "step": 3738
    },
    {
      "epoch": 4.732911392405064,
      "grad_norm": 1.3498849868774414,
      "learning_rate": 5e-05,
      "loss": 0.9472,
      "step": 3739
    },
    {
      "epoch": 4.734177215189874,
      "grad_norm": 1.2974255084991455,
      "learning_rate": 5e-05,
      "loss": 0.8975,
      "step": 3740
    },
    {
      "epoch": 4.735443037974684,
      "grad_norm": 1.4044034481048584,
      "learning_rate": 5e-05,
      "loss": 0.9805,
      "step": 3741
    },
    {
      "epoch": 4.736708860759494,
      "grad_norm": 1.3131048679351807,
      "learning_rate": 5e-05,
      "loss": 0.9355,
      "step": 3742
    },
    {
      "epoch": 4.737974683544304,
      "grad_norm": 1.3113462924957275,
      "learning_rate": 5e-05,
      "loss": 0.9057,
      "step": 3743
    },
    {
      "epoch": 4.739240506329114,
      "grad_norm": 1.3152997493743896,
      "learning_rate": 5e-05,
      "loss": 0.927,
      "step": 3744
    },
    {
      "epoch": 4.740506329113924,
      "grad_norm": 1.3551982641220093,
      "learning_rate": 5e-05,
      "loss": 0.9661,
      "step": 3745
    },
    {
      "epoch": 4.741772151898735,
      "grad_norm": 1.3272879123687744,
      "learning_rate": 5e-05,
      "loss": 0.9265,
      "step": 3746
    },
    {
      "epoch": 4.743037974683544,
      "grad_norm": 1.3898462057113647,
      "learning_rate": 5e-05,
      "loss": 0.9405,
      "step": 3747
    },
    {
      "epoch": 4.744303797468355,
      "grad_norm": 1.2707267999649048,
      "learning_rate": 5e-05,
      "loss": 0.9251,
      "step": 3748
    },
    {
      "epoch": 4.745569620253164,
      "grad_norm": 1.306382179260254,
      "learning_rate": 5e-05,
      "loss": 0.9401,
      "step": 3749
    },
    {
      "epoch": 4.746835443037975,
      "grad_norm": 1.3254915475845337,
      "learning_rate": 5e-05,
      "loss": 0.9284,
      "step": 3750
    },
    {
      "epoch": 4.748101265822784,
      "grad_norm": 1.3291521072387695,
      "learning_rate": 5e-05,
      "loss": 0.9316,
      "step": 3751
    },
    {
      "epoch": 4.749367088607595,
      "grad_norm": 1.296053171157837,
      "learning_rate": 5e-05,
      "loss": 0.9073,
      "step": 3752
    },
    {
      "epoch": 4.750632911392405,
      "grad_norm": 1.3552240133285522,
      "learning_rate": 5e-05,
      "loss": 0.9355,
      "step": 3753
    },
    {
      "epoch": 4.751898734177216,
      "grad_norm": 1.3130367994308472,
      "learning_rate": 5e-05,
      "loss": 0.8989,
      "step": 3754
    },
    {
      "epoch": 4.753164556962025,
      "grad_norm": 1.338387370109558,
      "learning_rate": 5e-05,
      "loss": 0.9479,
      "step": 3755
    },
    {
      "epoch": 4.754430379746836,
      "grad_norm": 1.284788727760315,
      "learning_rate": 5e-05,
      "loss": 0.918,
      "step": 3756
    },
    {
      "epoch": 4.755696202531645,
      "grad_norm": 1.381805181503296,
      "learning_rate": 5e-05,
      "loss": 0.9283,
      "step": 3757
    },
    {
      "epoch": 4.756962025316456,
      "grad_norm": 1.362396240234375,
      "learning_rate": 5e-05,
      "loss": 0.9055,
      "step": 3758
    },
    {
      "epoch": 4.758227848101265,
      "grad_norm": 1.3585458993911743,
      "learning_rate": 5e-05,
      "loss": 0.9157,
      "step": 3759
    },
    {
      "epoch": 4.759493670886076,
      "grad_norm": 1.2667747735977173,
      "learning_rate": 5e-05,
      "loss": 0.8873,
      "step": 3760
    },
    {
      "epoch": 4.760759493670886,
      "grad_norm": 1.3542263507843018,
      "learning_rate": 5e-05,
      "loss": 0.9459,
      "step": 3761
    },
    {
      "epoch": 4.762025316455696,
      "grad_norm": 1.286817193031311,
      "learning_rate": 5e-05,
      "loss": 0.9356,
      "step": 3762
    },
    {
      "epoch": 4.763291139240506,
      "grad_norm": 1.352238655090332,
      "learning_rate": 5e-05,
      "loss": 0.9206,
      "step": 3763
    },
    {
      "epoch": 4.764556962025316,
      "grad_norm": 1.3428819179534912,
      "learning_rate": 5e-05,
      "loss": 0.9224,
      "step": 3764
    },
    {
      "epoch": 4.765822784810126,
      "grad_norm": 1.3346831798553467,
      "learning_rate": 5e-05,
      "loss": 0.9529,
      "step": 3765
    },
    {
      "epoch": 4.767088607594936,
      "grad_norm": 1.3708529472351074,
      "learning_rate": 5e-05,
      "loss": 0.9763,
      "step": 3766
    },
    {
      "epoch": 4.7683544303797465,
      "grad_norm": 1.334065556526184,
      "learning_rate": 5e-05,
      "loss": 0.8976,
      "step": 3767
    },
    {
      "epoch": 4.769620253164557,
      "grad_norm": 1.3147255182266235,
      "learning_rate": 5e-05,
      "loss": 0.9476,
      "step": 3768
    },
    {
      "epoch": 4.770886075949367,
      "grad_norm": 1.324533224105835,
      "learning_rate": 5e-05,
      "loss": 0.9519,
      "step": 3769
    },
    {
      "epoch": 4.772151898734177,
      "grad_norm": 1.3426448106765747,
      "learning_rate": 5e-05,
      "loss": 0.9122,
      "step": 3770
    },
    {
      "epoch": 4.773417721518987,
      "grad_norm": 1.345435380935669,
      "learning_rate": 5e-05,
      "loss": 0.8919,
      "step": 3771
    },
    {
      "epoch": 4.774683544303797,
      "grad_norm": 1.3251607418060303,
      "learning_rate": 5e-05,
      "loss": 0.9776,
      "step": 3772
    },
    {
      "epoch": 4.775949367088607,
      "grad_norm": 1.3507397174835205,
      "learning_rate": 5e-05,
      "loss": 0.9177,
      "step": 3773
    },
    {
      "epoch": 4.777215189873417,
      "grad_norm": 1.3029062747955322,
      "learning_rate": 5e-05,
      "loss": 0.8976,
      "step": 3774
    },
    {
      "epoch": 4.7784810126582276,
      "grad_norm": 1.3001731634140015,
      "learning_rate": 5e-05,
      "loss": 0.8853,
      "step": 3775
    },
    {
      "epoch": 4.779746835443038,
      "grad_norm": 1.3356575965881348,
      "learning_rate": 5e-05,
      "loss": 0.9201,
      "step": 3776
    },
    {
      "epoch": 4.781012658227848,
      "grad_norm": 1.3635203838348389,
      "learning_rate": 5e-05,
      "loss": 0.9284,
      "step": 3777
    },
    {
      "epoch": 4.782278481012658,
      "grad_norm": 1.2899104356765747,
      "learning_rate": 5e-05,
      "loss": 0.923,
      "step": 3778
    },
    {
      "epoch": 4.783544303797468,
      "grad_norm": 1.3119679689407349,
      "learning_rate": 5e-05,
      "loss": 0.8674,
      "step": 3779
    },
    {
      "epoch": 4.784810126582278,
      "grad_norm": 1.3750576972961426,
      "learning_rate": 5e-05,
      "loss": 0.9352,
      "step": 3780
    },
    {
      "epoch": 4.786075949367088,
      "grad_norm": 1.3385547399520874,
      "learning_rate": 5e-05,
      "loss": 0.9016,
      "step": 3781
    },
    {
      "epoch": 4.7873417721518985,
      "grad_norm": 1.3083070516586304,
      "learning_rate": 5e-05,
      "loss": 0.8849,
      "step": 3782
    },
    {
      "epoch": 4.788607594936709,
      "grad_norm": 1.318949580192566,
      "learning_rate": 5e-05,
      "loss": 0.885,
      "step": 3783
    },
    {
      "epoch": 4.789873417721519,
      "grad_norm": 1.337907314300537,
      "learning_rate": 5e-05,
      "loss": 0.9134,
      "step": 3784
    },
    {
      "epoch": 4.791139240506329,
      "grad_norm": 1.3551084995269775,
      "learning_rate": 5e-05,
      "loss": 0.9356,
      "step": 3785
    },
    {
      "epoch": 4.792405063291139,
      "grad_norm": 1.3949123620986938,
      "learning_rate": 5e-05,
      "loss": 1.0017,
      "step": 3786
    },
    {
      "epoch": 4.793670886075949,
      "grad_norm": 1.294576644897461,
      "learning_rate": 5e-05,
      "loss": 0.8632,
      "step": 3787
    },
    {
      "epoch": 4.794936708860759,
      "grad_norm": 1.2809864282608032,
      "learning_rate": 5e-05,
      "loss": 0.8611,
      "step": 3788
    },
    {
      "epoch": 4.796202531645569,
      "grad_norm": 1.3170338869094849,
      "learning_rate": 5e-05,
      "loss": 0.9207,
      "step": 3789
    },
    {
      "epoch": 4.7974683544303796,
      "grad_norm": 1.3508081436157227,
      "learning_rate": 5e-05,
      "loss": 0.8973,
      "step": 3790
    },
    {
      "epoch": 4.79873417721519,
      "grad_norm": 1.317570686340332,
      "learning_rate": 5e-05,
      "loss": 0.9032,
      "step": 3791
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.3285287618637085,
      "learning_rate": 5e-05,
      "loss": 0.9644,
      "step": 3792
    },
    {
      "epoch": 4.80126582278481,
      "grad_norm": 1.3170326948165894,
      "learning_rate": 5e-05,
      "loss": 0.8791,
      "step": 3793
    },
    {
      "epoch": 4.80253164556962,
      "grad_norm": 1.3187472820281982,
      "learning_rate": 5e-05,
      "loss": 0.9833,
      "step": 3794
    },
    {
      "epoch": 4.80379746835443,
      "grad_norm": 1.4038605690002441,
      "learning_rate": 5e-05,
      "loss": 0.9954,
      "step": 3795
    },
    {
      "epoch": 4.80506329113924,
      "grad_norm": 1.3126981258392334,
      "learning_rate": 5e-05,
      "loss": 0.9027,
      "step": 3796
    },
    {
      "epoch": 4.8063291139240505,
      "grad_norm": 1.3571999073028564,
      "learning_rate": 5e-05,
      "loss": 0.9895,
      "step": 3797
    },
    {
      "epoch": 4.807594936708861,
      "grad_norm": 1.2747334241867065,
      "learning_rate": 5e-05,
      "loss": 0.9107,
      "step": 3798
    },
    {
      "epoch": 4.808860759493671,
      "grad_norm": 1.2992863655090332,
      "learning_rate": 5e-05,
      "loss": 0.8272,
      "step": 3799
    },
    {
      "epoch": 4.810126582278481,
      "grad_norm": 1.3164119720458984,
      "learning_rate": 5e-05,
      "loss": 0.9315,
      "step": 3800
    },
    {
      "epoch": 4.811392405063291,
      "grad_norm": 1.3203989267349243,
      "learning_rate": 5e-05,
      "loss": 0.9166,
      "step": 3801
    },
    {
      "epoch": 4.812658227848101,
      "grad_norm": 1.3817903995513916,
      "learning_rate": 5e-05,
      "loss": 0.9678,
      "step": 3802
    },
    {
      "epoch": 4.813924050632911,
      "grad_norm": 1.3464866876602173,
      "learning_rate": 5e-05,
      "loss": 0.8911,
      "step": 3803
    },
    {
      "epoch": 4.815189873417721,
      "grad_norm": 1.3456834554672241,
      "learning_rate": 5e-05,
      "loss": 0.9625,
      "step": 3804
    },
    {
      "epoch": 4.8164556962025316,
      "grad_norm": 1.375614047050476,
      "learning_rate": 5e-05,
      "loss": 0.9214,
      "step": 3805
    },
    {
      "epoch": 4.817721518987342,
      "grad_norm": 1.2970939874649048,
      "learning_rate": 5e-05,
      "loss": 0.8759,
      "step": 3806
    },
    {
      "epoch": 4.818987341772152,
      "grad_norm": 1.35835599899292,
      "learning_rate": 5e-05,
      "loss": 0.9494,
      "step": 3807
    },
    {
      "epoch": 4.820253164556962,
      "grad_norm": 1.344421625137329,
      "learning_rate": 5e-05,
      "loss": 0.9387,
      "step": 3808
    },
    {
      "epoch": 4.821518987341772,
      "grad_norm": 1.3847260475158691,
      "learning_rate": 5e-05,
      "loss": 0.9766,
      "step": 3809
    },
    {
      "epoch": 4.822784810126582,
      "grad_norm": 1.3799269199371338,
      "learning_rate": 5e-05,
      "loss": 0.9503,
      "step": 3810
    },
    {
      "epoch": 4.824050632911392,
      "grad_norm": 1.2960649728775024,
      "learning_rate": 5e-05,
      "loss": 0.8806,
      "step": 3811
    },
    {
      "epoch": 4.8253164556962025,
      "grad_norm": 1.3255600929260254,
      "learning_rate": 5e-05,
      "loss": 0.8963,
      "step": 3812
    },
    {
      "epoch": 4.826582278481013,
      "grad_norm": 1.375435709953308,
      "learning_rate": 5e-05,
      "loss": 1.0303,
      "step": 3813
    },
    {
      "epoch": 4.827848101265823,
      "grad_norm": 1.277016043663025,
      "learning_rate": 5e-05,
      "loss": 0.8579,
      "step": 3814
    },
    {
      "epoch": 4.829113924050633,
      "grad_norm": 1.3062374591827393,
      "learning_rate": 5e-05,
      "loss": 0.9175,
      "step": 3815
    },
    {
      "epoch": 4.830379746835443,
      "grad_norm": 1.3121765851974487,
      "learning_rate": 5e-05,
      "loss": 0.8629,
      "step": 3816
    },
    {
      "epoch": 4.831645569620253,
      "grad_norm": 1.3486193418502808,
      "learning_rate": 5e-05,
      "loss": 0.9649,
      "step": 3817
    },
    {
      "epoch": 4.832911392405063,
      "grad_norm": 1.316567301750183,
      "learning_rate": 5e-05,
      "loss": 0.9116,
      "step": 3818
    },
    {
      "epoch": 4.834177215189873,
      "grad_norm": 1.3602392673492432,
      "learning_rate": 5e-05,
      "loss": 0.893,
      "step": 3819
    },
    {
      "epoch": 4.8354430379746836,
      "grad_norm": 1.3173861503601074,
      "learning_rate": 5e-05,
      "loss": 0.8735,
      "step": 3820
    },
    {
      "epoch": 4.836708860759494,
      "grad_norm": 1.3004120588302612,
      "learning_rate": 5e-05,
      "loss": 0.8769,
      "step": 3821
    },
    {
      "epoch": 4.837974683544304,
      "grad_norm": 1.3437470197677612,
      "learning_rate": 5e-05,
      "loss": 0.9944,
      "step": 3822
    },
    {
      "epoch": 4.839240506329114,
      "grad_norm": 1.303939938545227,
      "learning_rate": 5e-05,
      "loss": 0.8842,
      "step": 3823
    },
    {
      "epoch": 4.840506329113924,
      "grad_norm": 1.359716773033142,
      "learning_rate": 5e-05,
      "loss": 0.929,
      "step": 3824
    },
    {
      "epoch": 4.841772151898734,
      "grad_norm": 1.3850111961364746,
      "learning_rate": 5e-05,
      "loss": 0.94,
      "step": 3825
    },
    {
      "epoch": 4.843037974683544,
      "grad_norm": 1.290941834449768,
      "learning_rate": 5e-05,
      "loss": 0.9179,
      "step": 3826
    },
    {
      "epoch": 4.8443037974683545,
      "grad_norm": 1.3741008043289185,
      "learning_rate": 5e-05,
      "loss": 0.9333,
      "step": 3827
    },
    {
      "epoch": 4.845569620253165,
      "grad_norm": 1.3202491998672485,
      "learning_rate": 5e-05,
      "loss": 0.9722,
      "step": 3828
    },
    {
      "epoch": 4.846835443037975,
      "grad_norm": 1.3434576988220215,
      "learning_rate": 5e-05,
      "loss": 0.9265,
      "step": 3829
    },
    {
      "epoch": 4.848101265822785,
      "grad_norm": 1.313858985900879,
      "learning_rate": 5e-05,
      "loss": 0.8647,
      "step": 3830
    },
    {
      "epoch": 4.849367088607595,
      "grad_norm": 1.2984693050384521,
      "learning_rate": 5e-05,
      "loss": 0.8785,
      "step": 3831
    },
    {
      "epoch": 4.850632911392405,
      "grad_norm": 1.381662130355835,
      "learning_rate": 5e-05,
      "loss": 0.9416,
      "step": 3832
    },
    {
      "epoch": 4.851898734177215,
      "grad_norm": 1.3039608001708984,
      "learning_rate": 5e-05,
      "loss": 0.8993,
      "step": 3833
    },
    {
      "epoch": 4.853164556962025,
      "grad_norm": 1.315008521080017,
      "learning_rate": 5e-05,
      "loss": 0.8973,
      "step": 3834
    },
    {
      "epoch": 4.8544303797468356,
      "grad_norm": 1.3396975994110107,
      "learning_rate": 5e-05,
      "loss": 0.9206,
      "step": 3835
    },
    {
      "epoch": 4.855696202531646,
      "grad_norm": 1.3476797342300415,
      "learning_rate": 5e-05,
      "loss": 0.8965,
      "step": 3836
    },
    {
      "epoch": 4.856962025316456,
      "grad_norm": 1.34067702293396,
      "learning_rate": 5e-05,
      "loss": 0.8925,
      "step": 3837
    },
    {
      "epoch": 4.858227848101266,
      "grad_norm": 1.3543094396591187,
      "learning_rate": 5e-05,
      "loss": 0.954,
      "step": 3838
    },
    {
      "epoch": 4.859493670886076,
      "grad_norm": 1.3090002536773682,
      "learning_rate": 5e-05,
      "loss": 0.9059,
      "step": 3839
    },
    {
      "epoch": 4.860759493670886,
      "grad_norm": 1.2992725372314453,
      "learning_rate": 5e-05,
      "loss": 0.8633,
      "step": 3840
    },
    {
      "epoch": 4.862025316455696,
      "grad_norm": 1.324681282043457,
      "learning_rate": 5e-05,
      "loss": 0.8774,
      "step": 3841
    },
    {
      "epoch": 4.8632911392405065,
      "grad_norm": 1.3574031591415405,
      "learning_rate": 5e-05,
      "loss": 0.9477,
      "step": 3842
    },
    {
      "epoch": 4.864556962025317,
      "grad_norm": 1.3571475744247437,
      "learning_rate": 5e-05,
      "loss": 0.9434,
      "step": 3843
    },
    {
      "epoch": 4.865822784810127,
      "grad_norm": 1.307592511177063,
      "learning_rate": 5e-05,
      "loss": 0.9135,
      "step": 3844
    },
    {
      "epoch": 4.867088607594937,
      "grad_norm": 1.3375483751296997,
      "learning_rate": 5e-05,
      "loss": 0.9468,
      "step": 3845
    },
    {
      "epoch": 4.868354430379747,
      "grad_norm": 1.3026714324951172,
      "learning_rate": 5e-05,
      "loss": 0.9215,
      "step": 3846
    },
    {
      "epoch": 4.869620253164557,
      "grad_norm": 1.3322213888168335,
      "learning_rate": 5e-05,
      "loss": 0.9134,
      "step": 3847
    },
    {
      "epoch": 4.870886075949367,
      "grad_norm": 1.3409162759780884,
      "learning_rate": 5e-05,
      "loss": 0.9036,
      "step": 3848
    },
    {
      "epoch": 4.872151898734177,
      "grad_norm": 1.3544385433197021,
      "learning_rate": 5e-05,
      "loss": 0.8938,
      "step": 3849
    },
    {
      "epoch": 4.8734177215189876,
      "grad_norm": 1.2614250183105469,
      "learning_rate": 5e-05,
      "loss": 0.8825,
      "step": 3850
    },
    {
      "epoch": 4.874683544303798,
      "grad_norm": 1.3340877294540405,
      "learning_rate": 5e-05,
      "loss": 0.9365,
      "step": 3851
    },
    {
      "epoch": 4.875949367088608,
      "grad_norm": 1.2695190906524658,
      "learning_rate": 5e-05,
      "loss": 0.8605,
      "step": 3852
    },
    {
      "epoch": 4.877215189873418,
      "grad_norm": 1.346001148223877,
      "learning_rate": 5e-05,
      "loss": 0.9411,
      "step": 3853
    },
    {
      "epoch": 4.878481012658228,
      "grad_norm": 1.3317725658416748,
      "learning_rate": 5e-05,
      "loss": 0.9086,
      "step": 3854
    },
    {
      "epoch": 4.879746835443038,
      "grad_norm": 1.3106188774108887,
      "learning_rate": 5e-05,
      "loss": 0.8994,
      "step": 3855
    },
    {
      "epoch": 4.881012658227848,
      "grad_norm": 1.3399395942687988,
      "learning_rate": 5e-05,
      "loss": 0.9226,
      "step": 3856
    },
    {
      "epoch": 4.8822784810126585,
      "grad_norm": 1.289354681968689,
      "learning_rate": 5e-05,
      "loss": 0.8873,
      "step": 3857
    },
    {
      "epoch": 4.883544303797469,
      "grad_norm": 1.3372679948806763,
      "learning_rate": 5e-05,
      "loss": 0.9403,
      "step": 3858
    },
    {
      "epoch": 4.884810126582279,
      "grad_norm": 1.3420411348342896,
      "learning_rate": 5e-05,
      "loss": 0.9497,
      "step": 3859
    },
    {
      "epoch": 4.886075949367089,
      "grad_norm": 1.2978113889694214,
      "learning_rate": 5e-05,
      "loss": 0.8503,
      "step": 3860
    },
    {
      "epoch": 4.887341772151899,
      "grad_norm": 1.3534592390060425,
      "learning_rate": 5e-05,
      "loss": 0.9225,
      "step": 3861
    },
    {
      "epoch": 4.888607594936709,
      "grad_norm": 1.3633896112442017,
      "learning_rate": 5e-05,
      "loss": 0.9343,
      "step": 3862
    },
    {
      "epoch": 4.889873417721519,
      "grad_norm": 1.2943124771118164,
      "learning_rate": 5e-05,
      "loss": 0.8988,
      "step": 3863
    },
    {
      "epoch": 4.891139240506329,
      "grad_norm": 1.3207091093063354,
      "learning_rate": 5e-05,
      "loss": 0.9605,
      "step": 3864
    },
    {
      "epoch": 4.8924050632911396,
      "grad_norm": 1.3703957796096802,
      "learning_rate": 5e-05,
      "loss": 1.0019,
      "step": 3865
    },
    {
      "epoch": 4.89367088607595,
      "grad_norm": 1.32120943069458,
      "learning_rate": 5e-05,
      "loss": 0.8888,
      "step": 3866
    },
    {
      "epoch": 4.89493670886076,
      "grad_norm": 1.3137072324752808,
      "learning_rate": 5e-05,
      "loss": 0.9042,
      "step": 3867
    },
    {
      "epoch": 4.89620253164557,
      "grad_norm": 1.3194527626037598,
      "learning_rate": 5e-05,
      "loss": 0.8901,
      "step": 3868
    },
    {
      "epoch": 4.89746835443038,
      "grad_norm": 1.3073312044143677,
      "learning_rate": 5e-05,
      "loss": 0.8914,
      "step": 3869
    },
    {
      "epoch": 4.89873417721519,
      "grad_norm": 1.304787516593933,
      "learning_rate": 5e-05,
      "loss": 0.8908,
      "step": 3870
    },
    {
      "epoch": 4.9,
      "grad_norm": 1.3217586278915405,
      "learning_rate": 5e-05,
      "loss": 0.9522,
      "step": 3871
    },
    {
      "epoch": 4.9012658227848105,
      "grad_norm": 1.2798925638198853,
      "learning_rate": 5e-05,
      "loss": 0.8358,
      "step": 3872
    },
    {
      "epoch": 4.902531645569621,
      "grad_norm": 1.3643368482589722,
      "learning_rate": 5e-05,
      "loss": 0.9229,
      "step": 3873
    },
    {
      "epoch": 4.903797468354431,
      "grad_norm": 1.3672988414764404,
      "learning_rate": 5e-05,
      "loss": 0.943,
      "step": 3874
    },
    {
      "epoch": 4.905063291139241,
      "grad_norm": 1.385848045349121,
      "learning_rate": 5e-05,
      "loss": 0.9376,
      "step": 3875
    },
    {
      "epoch": 4.90632911392405,
      "grad_norm": 1.3224495649337769,
      "learning_rate": 5e-05,
      "loss": 0.9116,
      "step": 3876
    },
    {
      "epoch": 4.907594936708861,
      "grad_norm": 1.2809194326400757,
      "learning_rate": 5e-05,
      "loss": 0.8452,
      "step": 3877
    },
    {
      "epoch": 4.90886075949367,
      "grad_norm": 1.3565983772277832,
      "learning_rate": 5e-05,
      "loss": 0.947,
      "step": 3878
    },
    {
      "epoch": 4.910126582278481,
      "grad_norm": 1.3363523483276367,
      "learning_rate": 5e-05,
      "loss": 0.8776,
      "step": 3879
    },
    {
      "epoch": 4.911392405063291,
      "grad_norm": 1.3099011182785034,
      "learning_rate": 5e-05,
      "loss": 0.8871,
      "step": 3880
    },
    {
      "epoch": 4.912658227848102,
      "grad_norm": 1.3347593545913696,
      "learning_rate": 5e-05,
      "loss": 0.8965,
      "step": 3881
    },
    {
      "epoch": 4.913924050632911,
      "grad_norm": 1.3204745054244995,
      "learning_rate": 5e-05,
      "loss": 0.9383,
      "step": 3882
    },
    {
      "epoch": 4.915189873417722,
      "grad_norm": 1.3440978527069092,
      "learning_rate": 5e-05,
      "loss": 0.9501,
      "step": 3883
    },
    {
      "epoch": 4.916455696202531,
      "grad_norm": 1.3208673000335693,
      "learning_rate": 5e-05,
      "loss": 0.8827,
      "step": 3884
    },
    {
      "epoch": 4.917721518987342,
      "grad_norm": 1.3681410551071167,
      "learning_rate": 5e-05,
      "loss": 0.9279,
      "step": 3885
    },
    {
      "epoch": 4.9189873417721515,
      "grad_norm": 1.3148332834243774,
      "learning_rate": 5e-05,
      "loss": 0.8222,
      "step": 3886
    },
    {
      "epoch": 4.9202531645569625,
      "grad_norm": 1.3331190347671509,
      "learning_rate": 5e-05,
      "loss": 0.8905,
      "step": 3887
    },
    {
      "epoch": 4.921518987341772,
      "grad_norm": 1.3756794929504395,
      "learning_rate": 5e-05,
      "loss": 0.9286,
      "step": 3888
    },
    {
      "epoch": 4.922784810126583,
      "grad_norm": 1.3332048654556274,
      "learning_rate": 5e-05,
      "loss": 0.912,
      "step": 3889
    },
    {
      "epoch": 4.924050632911392,
      "grad_norm": 1.3102500438690186,
      "learning_rate": 5e-05,
      "loss": 0.9589,
      "step": 3890
    },
    {
      "epoch": 4.925316455696203,
      "grad_norm": 1.3937448263168335,
      "learning_rate": 5e-05,
      "loss": 0.9949,
      "step": 3891
    },
    {
      "epoch": 4.926582278481012,
      "grad_norm": 1.3219701051712036,
      "learning_rate": 5e-05,
      "loss": 0.9248,
      "step": 3892
    },
    {
      "epoch": 4.927848101265822,
      "grad_norm": 1.279735803604126,
      "learning_rate": 5e-05,
      "loss": 0.9358,
      "step": 3893
    },
    {
      "epoch": 4.9291139240506325,
      "grad_norm": 1.3414325714111328,
      "learning_rate": 5e-05,
      "loss": 0.9068,
      "step": 3894
    },
    {
      "epoch": 4.930379746835443,
      "grad_norm": 1.28757905960083,
      "learning_rate": 5e-05,
      "loss": 0.8789,
      "step": 3895
    },
    {
      "epoch": 4.931645569620253,
      "grad_norm": 1.35020911693573,
      "learning_rate": 5e-05,
      "loss": 0.9684,
      "step": 3896
    },
    {
      "epoch": 4.932911392405063,
      "grad_norm": 1.3427654504776,
      "learning_rate": 5e-05,
      "loss": 0.9514,
      "step": 3897
    },
    {
      "epoch": 4.934177215189873,
      "grad_norm": 1.3532915115356445,
      "learning_rate": 5e-05,
      "loss": 0.9284,
      "step": 3898
    },
    {
      "epoch": 4.935443037974683,
      "grad_norm": 1.355892300605774,
      "learning_rate": 5e-05,
      "loss": 0.9367,
      "step": 3899
    },
    {
      "epoch": 4.936708860759493,
      "grad_norm": 1.3457934856414795,
      "learning_rate": 5e-05,
      "loss": 0.9368,
      "step": 3900
    },
    {
      "epoch": 4.9379746835443035,
      "grad_norm": 1.350425362586975,
      "learning_rate": 5e-05,
      "loss": 0.9216,
      "step": 3901
    },
    {
      "epoch": 4.939240506329114,
      "grad_norm": 1.388131022453308,
      "learning_rate": 5e-05,
      "loss": 0.9242,
      "step": 3902
    },
    {
      "epoch": 4.940506329113924,
      "grad_norm": 1.278381109237671,
      "learning_rate": 5e-05,
      "loss": 0.8547,
      "step": 3903
    },
    {
      "epoch": 4.941772151898734,
      "grad_norm": 1.3414890766143799,
      "learning_rate": 5e-05,
      "loss": 0.9337,
      "step": 3904
    },
    {
      "epoch": 4.943037974683544,
      "grad_norm": 1.3123656511306763,
      "learning_rate": 5e-05,
      "loss": 0.897,
      "step": 3905
    },
    {
      "epoch": 4.944303797468354,
      "grad_norm": 1.3471152782440186,
      "learning_rate": 5e-05,
      "loss": 0.9547,
      "step": 3906
    },
    {
      "epoch": 4.945569620253164,
      "grad_norm": 1.2902966737747192,
      "learning_rate": 5e-05,
      "loss": 0.8544,
      "step": 3907
    },
    {
      "epoch": 4.946835443037974,
      "grad_norm": 1.322995901107788,
      "learning_rate": 5e-05,
      "loss": 0.894,
      "step": 3908
    },
    {
      "epoch": 4.9481012658227845,
      "grad_norm": 1.3406425714492798,
      "learning_rate": 5e-05,
      "loss": 0.8972,
      "step": 3909
    },
    {
      "epoch": 4.949367088607595,
      "grad_norm": 1.2619935274124146,
      "learning_rate": 5e-05,
      "loss": 0.8726,
      "step": 3910
    },
    {
      "epoch": 4.950632911392405,
      "grad_norm": 1.3648533821105957,
      "learning_rate": 5e-05,
      "loss": 0.8966,
      "step": 3911
    },
    {
      "epoch": 4.951898734177215,
      "grad_norm": 1.2849524021148682,
      "learning_rate": 5e-05,
      "loss": 0.8513,
      "step": 3912
    },
    {
      "epoch": 4.953164556962025,
      "grad_norm": 1.3150585889816284,
      "learning_rate": 5e-05,
      "loss": 0.8823,
      "step": 3913
    },
    {
      "epoch": 4.954430379746835,
      "grad_norm": 1.3764268159866333,
      "learning_rate": 5e-05,
      "loss": 0.9316,
      "step": 3914
    },
    {
      "epoch": 4.955696202531645,
      "grad_norm": 1.3515818119049072,
      "learning_rate": 5e-05,
      "loss": 0.9256,
      "step": 3915
    },
    {
      "epoch": 4.9569620253164555,
      "grad_norm": 1.3201477527618408,
      "learning_rate": 5e-05,
      "loss": 0.9049,
      "step": 3916
    },
    {
      "epoch": 4.958227848101266,
      "grad_norm": 1.3473200798034668,
      "learning_rate": 5e-05,
      "loss": 0.9286,
      "step": 3917
    },
    {
      "epoch": 4.959493670886076,
      "grad_norm": 1.3583465814590454,
      "learning_rate": 5e-05,
      "loss": 0.971,
      "step": 3918
    },
    {
      "epoch": 4.960759493670886,
      "grad_norm": 1.3226765394210815,
      "learning_rate": 5e-05,
      "loss": 0.8632,
      "step": 3919
    },
    {
      "epoch": 4.962025316455696,
      "grad_norm": 1.3365180492401123,
      "learning_rate": 5e-05,
      "loss": 0.9054,
      "step": 3920
    },
    {
      "epoch": 4.963291139240506,
      "grad_norm": 1.3638137578964233,
      "learning_rate": 5e-05,
      "loss": 0.949,
      "step": 3921
    },
    {
      "epoch": 4.964556962025316,
      "grad_norm": 1.2545148134231567,
      "learning_rate": 5e-05,
      "loss": 0.8821,
      "step": 3922
    },
    {
      "epoch": 4.965822784810126,
      "grad_norm": 1.331530213356018,
      "learning_rate": 5e-05,
      "loss": 0.8556,
      "step": 3923
    },
    {
      "epoch": 4.9670886075949365,
      "grad_norm": 1.3670481443405151,
      "learning_rate": 5e-05,
      "loss": 0.9342,
      "step": 3924
    },
    {
      "epoch": 4.968354430379747,
      "grad_norm": 1.3750180006027222,
      "learning_rate": 5e-05,
      "loss": 0.9115,
      "step": 3925
    },
    {
      "epoch": 4.969620253164557,
      "grad_norm": 1.3049242496490479,
      "learning_rate": 5e-05,
      "loss": 0.8753,
      "step": 3926
    },
    {
      "epoch": 4.970886075949367,
      "grad_norm": 1.3386057615280151,
      "learning_rate": 5e-05,
      "loss": 0.8876,
      "step": 3927
    },
    {
      "epoch": 4.972151898734177,
      "grad_norm": 1.3697173595428467,
      "learning_rate": 5e-05,
      "loss": 0.935,
      "step": 3928
    },
    {
      "epoch": 4.973417721518987,
      "grad_norm": 1.2841122150421143,
      "learning_rate": 5e-05,
      "loss": 0.8905,
      "step": 3929
    },
    {
      "epoch": 4.974683544303797,
      "grad_norm": 1.289543628692627,
      "learning_rate": 5e-05,
      "loss": 0.8473,
      "step": 3930
    },
    {
      "epoch": 4.9759493670886075,
      "grad_norm": 1.313326120376587,
      "learning_rate": 5e-05,
      "loss": 0.9181,
      "step": 3931
    },
    {
      "epoch": 4.977215189873418,
      "grad_norm": 1.375220537185669,
      "learning_rate": 5e-05,
      "loss": 0.8859,
      "step": 3932
    },
    {
      "epoch": 4.978481012658228,
      "grad_norm": 1.324934482574463,
      "learning_rate": 5e-05,
      "loss": 0.8854,
      "step": 3933
    },
    {
      "epoch": 4.979746835443038,
      "grad_norm": 1.3374593257904053,
      "learning_rate": 5e-05,
      "loss": 0.9059,
      "step": 3934
    },
    {
      "epoch": 4.981012658227848,
      "grad_norm": 1.3353039026260376,
      "learning_rate": 5e-05,
      "loss": 0.8782,
      "step": 3935
    },
    {
      "epoch": 4.982278481012658,
      "grad_norm": 1.2761863470077515,
      "learning_rate": 5e-05,
      "loss": 0.865,
      "step": 3936
    },
    {
      "epoch": 4.983544303797468,
      "grad_norm": 1.3862460851669312,
      "learning_rate": 5e-05,
      "loss": 0.9609,
      "step": 3937
    },
    {
      "epoch": 4.984810126582278,
      "grad_norm": 1.3594416379928589,
      "learning_rate": 5e-05,
      "loss": 0.9285,
      "step": 3938
    },
    {
      "epoch": 4.9860759493670885,
      "grad_norm": 1.3281704187393188,
      "learning_rate": 5e-05,
      "loss": 0.9556,
      "step": 3939
    },
    {
      "epoch": 4.987341772151899,
      "grad_norm": 1.30205237865448,
      "learning_rate": 5e-05,
      "loss": 0.8653,
      "step": 3940
    },
    {
      "epoch": 4.988607594936709,
      "grad_norm": 1.3503665924072266,
      "learning_rate": 5e-05,
      "loss": 0.9555,
      "step": 3941
    },
    {
      "epoch": 4.989873417721519,
      "grad_norm": 1.369154930114746,
      "learning_rate": 5e-05,
      "loss": 0.9125,
      "step": 3942
    },
    {
      "epoch": 4.991139240506329,
      "grad_norm": 1.3440253734588623,
      "learning_rate": 5e-05,
      "loss": 0.9297,
      "step": 3943
    },
    {
      "epoch": 4.992405063291139,
      "grad_norm": 1.2966885566711426,
      "learning_rate": 5e-05,
      "loss": 0.9152,
      "step": 3944
    },
    {
      "epoch": 4.993670886075949,
      "grad_norm": 1.2774302959442139,
      "learning_rate": 5e-05,
      "loss": 0.8855,
      "step": 3945
    },
    {
      "epoch": 4.9949367088607595,
      "grad_norm": 1.311885952949524,
      "learning_rate": 5e-05,
      "loss": 0.8784,
      "step": 3946
    },
    {
      "epoch": 4.99620253164557,
      "grad_norm": 1.3143377304077148,
      "learning_rate": 5e-05,
      "loss": 0.8848,
      "step": 3947
    },
    {
      "epoch": 4.99746835443038,
      "grad_norm": 1.2893754243850708,
      "learning_rate": 5e-05,
      "loss": 0.8873,
      "step": 3948
    },
    {
      "epoch": 4.99873417721519,
      "grad_norm": 1.340631127357483,
      "learning_rate": 5e-05,
      "loss": 0.8999,
      "step": 3949
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.304174780845642,
      "learning_rate": 5e-05,
      "loss": 0.9399,
      "step": 3950
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.9116169214248657,
      "eval_runtime": 163.3549,
      "eval_samples_per_second": 550.213,
      "eval_steps_per_second": 4.304,
      "step": 3950
    },
    {
      "epoch": 5.0,
      "step": 3950,
      "total_flos": 0.0,
      "train_loss": 1.5014553476436228,
      "train_runtime": 16814.8169,
      "train_samples_per_second": 240.535,
      "train_steps_per_second": 0.235
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 3950,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 395,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
