{
  "best_metric": 1.6875070333480835,
  "best_model_checkpoint": "./ckpt/eff4_v01_discard:1_epoch:5/checkpoint-1638",
  "epoch": 5.0,
  "eval_steps": 182,
  "global_step": 1820,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0027472527472527475,
      "grad_norm": 2.6695716381073,
      "learning_rate": 2.7472527472527475e-07,
      "loss": 3.8308,
      "step": 1
    },
    {
      "epoch": 0.005494505494505495,
      "grad_norm": 2.695789337158203,
      "learning_rate": 5.494505494505495e-07,
      "loss": 3.8117,
      "step": 2
    },
    {
      "epoch": 0.008241758241758242,
      "grad_norm": 2.7530086040496826,
      "learning_rate": 8.241758241758242e-07,
      "loss": 3.8081,
      "step": 3
    },
    {
      "epoch": 0.01098901098901099,
      "grad_norm": 2.5837371349334717,
      "learning_rate": 1.098901098901099e-06,
      "loss": 3.7349,
      "step": 4
    },
    {
      "epoch": 0.013736263736263736,
      "grad_norm": 2.6968367099761963,
      "learning_rate": 1.3736263736263736e-06,
      "loss": 3.7585,
      "step": 5
    },
    {
      "epoch": 0.016483516483516484,
      "grad_norm": 2.6701505184173584,
      "learning_rate": 1.6483516483516484e-06,
      "loss": 3.815,
      "step": 6
    },
    {
      "epoch": 0.019230769230769232,
      "grad_norm": 2.7446558475494385,
      "learning_rate": 1.9230769230769234e-06,
      "loss": 3.7443,
      "step": 7
    },
    {
      "epoch": 0.02197802197802198,
      "grad_norm": 2.6301774978637695,
      "learning_rate": 2.197802197802198e-06,
      "loss": 3.7587,
      "step": 8
    },
    {
      "epoch": 0.024725274725274724,
      "grad_norm": 2.7820968627929688,
      "learning_rate": 2.4725274725274726e-06,
      "loss": 3.7644,
      "step": 9
    },
    {
      "epoch": 0.027472527472527472,
      "grad_norm": 2.5648303031921387,
      "learning_rate": 2.747252747252747e-06,
      "loss": 3.7397,
      "step": 10
    },
    {
      "epoch": 0.03021978021978022,
      "grad_norm": 2.841571569442749,
      "learning_rate": 3.021978021978022e-06,
      "loss": 3.8159,
      "step": 11
    },
    {
      "epoch": 0.03296703296703297,
      "grad_norm": 2.733649253845215,
      "learning_rate": 3.2967032967032968e-06,
      "loss": 3.7962,
      "step": 12
    },
    {
      "epoch": 0.03571428571428571,
      "grad_norm": 2.5099432468414307,
      "learning_rate": 3.5714285714285714e-06,
      "loss": 3.6991,
      "step": 13
    },
    {
      "epoch": 0.038461538461538464,
      "grad_norm": 2.5131521224975586,
      "learning_rate": 3.846153846153847e-06,
      "loss": 3.7689,
      "step": 14
    },
    {
      "epoch": 0.04120879120879121,
      "grad_norm": 2.6880006790161133,
      "learning_rate": 4.120879120879121e-06,
      "loss": 3.7429,
      "step": 15
    },
    {
      "epoch": 0.04395604395604396,
      "grad_norm": 2.385209321975708,
      "learning_rate": 4.395604395604396e-06,
      "loss": 3.6769,
      "step": 16
    },
    {
      "epoch": 0.046703296703296704,
      "grad_norm": 2.338327407836914,
      "learning_rate": 4.6703296703296706e-06,
      "loss": 3.7278,
      "step": 17
    },
    {
      "epoch": 0.04945054945054945,
      "grad_norm": 2.639044761657715,
      "learning_rate": 4.945054945054945e-06,
      "loss": 3.7209,
      "step": 18
    },
    {
      "epoch": 0.0521978021978022,
      "grad_norm": 2.50712513923645,
      "learning_rate": 5.219780219780221e-06,
      "loss": 3.729,
      "step": 19
    },
    {
      "epoch": 0.054945054945054944,
      "grad_norm": 2.4578194618225098,
      "learning_rate": 5.494505494505494e-06,
      "loss": 3.7742,
      "step": 20
    },
    {
      "epoch": 0.057692307692307696,
      "grad_norm": 2.4845147132873535,
      "learning_rate": 5.76923076923077e-06,
      "loss": 3.6913,
      "step": 21
    },
    {
      "epoch": 0.06043956043956044,
      "grad_norm": 2.4679551124572754,
      "learning_rate": 6.043956043956044e-06,
      "loss": 3.7211,
      "step": 22
    },
    {
      "epoch": 0.06318681318681318,
      "grad_norm": 2.5654706954956055,
      "learning_rate": 6.318681318681319e-06,
      "loss": 3.7347,
      "step": 23
    },
    {
      "epoch": 0.06593406593406594,
      "grad_norm": 2.337947368621826,
      "learning_rate": 6.5934065934065935e-06,
      "loss": 3.6698,
      "step": 24
    },
    {
      "epoch": 0.06868131868131869,
      "grad_norm": 2.6645703315734863,
      "learning_rate": 6.868131868131869e-06,
      "loss": 3.6846,
      "step": 25
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 2.622396230697632,
      "learning_rate": 7.142857142857143e-06,
      "loss": 3.6391,
      "step": 26
    },
    {
      "epoch": 0.07417582417582418,
      "grad_norm": 2.3327388763427734,
      "learning_rate": 7.417582417582418e-06,
      "loss": 3.6342,
      "step": 27
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 2.2201972007751465,
      "learning_rate": 7.692307692307694e-06,
      "loss": 3.673,
      "step": 28
    },
    {
      "epoch": 0.07967032967032966,
      "grad_norm": 2.2635276317596436,
      "learning_rate": 7.967032967032966e-06,
      "loss": 3.6567,
      "step": 29
    },
    {
      "epoch": 0.08241758241758242,
      "grad_norm": 2.2561936378479004,
      "learning_rate": 8.241758241758243e-06,
      "loss": 3.6607,
      "step": 30
    },
    {
      "epoch": 0.08516483516483517,
      "grad_norm": 2.2346742153167725,
      "learning_rate": 8.516483516483517e-06,
      "loss": 3.616,
      "step": 31
    },
    {
      "epoch": 0.08791208791208792,
      "grad_norm": 2.280512571334839,
      "learning_rate": 8.791208791208792e-06,
      "loss": 3.5853,
      "step": 32
    },
    {
      "epoch": 0.09065934065934066,
      "grad_norm": 2.3546998500823975,
      "learning_rate": 9.065934065934067e-06,
      "loss": 3.5281,
      "step": 33
    },
    {
      "epoch": 0.09340659340659341,
      "grad_norm": 2.126587152481079,
      "learning_rate": 9.340659340659341e-06,
      "loss": 3.6123,
      "step": 34
    },
    {
      "epoch": 0.09615384615384616,
      "grad_norm": 2.181060791015625,
      "learning_rate": 9.615384615384616e-06,
      "loss": 3.579,
      "step": 35
    },
    {
      "epoch": 0.0989010989010989,
      "grad_norm": 2.212026834487915,
      "learning_rate": 9.89010989010989e-06,
      "loss": 3.5951,
      "step": 36
    },
    {
      "epoch": 0.10164835164835165,
      "grad_norm": 2.1457431316375732,
      "learning_rate": 1.0164835164835165e-05,
      "loss": 3.5562,
      "step": 37
    },
    {
      "epoch": 0.1043956043956044,
      "grad_norm": 2.077655076980591,
      "learning_rate": 1.0439560439560441e-05,
      "loss": 3.565,
      "step": 38
    },
    {
      "epoch": 0.10714285714285714,
      "grad_norm": 2.0954089164733887,
      "learning_rate": 1.0714285714285714e-05,
      "loss": 3.5221,
      "step": 39
    },
    {
      "epoch": 0.10989010989010989,
      "grad_norm": 2.2588603496551514,
      "learning_rate": 1.0989010989010989e-05,
      "loss": 3.5659,
      "step": 40
    },
    {
      "epoch": 0.11263736263736264,
      "grad_norm": 2.1180875301361084,
      "learning_rate": 1.1263736263736265e-05,
      "loss": 3.5139,
      "step": 41
    },
    {
      "epoch": 0.11538461538461539,
      "grad_norm": 2.066347122192383,
      "learning_rate": 1.153846153846154e-05,
      "loss": 3.5277,
      "step": 42
    },
    {
      "epoch": 0.11813186813186813,
      "grad_norm": 2.0453360080718994,
      "learning_rate": 1.1813186813186814e-05,
      "loss": 3.5099,
      "step": 43
    },
    {
      "epoch": 0.12087912087912088,
      "grad_norm": 2.1064748764038086,
      "learning_rate": 1.2087912087912089e-05,
      "loss": 3.5775,
      "step": 44
    },
    {
      "epoch": 0.12362637362637363,
      "grad_norm": 2.093380928039551,
      "learning_rate": 1.2362637362637363e-05,
      "loss": 3.508,
      "step": 45
    },
    {
      "epoch": 0.12637362637362637,
      "grad_norm": 2.0491552352905273,
      "learning_rate": 1.2637362637362638e-05,
      "loss": 3.4744,
      "step": 46
    },
    {
      "epoch": 0.12912087912087913,
      "grad_norm": 2.1033029556274414,
      "learning_rate": 1.2912087912087914e-05,
      "loss": 3.521,
      "step": 47
    },
    {
      "epoch": 0.13186813186813187,
      "grad_norm": 2.126673460006714,
      "learning_rate": 1.3186813186813187e-05,
      "loss": 3.5338,
      "step": 48
    },
    {
      "epoch": 0.1346153846153846,
      "grad_norm": 2.0922465324401855,
      "learning_rate": 1.3461538461538462e-05,
      "loss": 3.4999,
      "step": 49
    },
    {
      "epoch": 0.13736263736263737,
      "grad_norm": 2.2107789516448975,
      "learning_rate": 1.3736263736263738e-05,
      "loss": 3.427,
      "step": 50
    },
    {
      "epoch": 0.1401098901098901,
      "grad_norm": 1.961550235748291,
      "learning_rate": 1.4010989010989013e-05,
      "loss": 3.44,
      "step": 51
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 2.0477519035339355,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 3.4912,
      "step": 52
    },
    {
      "epoch": 0.14560439560439561,
      "grad_norm": 2.0505475997924805,
      "learning_rate": 1.4560439560439562e-05,
      "loss": 3.4009,
      "step": 53
    },
    {
      "epoch": 0.14835164835164835,
      "grad_norm": 2.1129989624023438,
      "learning_rate": 1.4835164835164836e-05,
      "loss": 3.4144,
      "step": 54
    },
    {
      "epoch": 0.1510989010989011,
      "grad_norm": 2.0145461559295654,
      "learning_rate": 1.510989010989011e-05,
      "loss": 3.447,
      "step": 55
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 2.0936801433563232,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 3.3904,
      "step": 56
    },
    {
      "epoch": 0.1565934065934066,
      "grad_norm": 1.9659879207611084,
      "learning_rate": 1.565934065934066e-05,
      "loss": 3.4651,
      "step": 57
    },
    {
      "epoch": 0.15934065934065933,
      "grad_norm": 1.9319560527801514,
      "learning_rate": 1.5934065934065933e-05,
      "loss": 3.3936,
      "step": 58
    },
    {
      "epoch": 0.1620879120879121,
      "grad_norm": 2.144106864929199,
      "learning_rate": 1.620879120879121e-05,
      "loss": 3.3642,
      "step": 59
    },
    {
      "epoch": 0.16483516483516483,
      "grad_norm": 2.0298218727111816,
      "learning_rate": 1.6483516483516486e-05,
      "loss": 3.3761,
      "step": 60
    },
    {
      "epoch": 0.16758241758241757,
      "grad_norm": 2.0859930515289307,
      "learning_rate": 1.6758241758241757e-05,
      "loss": 3.3306,
      "step": 61
    },
    {
      "epoch": 0.17032967032967034,
      "grad_norm": 1.991898536682129,
      "learning_rate": 1.7032967032967035e-05,
      "loss": 3.4013,
      "step": 62
    },
    {
      "epoch": 0.17307692307692307,
      "grad_norm": 1.8938087224960327,
      "learning_rate": 1.730769230769231e-05,
      "loss": 3.2727,
      "step": 63
    },
    {
      "epoch": 0.17582417582417584,
      "grad_norm": 1.94041109085083,
      "learning_rate": 1.7582417582417584e-05,
      "loss": 3.3698,
      "step": 64
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 2.020461082458496,
      "learning_rate": 1.785714285714286e-05,
      "loss": 3.3143,
      "step": 65
    },
    {
      "epoch": 0.1813186813186813,
      "grad_norm": 2.013946533203125,
      "learning_rate": 1.8131868131868133e-05,
      "loss": 3.3479,
      "step": 66
    },
    {
      "epoch": 0.18406593406593408,
      "grad_norm": 2.031998872756958,
      "learning_rate": 1.8406593406593408e-05,
      "loss": 3.3436,
      "step": 67
    },
    {
      "epoch": 0.18681318681318682,
      "grad_norm": 2.0436527729034424,
      "learning_rate": 1.8681318681318682e-05,
      "loss": 3.3608,
      "step": 68
    },
    {
      "epoch": 0.18956043956043955,
      "grad_norm": 1.9787884950637817,
      "learning_rate": 1.8956043956043957e-05,
      "loss": 3.3184,
      "step": 69
    },
    {
      "epoch": 0.19230769230769232,
      "grad_norm": 2.1024656295776367,
      "learning_rate": 1.923076923076923e-05,
      "loss": 3.2799,
      "step": 70
    },
    {
      "epoch": 0.19505494505494506,
      "grad_norm": 1.943414330482483,
      "learning_rate": 1.9505494505494506e-05,
      "loss": 3.2302,
      "step": 71
    },
    {
      "epoch": 0.1978021978021978,
      "grad_norm": 1.9316757917404175,
      "learning_rate": 1.978021978021978e-05,
      "loss": 3.3004,
      "step": 72
    },
    {
      "epoch": 0.20054945054945056,
      "grad_norm": 1.9432404041290283,
      "learning_rate": 2.0054945054945055e-05,
      "loss": 3.2771,
      "step": 73
    },
    {
      "epoch": 0.2032967032967033,
      "grad_norm": 1.9354809522628784,
      "learning_rate": 2.032967032967033e-05,
      "loss": 3.2909,
      "step": 74
    },
    {
      "epoch": 0.20604395604395603,
      "grad_norm": 1.9573537111282349,
      "learning_rate": 2.0604395604395604e-05,
      "loss": 3.2904,
      "step": 75
    },
    {
      "epoch": 0.2087912087912088,
      "grad_norm": 1.944407343864441,
      "learning_rate": 2.0879120879120882e-05,
      "loss": 3.2139,
      "step": 76
    },
    {
      "epoch": 0.21153846153846154,
      "grad_norm": 2.0116758346557617,
      "learning_rate": 2.1153846153846154e-05,
      "loss": 3.213,
      "step": 77
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 2.0485424995422363,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 3.2911,
      "step": 78
    },
    {
      "epoch": 0.21703296703296704,
      "grad_norm": 2.047485589981079,
      "learning_rate": 2.1703296703296706e-05,
      "loss": 3.2428,
      "step": 79
    },
    {
      "epoch": 0.21978021978021978,
      "grad_norm": 1.8788222074508667,
      "learning_rate": 2.1978021978021977e-05,
      "loss": 3.1951,
      "step": 80
    },
    {
      "epoch": 0.22252747252747251,
      "grad_norm": 2.0106253623962402,
      "learning_rate": 2.2252747252747252e-05,
      "loss": 3.2375,
      "step": 81
    },
    {
      "epoch": 0.22527472527472528,
      "grad_norm": 2.0893564224243164,
      "learning_rate": 2.252747252747253e-05,
      "loss": 3.2425,
      "step": 82
    },
    {
      "epoch": 0.22802197802197802,
      "grad_norm": 2.0387015342712402,
      "learning_rate": 2.28021978021978e-05,
      "loss": 3.2285,
      "step": 83
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 2.002000570297241,
      "learning_rate": 2.307692307692308e-05,
      "loss": 3.2097,
      "step": 84
    },
    {
      "epoch": 0.23351648351648352,
      "grad_norm": 1.9712398052215576,
      "learning_rate": 2.3351648351648354e-05,
      "loss": 3.174,
      "step": 85
    },
    {
      "epoch": 0.23626373626373626,
      "grad_norm": 1.9950282573699951,
      "learning_rate": 2.3626373626373628e-05,
      "loss": 3.1911,
      "step": 86
    },
    {
      "epoch": 0.23901098901098902,
      "grad_norm": 2.0076892375946045,
      "learning_rate": 2.3901098901098903e-05,
      "loss": 3.2423,
      "step": 87
    },
    {
      "epoch": 0.24175824175824176,
      "grad_norm": 1.9138778448104858,
      "learning_rate": 2.4175824175824177e-05,
      "loss": 3.1943,
      "step": 88
    },
    {
      "epoch": 0.2445054945054945,
      "grad_norm": 1.9054242372512817,
      "learning_rate": 2.4450549450549452e-05,
      "loss": 3.1009,
      "step": 89
    },
    {
      "epoch": 0.24725274725274726,
      "grad_norm": 1.8971920013427734,
      "learning_rate": 2.4725274725274727e-05,
      "loss": 3.2229,
      "step": 90
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.9526162147521973,
      "learning_rate": 2.5e-05,
      "loss": 3.1689,
      "step": 91
    },
    {
      "epoch": 0.25274725274725274,
      "grad_norm": 1.9189045429229736,
      "learning_rate": 2.5274725274725276e-05,
      "loss": 3.1542,
      "step": 92
    },
    {
      "epoch": 0.2554945054945055,
      "grad_norm": 1.9006931781768799,
      "learning_rate": 2.554945054945055e-05,
      "loss": 3.1499,
      "step": 93
    },
    {
      "epoch": 0.25824175824175827,
      "grad_norm": 1.9519175291061401,
      "learning_rate": 2.582417582417583e-05,
      "loss": 3.1629,
      "step": 94
    },
    {
      "epoch": 0.260989010989011,
      "grad_norm": 1.9140150547027588,
      "learning_rate": 2.6098901098901103e-05,
      "loss": 3.2441,
      "step": 95
    },
    {
      "epoch": 0.26373626373626374,
      "grad_norm": 1.879737138748169,
      "learning_rate": 2.6373626373626374e-05,
      "loss": 3.1261,
      "step": 96
    },
    {
      "epoch": 0.2664835164835165,
      "grad_norm": 1.9698532819747925,
      "learning_rate": 2.664835164835165e-05,
      "loss": 3.0972,
      "step": 97
    },
    {
      "epoch": 0.2692307692307692,
      "grad_norm": 1.8737112283706665,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 3.1121,
      "step": 98
    },
    {
      "epoch": 0.27197802197802196,
      "grad_norm": 1.9081770181655884,
      "learning_rate": 2.7197802197802198e-05,
      "loss": 3.1263,
      "step": 99
    },
    {
      "epoch": 0.27472527472527475,
      "grad_norm": 1.962514877319336,
      "learning_rate": 2.7472527472527476e-05,
      "loss": 3.1217,
      "step": 100
    },
    {
      "epoch": 0.2774725274725275,
      "grad_norm": 1.9108518362045288,
      "learning_rate": 2.774725274725275e-05,
      "loss": 3.0859,
      "step": 101
    },
    {
      "epoch": 0.2802197802197802,
      "grad_norm": 1.8720567226409912,
      "learning_rate": 2.8021978021978025e-05,
      "loss": 3.0801,
      "step": 102
    },
    {
      "epoch": 0.28296703296703296,
      "grad_norm": 1.8986077308654785,
      "learning_rate": 2.8296703296703296e-05,
      "loss": 3.0569,
      "step": 103
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 1.9247026443481445,
      "learning_rate": 2.857142857142857e-05,
      "loss": 3.0162,
      "step": 104
    },
    {
      "epoch": 0.28846153846153844,
      "grad_norm": 1.960639476776123,
      "learning_rate": 2.8846153846153845e-05,
      "loss": 3.0485,
      "step": 105
    },
    {
      "epoch": 0.29120879120879123,
      "grad_norm": 1.9106675386428833,
      "learning_rate": 2.9120879120879123e-05,
      "loss": 3.0428,
      "step": 106
    },
    {
      "epoch": 0.29395604395604397,
      "grad_norm": 1.9188185930252075,
      "learning_rate": 2.9395604395604398e-05,
      "loss": 3.086,
      "step": 107
    },
    {
      "epoch": 0.2967032967032967,
      "grad_norm": 1.9426991939544678,
      "learning_rate": 2.9670329670329673e-05,
      "loss": 3.0496,
      "step": 108
    },
    {
      "epoch": 0.29945054945054944,
      "grad_norm": 1.8970470428466797,
      "learning_rate": 2.9945054945054947e-05,
      "loss": 3.0346,
      "step": 109
    },
    {
      "epoch": 0.3021978021978022,
      "grad_norm": 1.881933331489563,
      "learning_rate": 3.021978021978022e-05,
      "loss": 3.0132,
      "step": 110
    },
    {
      "epoch": 0.30494505494505497,
      "grad_norm": 1.978031873703003,
      "learning_rate": 3.04945054945055e-05,
      "loss": 3.0599,
      "step": 111
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 1.8906649351119995,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 2.9946,
      "step": 112
    },
    {
      "epoch": 0.31043956043956045,
      "grad_norm": 1.8537652492523193,
      "learning_rate": 3.1043956043956046e-05,
      "loss": 2.9818,
      "step": 113
    },
    {
      "epoch": 0.3131868131868132,
      "grad_norm": 2.0496089458465576,
      "learning_rate": 3.131868131868132e-05,
      "loss": 2.9463,
      "step": 114
    },
    {
      "epoch": 0.3159340659340659,
      "grad_norm": 1.9013385772705078,
      "learning_rate": 3.1593406593406595e-05,
      "loss": 2.952,
      "step": 115
    },
    {
      "epoch": 0.31868131868131866,
      "grad_norm": 1.984520435333252,
      "learning_rate": 3.1868131868131866e-05,
      "loss": 2.9543,
      "step": 116
    },
    {
      "epoch": 0.32142857142857145,
      "grad_norm": 1.8449915647506714,
      "learning_rate": 3.2142857142857144e-05,
      "loss": 3.0118,
      "step": 117
    },
    {
      "epoch": 0.3241758241758242,
      "grad_norm": 1.861899495124817,
      "learning_rate": 3.241758241758242e-05,
      "loss": 2.9644,
      "step": 118
    },
    {
      "epoch": 0.3269230769230769,
      "grad_norm": 1.8066151142120361,
      "learning_rate": 3.269230769230769e-05,
      "loss": 2.976,
      "step": 119
    },
    {
      "epoch": 0.32967032967032966,
      "grad_norm": 1.9555846452713013,
      "learning_rate": 3.296703296703297e-05,
      "loss": 2.8611,
      "step": 120
    },
    {
      "epoch": 0.3324175824175824,
      "grad_norm": 1.8385919332504272,
      "learning_rate": 3.324175824175824e-05,
      "loss": 2.9248,
      "step": 121
    },
    {
      "epoch": 0.33516483516483514,
      "grad_norm": 1.9196571111679077,
      "learning_rate": 3.3516483516483513e-05,
      "loss": 2.9165,
      "step": 122
    },
    {
      "epoch": 0.33791208791208793,
      "grad_norm": 2.1037487983703613,
      "learning_rate": 3.37912087912088e-05,
      "loss": 2.9135,
      "step": 123
    },
    {
      "epoch": 0.34065934065934067,
      "grad_norm": 1.90742826461792,
      "learning_rate": 3.406593406593407e-05,
      "loss": 2.8599,
      "step": 124
    },
    {
      "epoch": 0.3434065934065934,
      "grad_norm": 1.8678655624389648,
      "learning_rate": 3.434065934065934e-05,
      "loss": 2.8981,
      "step": 125
    },
    {
      "epoch": 0.34615384615384615,
      "grad_norm": 1.8400131464004517,
      "learning_rate": 3.461538461538462e-05,
      "loss": 2.8873,
      "step": 126
    },
    {
      "epoch": 0.3489010989010989,
      "grad_norm": 1.9155598878860474,
      "learning_rate": 3.489010989010989e-05,
      "loss": 2.8825,
      "step": 127
    },
    {
      "epoch": 0.3516483516483517,
      "grad_norm": 1.9427584409713745,
      "learning_rate": 3.516483516483517e-05,
      "loss": 2.9322,
      "step": 128
    },
    {
      "epoch": 0.3543956043956044,
      "grad_norm": 1.785679817199707,
      "learning_rate": 3.5439560439560446e-05,
      "loss": 2.8734,
      "step": 129
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 1.833131194114685,
      "learning_rate": 3.571428571428572e-05,
      "loss": 2.8293,
      "step": 130
    },
    {
      "epoch": 0.3598901098901099,
      "grad_norm": 1.8129123449325562,
      "learning_rate": 3.598901098901099e-05,
      "loss": 2.8327,
      "step": 131
    },
    {
      "epoch": 0.3626373626373626,
      "grad_norm": 1.858169436454773,
      "learning_rate": 3.6263736263736266e-05,
      "loss": 2.8745,
      "step": 132
    },
    {
      "epoch": 0.36538461538461536,
      "grad_norm": 1.857663631439209,
      "learning_rate": 3.653846153846154e-05,
      "loss": 2.8095,
      "step": 133
    },
    {
      "epoch": 0.36813186813186816,
      "grad_norm": 2.025033950805664,
      "learning_rate": 3.6813186813186815e-05,
      "loss": 2.8029,
      "step": 134
    },
    {
      "epoch": 0.3708791208791209,
      "grad_norm": 1.8506360054016113,
      "learning_rate": 3.708791208791209e-05,
      "loss": 2.8892,
      "step": 135
    },
    {
      "epoch": 0.37362637362637363,
      "grad_norm": 1.8684399127960205,
      "learning_rate": 3.7362637362637365e-05,
      "loss": 2.7691,
      "step": 136
    },
    {
      "epoch": 0.37637362637362637,
      "grad_norm": 1.817342758178711,
      "learning_rate": 3.7637362637362636e-05,
      "loss": 2.8446,
      "step": 137
    },
    {
      "epoch": 0.3791208791208791,
      "grad_norm": 1.865785837173462,
      "learning_rate": 3.7912087912087914e-05,
      "loss": 2.7323,
      "step": 138
    },
    {
      "epoch": 0.38186813186813184,
      "grad_norm": 1.9865386486053467,
      "learning_rate": 3.8186813186813185e-05,
      "loss": 2.792,
      "step": 139
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 1.9063862562179565,
      "learning_rate": 3.846153846153846e-05,
      "loss": 2.817,
      "step": 140
    },
    {
      "epoch": 0.3873626373626374,
      "grad_norm": 1.871556282043457,
      "learning_rate": 3.873626373626374e-05,
      "loss": 2.8028,
      "step": 141
    },
    {
      "epoch": 0.3901098901098901,
      "grad_norm": 1.7690999507904053,
      "learning_rate": 3.901098901098901e-05,
      "loss": 2.7758,
      "step": 142
    },
    {
      "epoch": 0.39285714285714285,
      "grad_norm": 1.80843186378479,
      "learning_rate": 3.928571428571429e-05,
      "loss": 2.7181,
      "step": 143
    },
    {
      "epoch": 0.3956043956043956,
      "grad_norm": 1.8864679336547852,
      "learning_rate": 3.956043956043956e-05,
      "loss": 2.8074,
      "step": 144
    },
    {
      "epoch": 0.3983516483516483,
      "grad_norm": 1.782906174659729,
      "learning_rate": 3.983516483516483e-05,
      "loss": 2.8158,
      "step": 145
    },
    {
      "epoch": 0.4010989010989011,
      "grad_norm": 1.9059228897094727,
      "learning_rate": 4.010989010989011e-05,
      "loss": 2.7695,
      "step": 146
    },
    {
      "epoch": 0.40384615384615385,
      "grad_norm": 1.8609648942947388,
      "learning_rate": 4.038461538461539e-05,
      "loss": 2.8075,
      "step": 147
    },
    {
      "epoch": 0.4065934065934066,
      "grad_norm": 1.816511869430542,
      "learning_rate": 4.065934065934066e-05,
      "loss": 2.6919,
      "step": 148
    },
    {
      "epoch": 0.40934065934065933,
      "grad_norm": 1.8784290552139282,
      "learning_rate": 4.093406593406594e-05,
      "loss": 2.7096,
      "step": 149
    },
    {
      "epoch": 0.41208791208791207,
      "grad_norm": 1.8354346752166748,
      "learning_rate": 4.120879120879121e-05,
      "loss": 2.7939,
      "step": 150
    },
    {
      "epoch": 0.41483516483516486,
      "grad_norm": 1.8181756734848022,
      "learning_rate": 4.148351648351649e-05,
      "loss": 2.7717,
      "step": 151
    },
    {
      "epoch": 0.4175824175824176,
      "grad_norm": 1.835263967514038,
      "learning_rate": 4.1758241758241765e-05,
      "loss": 2.5935,
      "step": 152
    },
    {
      "epoch": 0.42032967032967034,
      "grad_norm": 1.7749336957931519,
      "learning_rate": 4.2032967032967036e-05,
      "loss": 2.7407,
      "step": 153
    },
    {
      "epoch": 0.4230769230769231,
      "grad_norm": 1.7637940645217896,
      "learning_rate": 4.230769230769231e-05,
      "loss": 2.7178,
      "step": 154
    },
    {
      "epoch": 0.4258241758241758,
      "grad_norm": 1.766371250152588,
      "learning_rate": 4.2582417582417585e-05,
      "loss": 2.7182,
      "step": 155
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 1.9264582395553589,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 2.7787,
      "step": 156
    },
    {
      "epoch": 0.43131868131868134,
      "grad_norm": 1.7958141565322876,
      "learning_rate": 4.3131868131868134e-05,
      "loss": 2.6657,
      "step": 157
    },
    {
      "epoch": 0.4340659340659341,
      "grad_norm": 1.7843196392059326,
      "learning_rate": 4.340659340659341e-05,
      "loss": 2.7004,
      "step": 158
    },
    {
      "epoch": 0.4368131868131868,
      "grad_norm": 1.737240195274353,
      "learning_rate": 4.3681318681318683e-05,
      "loss": 2.6663,
      "step": 159
    },
    {
      "epoch": 0.43956043956043955,
      "grad_norm": 1.8179359436035156,
      "learning_rate": 4.3956043956043955e-05,
      "loss": 2.7249,
      "step": 160
    },
    {
      "epoch": 0.4423076923076923,
      "grad_norm": 1.801227331161499,
      "learning_rate": 4.423076923076923e-05,
      "loss": 2.6619,
      "step": 161
    },
    {
      "epoch": 0.44505494505494503,
      "grad_norm": 1.7873129844665527,
      "learning_rate": 4.4505494505494504e-05,
      "loss": 2.6275,
      "step": 162
    },
    {
      "epoch": 0.4478021978021978,
      "grad_norm": 1.8113268613815308,
      "learning_rate": 4.478021978021978e-05,
      "loss": 2.6263,
      "step": 163
    },
    {
      "epoch": 0.45054945054945056,
      "grad_norm": 1.8451288938522339,
      "learning_rate": 4.505494505494506e-05,
      "loss": 2.683,
      "step": 164
    },
    {
      "epoch": 0.4532967032967033,
      "grad_norm": 1.8357001543045044,
      "learning_rate": 4.532967032967033e-05,
      "loss": 2.6085,
      "step": 165
    },
    {
      "epoch": 0.45604395604395603,
      "grad_norm": 1.7927731275558472,
      "learning_rate": 4.56043956043956e-05,
      "loss": 2.729,
      "step": 166
    },
    {
      "epoch": 0.45879120879120877,
      "grad_norm": 1.8401139974594116,
      "learning_rate": 4.587912087912088e-05,
      "loss": 2.699,
      "step": 167
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 1.7115384340286255,
      "learning_rate": 4.615384615384616e-05,
      "loss": 2.5831,
      "step": 168
    },
    {
      "epoch": 0.4642857142857143,
      "grad_norm": 1.7190330028533936,
      "learning_rate": 4.642857142857143e-05,
      "loss": 2.5809,
      "step": 169
    },
    {
      "epoch": 0.46703296703296704,
      "grad_norm": 2.0098836421966553,
      "learning_rate": 4.670329670329671e-05,
      "loss": 2.6416,
      "step": 170
    },
    {
      "epoch": 0.4697802197802198,
      "grad_norm": 1.9117839336395264,
      "learning_rate": 4.697802197802198e-05,
      "loss": 2.6164,
      "step": 171
    },
    {
      "epoch": 0.4725274725274725,
      "grad_norm": 1.9044749736785889,
      "learning_rate": 4.7252747252747257e-05,
      "loss": 2.6424,
      "step": 172
    },
    {
      "epoch": 0.47527472527472525,
      "grad_norm": 1.8196566104888916,
      "learning_rate": 4.752747252747253e-05,
      "loss": 2.6182,
      "step": 173
    },
    {
      "epoch": 0.47802197802197804,
      "grad_norm": 1.7407526969909668,
      "learning_rate": 4.7802197802197806e-05,
      "loss": 2.5636,
      "step": 174
    },
    {
      "epoch": 0.4807692307692308,
      "grad_norm": 1.752821683883667,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 2.561,
      "step": 175
    },
    {
      "epoch": 0.4835164835164835,
      "grad_norm": 1.680580973625183,
      "learning_rate": 4.8351648351648355e-05,
      "loss": 2.5506,
      "step": 176
    },
    {
      "epoch": 0.48626373626373626,
      "grad_norm": 1.8647080659866333,
      "learning_rate": 4.8626373626373626e-05,
      "loss": 2.6039,
      "step": 177
    },
    {
      "epoch": 0.489010989010989,
      "grad_norm": 1.7273218631744385,
      "learning_rate": 4.8901098901098904e-05,
      "loss": 2.5651,
      "step": 178
    },
    {
      "epoch": 0.49175824175824173,
      "grad_norm": 1.7525043487548828,
      "learning_rate": 4.9175824175824175e-05,
      "loss": 2.5723,
      "step": 179
    },
    {
      "epoch": 0.4945054945054945,
      "grad_norm": 1.6972748041152954,
      "learning_rate": 4.945054945054945e-05,
      "loss": 2.5726,
      "step": 180
    },
    {
      "epoch": 0.49725274725274726,
      "grad_norm": 1.7132107019424438,
      "learning_rate": 4.972527472527473e-05,
      "loss": 2.5446,
      "step": 181
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.8020648956298828,
      "learning_rate": 5e-05,
      "loss": 2.5128,
      "step": 182
    },
    {
      "epoch": 0.5,
      "eval_loss": 2.571676015853882,
      "eval_runtime": 94.4147,
      "eval_samples_per_second": 438.544,
      "eval_steps_per_second": 3.432,
      "step": 182
    },
    {
      "epoch": 0.5027472527472527,
      "grad_norm": 1.7039655447006226,
      "learning_rate": 5e-05,
      "loss": 2.4955,
      "step": 183
    },
    {
      "epoch": 0.5054945054945055,
      "grad_norm": 1.7512794733047485,
      "learning_rate": 5e-05,
      "loss": 2.5306,
      "step": 184
    },
    {
      "epoch": 0.5082417582417582,
      "grad_norm": 1.72220778465271,
      "learning_rate": 5e-05,
      "loss": 2.5028,
      "step": 185
    },
    {
      "epoch": 0.510989010989011,
      "grad_norm": 1.7146470546722412,
      "learning_rate": 5e-05,
      "loss": 2.4866,
      "step": 186
    },
    {
      "epoch": 0.5137362637362637,
      "grad_norm": 1.7559303045272827,
      "learning_rate": 5e-05,
      "loss": 2.5231,
      "step": 187
    },
    {
      "epoch": 0.5164835164835165,
      "grad_norm": 1.7240530252456665,
      "learning_rate": 5e-05,
      "loss": 2.58,
      "step": 188
    },
    {
      "epoch": 0.5192307692307693,
      "grad_norm": 1.6938302516937256,
      "learning_rate": 5e-05,
      "loss": 2.5346,
      "step": 189
    },
    {
      "epoch": 0.521978021978022,
      "grad_norm": 1.6902612447738647,
      "learning_rate": 5e-05,
      "loss": 2.5083,
      "step": 190
    },
    {
      "epoch": 0.5247252747252747,
      "grad_norm": 1.6685315370559692,
      "learning_rate": 5e-05,
      "loss": 2.4767,
      "step": 191
    },
    {
      "epoch": 0.5274725274725275,
      "grad_norm": 1.7048327922821045,
      "learning_rate": 5e-05,
      "loss": 2.5173,
      "step": 192
    },
    {
      "epoch": 0.5302197802197802,
      "grad_norm": 1.6906830072402954,
      "learning_rate": 5e-05,
      "loss": 2.4877,
      "step": 193
    },
    {
      "epoch": 0.532967032967033,
      "grad_norm": 1.73897123336792,
      "learning_rate": 5e-05,
      "loss": 2.5216,
      "step": 194
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 1.6927063465118408,
      "learning_rate": 5e-05,
      "loss": 2.4798,
      "step": 195
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 1.7024540901184082,
      "learning_rate": 5e-05,
      "loss": 2.4844,
      "step": 196
    },
    {
      "epoch": 0.5412087912087912,
      "grad_norm": 1.6455724239349365,
      "learning_rate": 5e-05,
      "loss": 2.3794,
      "step": 197
    },
    {
      "epoch": 0.5439560439560439,
      "grad_norm": 1.7207512855529785,
      "learning_rate": 5e-05,
      "loss": 2.4654,
      "step": 198
    },
    {
      "epoch": 0.5467032967032966,
      "grad_norm": 1.6607216596603394,
      "learning_rate": 5e-05,
      "loss": 2.4212,
      "step": 199
    },
    {
      "epoch": 0.5494505494505495,
      "grad_norm": 1.6890833377838135,
      "learning_rate": 5e-05,
      "loss": 2.4314,
      "step": 200
    },
    {
      "epoch": 0.5521978021978022,
      "grad_norm": 1.6272811889648438,
      "learning_rate": 5e-05,
      "loss": 2.4754,
      "step": 201
    },
    {
      "epoch": 0.554945054945055,
      "grad_norm": 1.6351059675216675,
      "learning_rate": 5e-05,
      "loss": 2.4088,
      "step": 202
    },
    {
      "epoch": 0.5576923076923077,
      "grad_norm": 1.7080113887786865,
      "learning_rate": 5e-05,
      "loss": 2.4721,
      "step": 203
    },
    {
      "epoch": 0.5604395604395604,
      "grad_norm": 1.6129796504974365,
      "learning_rate": 5e-05,
      "loss": 2.4742,
      "step": 204
    },
    {
      "epoch": 0.5631868131868132,
      "grad_norm": 1.6562845706939697,
      "learning_rate": 5e-05,
      "loss": 2.3665,
      "step": 205
    },
    {
      "epoch": 0.5659340659340659,
      "grad_norm": 1.6607834100723267,
      "learning_rate": 5e-05,
      "loss": 2.4463,
      "step": 206
    },
    {
      "epoch": 0.5686813186813187,
      "grad_norm": 1.7035841941833496,
      "learning_rate": 5e-05,
      "loss": 2.352,
      "step": 207
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.58823561668396,
      "learning_rate": 5e-05,
      "loss": 2.3679,
      "step": 208
    },
    {
      "epoch": 0.5741758241758241,
      "grad_norm": 1.7116628885269165,
      "learning_rate": 5e-05,
      "loss": 2.5557,
      "step": 209
    },
    {
      "epoch": 0.5769230769230769,
      "grad_norm": 1.594409704208374,
      "learning_rate": 5e-05,
      "loss": 2.4485,
      "step": 210
    },
    {
      "epoch": 0.5796703296703297,
      "grad_norm": 1.7334063053131104,
      "learning_rate": 5e-05,
      "loss": 2.4153,
      "step": 211
    },
    {
      "epoch": 0.5824175824175825,
      "grad_norm": 1.6772353649139404,
      "learning_rate": 5e-05,
      "loss": 2.4434,
      "step": 212
    },
    {
      "epoch": 0.5851648351648352,
      "grad_norm": 1.6082125902175903,
      "learning_rate": 5e-05,
      "loss": 2.4152,
      "step": 213
    },
    {
      "epoch": 0.5879120879120879,
      "grad_norm": 1.6474627256393433,
      "learning_rate": 5e-05,
      "loss": 2.5329,
      "step": 214
    },
    {
      "epoch": 0.5906593406593407,
      "grad_norm": 1.6037461757659912,
      "learning_rate": 5e-05,
      "loss": 2.3881,
      "step": 215
    },
    {
      "epoch": 0.5934065934065934,
      "grad_norm": 1.7498493194580078,
      "learning_rate": 5e-05,
      "loss": 2.4206,
      "step": 216
    },
    {
      "epoch": 0.5961538461538461,
      "grad_norm": 1.6257489919662476,
      "learning_rate": 5e-05,
      "loss": 2.4487,
      "step": 217
    },
    {
      "epoch": 0.5989010989010989,
      "grad_norm": 1.6277769804000854,
      "learning_rate": 5e-05,
      "loss": 2.3771,
      "step": 218
    },
    {
      "epoch": 0.6016483516483516,
      "grad_norm": 1.6125768423080444,
      "learning_rate": 5e-05,
      "loss": 2.364,
      "step": 219
    },
    {
      "epoch": 0.6043956043956044,
      "grad_norm": 1.6721078157424927,
      "learning_rate": 5e-05,
      "loss": 2.2975,
      "step": 220
    },
    {
      "epoch": 0.6071428571428571,
      "grad_norm": 1.6677743196487427,
      "learning_rate": 5e-05,
      "loss": 2.3689,
      "step": 221
    },
    {
      "epoch": 0.6098901098901099,
      "grad_norm": 1.6858372688293457,
      "learning_rate": 5e-05,
      "loss": 2.3675,
      "step": 222
    },
    {
      "epoch": 0.6126373626373627,
      "grad_norm": 1.6470950841903687,
      "learning_rate": 5e-05,
      "loss": 2.3342,
      "step": 223
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 1.573935866355896,
      "learning_rate": 5e-05,
      "loss": 2.27,
      "step": 224
    },
    {
      "epoch": 0.6181318681318682,
      "grad_norm": 1.6056450605392456,
      "learning_rate": 5e-05,
      "loss": 2.3426,
      "step": 225
    },
    {
      "epoch": 0.6208791208791209,
      "grad_norm": 1.574268102645874,
      "learning_rate": 5e-05,
      "loss": 2.3599,
      "step": 226
    },
    {
      "epoch": 0.6236263736263736,
      "grad_norm": 1.6403473615646362,
      "learning_rate": 5e-05,
      "loss": 2.3425,
      "step": 227
    },
    {
      "epoch": 0.6263736263736264,
      "grad_norm": 1.6363574266433716,
      "learning_rate": 5e-05,
      "loss": 2.3197,
      "step": 228
    },
    {
      "epoch": 0.6291208791208791,
      "grad_norm": 1.6185778379440308,
      "learning_rate": 5e-05,
      "loss": 2.3783,
      "step": 229
    },
    {
      "epoch": 0.6318681318681318,
      "grad_norm": 1.5758352279663086,
      "learning_rate": 5e-05,
      "loss": 2.2765,
      "step": 230
    },
    {
      "epoch": 0.6346153846153846,
      "grad_norm": 1.683774471282959,
      "learning_rate": 5e-05,
      "loss": 2.3734,
      "step": 231
    },
    {
      "epoch": 0.6373626373626373,
      "grad_norm": 1.678054690361023,
      "learning_rate": 5e-05,
      "loss": 2.3752,
      "step": 232
    },
    {
      "epoch": 0.6401098901098901,
      "grad_norm": 1.6398708820343018,
      "learning_rate": 5e-05,
      "loss": 2.3409,
      "step": 233
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 1.6273335218429565,
      "learning_rate": 5e-05,
      "loss": 2.3286,
      "step": 234
    },
    {
      "epoch": 0.6456043956043956,
      "grad_norm": 1.5776371955871582,
      "learning_rate": 5e-05,
      "loss": 2.3181,
      "step": 235
    },
    {
      "epoch": 0.6483516483516484,
      "grad_norm": 1.55674147605896,
      "learning_rate": 5e-05,
      "loss": 2.2854,
      "step": 236
    },
    {
      "epoch": 0.6510989010989011,
      "grad_norm": 1.6147643327713013,
      "learning_rate": 5e-05,
      "loss": 2.3021,
      "step": 237
    },
    {
      "epoch": 0.6538461538461539,
      "grad_norm": 1.5944563150405884,
      "learning_rate": 5e-05,
      "loss": 2.3059,
      "step": 238
    },
    {
      "epoch": 0.6565934065934066,
      "grad_norm": 1.5473041534423828,
      "learning_rate": 5e-05,
      "loss": 2.2271,
      "step": 239
    },
    {
      "epoch": 0.6593406593406593,
      "grad_norm": 1.5958088636398315,
      "learning_rate": 5e-05,
      "loss": 2.3527,
      "step": 240
    },
    {
      "epoch": 0.6620879120879121,
      "grad_norm": 1.6478958129882812,
      "learning_rate": 5e-05,
      "loss": 2.2945,
      "step": 241
    },
    {
      "epoch": 0.6648351648351648,
      "grad_norm": 1.5686142444610596,
      "learning_rate": 5e-05,
      "loss": 2.2534,
      "step": 242
    },
    {
      "epoch": 0.6675824175824175,
      "grad_norm": 1.6278791427612305,
      "learning_rate": 5e-05,
      "loss": 2.2983,
      "step": 243
    },
    {
      "epoch": 0.6703296703296703,
      "grad_norm": 1.603527545928955,
      "learning_rate": 5e-05,
      "loss": 2.2741,
      "step": 244
    },
    {
      "epoch": 0.6730769230769231,
      "grad_norm": 1.5868585109710693,
      "learning_rate": 5e-05,
      "loss": 2.3194,
      "step": 245
    },
    {
      "epoch": 0.6758241758241759,
      "grad_norm": 1.5823222398757935,
      "learning_rate": 5e-05,
      "loss": 2.2134,
      "step": 246
    },
    {
      "epoch": 0.6785714285714286,
      "grad_norm": 1.677514910697937,
      "learning_rate": 5e-05,
      "loss": 2.2729,
      "step": 247
    },
    {
      "epoch": 0.6813186813186813,
      "grad_norm": 1.6037518978118896,
      "learning_rate": 5e-05,
      "loss": 2.2099,
      "step": 248
    },
    {
      "epoch": 0.6840659340659341,
      "grad_norm": 1.8128677606582642,
      "learning_rate": 5e-05,
      "loss": 2.2968,
      "step": 249
    },
    {
      "epoch": 0.6868131868131868,
      "grad_norm": 1.648432731628418,
      "learning_rate": 5e-05,
      "loss": 2.2829,
      "step": 250
    },
    {
      "epoch": 0.6895604395604396,
      "grad_norm": 1.6041582822799683,
      "learning_rate": 5e-05,
      "loss": 2.3209,
      "step": 251
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 1.5838567018508911,
      "learning_rate": 5e-05,
      "loss": 2.2842,
      "step": 252
    },
    {
      "epoch": 0.695054945054945,
      "grad_norm": 1.5804598331451416,
      "learning_rate": 5e-05,
      "loss": 2.2273,
      "step": 253
    },
    {
      "epoch": 0.6978021978021978,
      "grad_norm": 1.6003515720367432,
      "learning_rate": 5e-05,
      "loss": 2.2501,
      "step": 254
    },
    {
      "epoch": 0.7005494505494505,
      "grad_norm": 1.6126832962036133,
      "learning_rate": 5e-05,
      "loss": 2.3116,
      "step": 255
    },
    {
      "epoch": 0.7032967032967034,
      "grad_norm": 1.546285629272461,
      "learning_rate": 5e-05,
      "loss": 2.1639,
      "step": 256
    },
    {
      "epoch": 0.7060439560439561,
      "grad_norm": 1.6295245885849,
      "learning_rate": 5e-05,
      "loss": 2.2899,
      "step": 257
    },
    {
      "epoch": 0.7087912087912088,
      "grad_norm": 1.551002860069275,
      "learning_rate": 5e-05,
      "loss": 2.1153,
      "step": 258
    },
    {
      "epoch": 0.7115384615384616,
      "grad_norm": 1.5750362873077393,
      "learning_rate": 5e-05,
      "loss": 2.241,
      "step": 259
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 1.6211134195327759,
      "learning_rate": 5e-05,
      "loss": 2.2554,
      "step": 260
    },
    {
      "epoch": 0.717032967032967,
      "grad_norm": 1.5940446853637695,
      "learning_rate": 5e-05,
      "loss": 2.2054,
      "step": 261
    },
    {
      "epoch": 0.7197802197802198,
      "grad_norm": 1.5368924140930176,
      "learning_rate": 5e-05,
      "loss": 2.1363,
      "step": 262
    },
    {
      "epoch": 0.7225274725274725,
      "grad_norm": 1.5660583972930908,
      "learning_rate": 5e-05,
      "loss": 2.229,
      "step": 263
    },
    {
      "epoch": 0.7252747252747253,
      "grad_norm": 1.5804519653320312,
      "learning_rate": 5e-05,
      "loss": 2.2457,
      "step": 264
    },
    {
      "epoch": 0.728021978021978,
      "grad_norm": 1.5644958019256592,
      "learning_rate": 5e-05,
      "loss": 2.2227,
      "step": 265
    },
    {
      "epoch": 0.7307692307692307,
      "grad_norm": 1.6100096702575684,
      "learning_rate": 5e-05,
      "loss": 2.2017,
      "step": 266
    },
    {
      "epoch": 0.7335164835164835,
      "grad_norm": 1.5467875003814697,
      "learning_rate": 5e-05,
      "loss": 2.2346,
      "step": 267
    },
    {
      "epoch": 0.7362637362637363,
      "grad_norm": 1.5923842191696167,
      "learning_rate": 5e-05,
      "loss": 2.2108,
      "step": 268
    },
    {
      "epoch": 0.739010989010989,
      "grad_norm": 1.559444546699524,
      "learning_rate": 5e-05,
      "loss": 2.1889,
      "step": 269
    },
    {
      "epoch": 0.7417582417582418,
      "grad_norm": 1.6012766361236572,
      "learning_rate": 5e-05,
      "loss": 2.199,
      "step": 270
    },
    {
      "epoch": 0.7445054945054945,
      "grad_norm": 1.5103129148483276,
      "learning_rate": 5e-05,
      "loss": 2.1965,
      "step": 271
    },
    {
      "epoch": 0.7472527472527473,
      "grad_norm": 1.5250959396362305,
      "learning_rate": 5e-05,
      "loss": 2.1989,
      "step": 272
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.5046128034591675,
      "learning_rate": 5e-05,
      "loss": 2.2622,
      "step": 273
    },
    {
      "epoch": 0.7527472527472527,
      "grad_norm": 1.629336953163147,
      "learning_rate": 5e-05,
      "loss": 2.2343,
      "step": 274
    },
    {
      "epoch": 0.7554945054945055,
      "grad_norm": 1.5674529075622559,
      "learning_rate": 5e-05,
      "loss": 2.1563,
      "step": 275
    },
    {
      "epoch": 0.7582417582417582,
      "grad_norm": 1.5155147314071655,
      "learning_rate": 5e-05,
      "loss": 2.1601,
      "step": 276
    },
    {
      "epoch": 0.760989010989011,
      "grad_norm": 1.5943288803100586,
      "learning_rate": 5e-05,
      "loss": 2.2358,
      "step": 277
    },
    {
      "epoch": 0.7637362637362637,
      "grad_norm": 1.5893968343734741,
      "learning_rate": 5e-05,
      "loss": 2.1942,
      "step": 278
    },
    {
      "epoch": 0.7664835164835165,
      "grad_norm": 1.5482678413391113,
      "learning_rate": 5e-05,
      "loss": 2.117,
      "step": 279
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 1.5336225032806396,
      "learning_rate": 5e-05,
      "loss": 2.1451,
      "step": 280
    },
    {
      "epoch": 0.771978021978022,
      "grad_norm": 1.558097243309021,
      "learning_rate": 5e-05,
      "loss": 2.1608,
      "step": 281
    },
    {
      "epoch": 0.7747252747252747,
      "grad_norm": 1.5149776935577393,
      "learning_rate": 5e-05,
      "loss": 2.1456,
      "step": 282
    },
    {
      "epoch": 0.7774725274725275,
      "grad_norm": 1.5252652168273926,
      "learning_rate": 5e-05,
      "loss": 2.168,
      "step": 283
    },
    {
      "epoch": 0.7802197802197802,
      "grad_norm": 1.5429894924163818,
      "learning_rate": 5e-05,
      "loss": 2.1425,
      "step": 284
    },
    {
      "epoch": 0.782967032967033,
      "grad_norm": 1.5542716979980469,
      "learning_rate": 5e-05,
      "loss": 2.2166,
      "step": 285
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 1.5422406196594238,
      "learning_rate": 5e-05,
      "loss": 2.2175,
      "step": 286
    },
    {
      "epoch": 0.7884615384615384,
      "grad_norm": 1.5504862070083618,
      "learning_rate": 5e-05,
      "loss": 2.1351,
      "step": 287
    },
    {
      "epoch": 0.7912087912087912,
      "grad_norm": 1.5678826570510864,
      "learning_rate": 5e-05,
      "loss": 2.1614,
      "step": 288
    },
    {
      "epoch": 0.7939560439560439,
      "grad_norm": 1.515020489692688,
      "learning_rate": 5e-05,
      "loss": 2.1442,
      "step": 289
    },
    {
      "epoch": 0.7967032967032966,
      "grad_norm": 1.5329023599624634,
      "learning_rate": 5e-05,
      "loss": 2.2122,
      "step": 290
    },
    {
      "epoch": 0.7994505494505495,
      "grad_norm": 1.5132819414138794,
      "learning_rate": 5e-05,
      "loss": 2.178,
      "step": 291
    },
    {
      "epoch": 0.8021978021978022,
      "grad_norm": 1.712611198425293,
      "learning_rate": 5e-05,
      "loss": 2.105,
      "step": 292
    },
    {
      "epoch": 0.804945054945055,
      "grad_norm": 1.593907117843628,
      "learning_rate": 5e-05,
      "loss": 2.0251,
      "step": 293
    },
    {
      "epoch": 0.8076923076923077,
      "grad_norm": 1.532799482345581,
      "learning_rate": 5e-05,
      "loss": 2.1857,
      "step": 294
    },
    {
      "epoch": 0.8104395604395604,
      "grad_norm": 1.6022045612335205,
      "learning_rate": 5e-05,
      "loss": 2.1519,
      "step": 295
    },
    {
      "epoch": 0.8131868131868132,
      "grad_norm": 1.507449984550476,
      "learning_rate": 5e-05,
      "loss": 2.1415,
      "step": 296
    },
    {
      "epoch": 0.8159340659340659,
      "grad_norm": 1.5465844869613647,
      "learning_rate": 5e-05,
      "loss": 2.1417,
      "step": 297
    },
    {
      "epoch": 0.8186813186813187,
      "grad_norm": 1.5465301275253296,
      "learning_rate": 5e-05,
      "loss": 2.1133,
      "step": 298
    },
    {
      "epoch": 0.8214285714285714,
      "grad_norm": 1.5629429817199707,
      "learning_rate": 5e-05,
      "loss": 2.1173,
      "step": 299
    },
    {
      "epoch": 0.8241758241758241,
      "grad_norm": 1.5857782363891602,
      "learning_rate": 5e-05,
      "loss": 2.1186,
      "step": 300
    },
    {
      "epoch": 0.8269230769230769,
      "grad_norm": 1.539143681526184,
      "learning_rate": 5e-05,
      "loss": 2.0606,
      "step": 301
    },
    {
      "epoch": 0.8296703296703297,
      "grad_norm": 1.546188235282898,
      "learning_rate": 5e-05,
      "loss": 2.1098,
      "step": 302
    },
    {
      "epoch": 0.8324175824175825,
      "grad_norm": 1.5334500074386597,
      "learning_rate": 5e-05,
      "loss": 2.1381,
      "step": 303
    },
    {
      "epoch": 0.8351648351648352,
      "grad_norm": 1.4904491901397705,
      "learning_rate": 5e-05,
      "loss": 2.0619,
      "step": 304
    },
    {
      "epoch": 0.8379120879120879,
      "grad_norm": 1.5400007963180542,
      "learning_rate": 5e-05,
      "loss": 2.095,
      "step": 305
    },
    {
      "epoch": 0.8406593406593407,
      "grad_norm": 1.5019904375076294,
      "learning_rate": 5e-05,
      "loss": 2.0325,
      "step": 306
    },
    {
      "epoch": 0.8434065934065934,
      "grad_norm": 1.5511103868484497,
      "learning_rate": 5e-05,
      "loss": 2.1196,
      "step": 307
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 1.567030429840088,
      "learning_rate": 5e-05,
      "loss": 2.1048,
      "step": 308
    },
    {
      "epoch": 0.8489010989010989,
      "grad_norm": 1.5151339769363403,
      "learning_rate": 5e-05,
      "loss": 2.0106,
      "step": 309
    },
    {
      "epoch": 0.8516483516483516,
      "grad_norm": 1.537487506866455,
      "learning_rate": 5e-05,
      "loss": 2.1286,
      "step": 310
    },
    {
      "epoch": 0.8543956043956044,
      "grad_norm": 1.5263086557388306,
      "learning_rate": 5e-05,
      "loss": 2.0653,
      "step": 311
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 1.4805383682250977,
      "learning_rate": 5e-05,
      "loss": 2.1139,
      "step": 312
    },
    {
      "epoch": 0.8598901098901099,
      "grad_norm": 1.4311367273330688,
      "learning_rate": 5e-05,
      "loss": 2.0331,
      "step": 313
    },
    {
      "epoch": 0.8626373626373627,
      "grad_norm": 1.5148742198944092,
      "learning_rate": 5e-05,
      "loss": 2.2267,
      "step": 314
    },
    {
      "epoch": 0.8653846153846154,
      "grad_norm": 1.5204427242279053,
      "learning_rate": 5e-05,
      "loss": 2.1321,
      "step": 315
    },
    {
      "epoch": 0.8681318681318682,
      "grad_norm": 1.5378845930099487,
      "learning_rate": 5e-05,
      "loss": 2.011,
      "step": 316
    },
    {
      "epoch": 0.8708791208791209,
      "grad_norm": 1.4486379623413086,
      "learning_rate": 5e-05,
      "loss": 1.994,
      "step": 317
    },
    {
      "epoch": 0.8736263736263736,
      "grad_norm": 1.5732662677764893,
      "learning_rate": 5e-05,
      "loss": 2.0282,
      "step": 318
    },
    {
      "epoch": 0.8763736263736264,
      "grad_norm": 1.5466514825820923,
      "learning_rate": 5e-05,
      "loss": 2.0168,
      "step": 319
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 1.4673750400543213,
      "learning_rate": 5e-05,
      "loss": 2.0233,
      "step": 320
    },
    {
      "epoch": 0.8818681318681318,
      "grad_norm": 1.459873080253601,
      "learning_rate": 5e-05,
      "loss": 2.0547,
      "step": 321
    },
    {
      "epoch": 0.8846153846153846,
      "grad_norm": 1.5440630912780762,
      "learning_rate": 5e-05,
      "loss": 2.181,
      "step": 322
    },
    {
      "epoch": 0.8873626373626373,
      "grad_norm": 1.4801167249679565,
      "learning_rate": 5e-05,
      "loss": 2.0844,
      "step": 323
    },
    {
      "epoch": 0.8901098901098901,
      "grad_norm": 1.5193971395492554,
      "learning_rate": 5e-05,
      "loss": 2.0846,
      "step": 324
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 1.4770071506500244,
      "learning_rate": 5e-05,
      "loss": 2.0382,
      "step": 325
    },
    {
      "epoch": 0.8956043956043956,
      "grad_norm": 1.5363045930862427,
      "learning_rate": 5e-05,
      "loss": 2.141,
      "step": 326
    },
    {
      "epoch": 0.8983516483516484,
      "grad_norm": 1.5640279054641724,
      "learning_rate": 5e-05,
      "loss": 2.0708,
      "step": 327
    },
    {
      "epoch": 0.9010989010989011,
      "grad_norm": 1.496978998184204,
      "learning_rate": 5e-05,
      "loss": 1.9968,
      "step": 328
    },
    {
      "epoch": 0.9038461538461539,
      "grad_norm": 1.4991785287857056,
      "learning_rate": 5e-05,
      "loss": 2.1106,
      "step": 329
    },
    {
      "epoch": 0.9065934065934066,
      "grad_norm": 1.4661716222763062,
      "learning_rate": 5e-05,
      "loss": 2.0087,
      "step": 330
    },
    {
      "epoch": 0.9093406593406593,
      "grad_norm": 1.439339518547058,
      "learning_rate": 5e-05,
      "loss": 2.0561,
      "step": 331
    },
    {
      "epoch": 0.9120879120879121,
      "grad_norm": 1.4855176210403442,
      "learning_rate": 5e-05,
      "loss": 2.0197,
      "step": 332
    },
    {
      "epoch": 0.9148351648351648,
      "grad_norm": 1.4583423137664795,
      "learning_rate": 5e-05,
      "loss": 2.0116,
      "step": 333
    },
    {
      "epoch": 0.9175824175824175,
      "grad_norm": 1.4341013431549072,
      "learning_rate": 5e-05,
      "loss": 2.009,
      "step": 334
    },
    {
      "epoch": 0.9203296703296703,
      "grad_norm": 1.4821529388427734,
      "learning_rate": 5e-05,
      "loss": 1.9925,
      "step": 335
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 1.4898866415023804,
      "learning_rate": 5e-05,
      "loss": 2.0127,
      "step": 336
    },
    {
      "epoch": 0.9258241758241759,
      "grad_norm": 1.5168020725250244,
      "learning_rate": 5e-05,
      "loss": 2.0674,
      "step": 337
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 1.4764726161956787,
      "learning_rate": 5e-05,
      "loss": 2.077,
      "step": 338
    },
    {
      "epoch": 0.9313186813186813,
      "grad_norm": 1.4823263883590698,
      "learning_rate": 5e-05,
      "loss": 1.997,
      "step": 339
    },
    {
      "epoch": 0.9340659340659341,
      "grad_norm": 1.4356162548065186,
      "learning_rate": 5e-05,
      "loss": 2.0268,
      "step": 340
    },
    {
      "epoch": 0.9368131868131868,
      "grad_norm": 1.5177897214889526,
      "learning_rate": 5e-05,
      "loss": 2.0463,
      "step": 341
    },
    {
      "epoch": 0.9395604395604396,
      "grad_norm": 1.4562934637069702,
      "learning_rate": 5e-05,
      "loss": 1.9998,
      "step": 342
    },
    {
      "epoch": 0.9423076923076923,
      "grad_norm": 1.4446477890014648,
      "learning_rate": 5e-05,
      "loss": 1.9906,
      "step": 343
    },
    {
      "epoch": 0.945054945054945,
      "grad_norm": 1.506056785583496,
      "learning_rate": 5e-05,
      "loss": 1.9997,
      "step": 344
    },
    {
      "epoch": 0.9478021978021978,
      "grad_norm": 1.4548804759979248,
      "learning_rate": 5e-05,
      "loss": 1.9259,
      "step": 345
    },
    {
      "epoch": 0.9505494505494505,
      "grad_norm": 1.5136079788208008,
      "learning_rate": 5e-05,
      "loss": 2.0186,
      "step": 346
    },
    {
      "epoch": 0.9532967032967034,
      "grad_norm": 1.4326083660125732,
      "learning_rate": 5e-05,
      "loss": 1.9922,
      "step": 347
    },
    {
      "epoch": 0.9560439560439561,
      "grad_norm": 1.4530861377716064,
      "learning_rate": 5e-05,
      "loss": 1.9371,
      "step": 348
    },
    {
      "epoch": 0.9587912087912088,
      "grad_norm": 1.469607949256897,
      "learning_rate": 5e-05,
      "loss": 2.0289,
      "step": 349
    },
    {
      "epoch": 0.9615384615384616,
      "grad_norm": 1.4734480381011963,
      "learning_rate": 5e-05,
      "loss": 2.0305,
      "step": 350
    },
    {
      "epoch": 0.9642857142857143,
      "grad_norm": 1.4450640678405762,
      "learning_rate": 5e-05,
      "loss": 2.0121,
      "step": 351
    },
    {
      "epoch": 0.967032967032967,
      "grad_norm": 1.48783540725708,
      "learning_rate": 5e-05,
      "loss": 2.0307,
      "step": 352
    },
    {
      "epoch": 0.9697802197802198,
      "grad_norm": 1.426539421081543,
      "learning_rate": 5e-05,
      "loss": 1.9874,
      "step": 353
    },
    {
      "epoch": 0.9725274725274725,
      "grad_norm": 1.4489542245864868,
      "learning_rate": 5e-05,
      "loss": 1.9414,
      "step": 354
    },
    {
      "epoch": 0.9752747252747253,
      "grad_norm": 1.466801404953003,
      "learning_rate": 5e-05,
      "loss": 2.028,
      "step": 355
    },
    {
      "epoch": 0.978021978021978,
      "grad_norm": 1.4731504917144775,
      "learning_rate": 5e-05,
      "loss": 1.9464,
      "step": 356
    },
    {
      "epoch": 0.9807692307692307,
      "grad_norm": 1.4953962564468384,
      "learning_rate": 5e-05,
      "loss": 1.9985,
      "step": 357
    },
    {
      "epoch": 0.9835164835164835,
      "grad_norm": 1.4498285055160522,
      "learning_rate": 5e-05,
      "loss": 2.0712,
      "step": 358
    },
    {
      "epoch": 0.9862637362637363,
      "grad_norm": 1.4364043474197388,
      "learning_rate": 5e-05,
      "loss": 1.9814,
      "step": 359
    },
    {
      "epoch": 0.989010989010989,
      "grad_norm": 1.4427597522735596,
      "learning_rate": 5e-05,
      "loss": 2.0141,
      "step": 360
    },
    {
      "epoch": 0.9917582417582418,
      "grad_norm": 1.4659594297409058,
      "learning_rate": 5e-05,
      "loss": 1.961,
      "step": 361
    },
    {
      "epoch": 0.9945054945054945,
      "grad_norm": 1.445909857749939,
      "learning_rate": 5e-05,
      "loss": 1.9731,
      "step": 362
    },
    {
      "epoch": 0.9972527472527473,
      "grad_norm": 1.439174771308899,
      "learning_rate": 5e-05,
      "loss": 2.0151,
      "step": 363
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.4467054605484009,
      "learning_rate": 5e-05,
      "loss": 1.9443,
      "step": 364
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.173699378967285,
      "eval_runtime": 81.678,
      "eval_samples_per_second": 506.93,
      "eval_steps_per_second": 3.967,
      "step": 364
    },
    {
      "epoch": 1.0027472527472527,
      "grad_norm": 1.532974362373352,
      "learning_rate": 5e-05,
      "loss": 1.9049,
      "step": 365
    },
    {
      "epoch": 1.0054945054945055,
      "grad_norm": 1.5223729610443115,
      "learning_rate": 5e-05,
      "loss": 1.9212,
      "step": 366
    },
    {
      "epoch": 1.0082417582417582,
      "grad_norm": 1.4460179805755615,
      "learning_rate": 5e-05,
      "loss": 1.9429,
      "step": 367
    },
    {
      "epoch": 1.010989010989011,
      "grad_norm": 1.4417989253997803,
      "learning_rate": 5e-05,
      "loss": 1.8472,
      "step": 368
    },
    {
      "epoch": 1.0137362637362637,
      "grad_norm": 1.4860484600067139,
      "learning_rate": 5e-05,
      "loss": 1.9699,
      "step": 369
    },
    {
      "epoch": 1.0164835164835164,
      "grad_norm": 1.4741133451461792,
      "learning_rate": 5e-05,
      "loss": 1.9246,
      "step": 370
    },
    {
      "epoch": 1.0192307692307692,
      "grad_norm": 1.4413061141967773,
      "learning_rate": 5e-05,
      "loss": 1.9333,
      "step": 371
    },
    {
      "epoch": 1.021978021978022,
      "grad_norm": 1.534925937652588,
      "learning_rate": 5e-05,
      "loss": 1.9642,
      "step": 372
    },
    {
      "epoch": 1.0247252747252746,
      "grad_norm": 1.4416618347167969,
      "learning_rate": 5e-05,
      "loss": 1.8657,
      "step": 373
    },
    {
      "epoch": 1.0274725274725274,
      "grad_norm": 1.4276071786880493,
      "learning_rate": 5e-05,
      "loss": 1.8656,
      "step": 374
    },
    {
      "epoch": 1.0302197802197801,
      "grad_norm": 1.5197426080703735,
      "learning_rate": 5e-05,
      "loss": 1.9133,
      "step": 375
    },
    {
      "epoch": 1.032967032967033,
      "grad_norm": 1.403001308441162,
      "learning_rate": 5e-05,
      "loss": 1.8316,
      "step": 376
    },
    {
      "epoch": 1.0357142857142858,
      "grad_norm": 1.4590215682983398,
      "learning_rate": 5e-05,
      "loss": 1.9446,
      "step": 377
    },
    {
      "epoch": 1.0384615384615385,
      "grad_norm": 1.4595704078674316,
      "learning_rate": 5e-05,
      "loss": 1.9517,
      "step": 378
    },
    {
      "epoch": 1.0412087912087913,
      "grad_norm": 1.426717758178711,
      "learning_rate": 5e-05,
      "loss": 1.8857,
      "step": 379
    },
    {
      "epoch": 1.043956043956044,
      "grad_norm": 1.4274410009384155,
      "learning_rate": 5e-05,
      "loss": 1.9229,
      "step": 380
    },
    {
      "epoch": 1.0467032967032968,
      "grad_norm": 1.4175342321395874,
      "learning_rate": 5e-05,
      "loss": 1.8942,
      "step": 381
    },
    {
      "epoch": 1.0494505494505495,
      "grad_norm": 1.4306186437606812,
      "learning_rate": 5e-05,
      "loss": 1.9111,
      "step": 382
    },
    {
      "epoch": 1.0521978021978022,
      "grad_norm": 1.4973394870758057,
      "learning_rate": 5e-05,
      "loss": 1.8443,
      "step": 383
    },
    {
      "epoch": 1.054945054945055,
      "grad_norm": 1.4516774415969849,
      "learning_rate": 5e-05,
      "loss": 1.8749,
      "step": 384
    },
    {
      "epoch": 1.0576923076923077,
      "grad_norm": 1.391077995300293,
      "learning_rate": 5e-05,
      "loss": 1.8972,
      "step": 385
    },
    {
      "epoch": 1.0604395604395604,
      "grad_norm": 1.4337005615234375,
      "learning_rate": 5e-05,
      "loss": 1.8735,
      "step": 386
    },
    {
      "epoch": 1.0631868131868132,
      "grad_norm": 1.447556734085083,
      "learning_rate": 5e-05,
      "loss": 1.8163,
      "step": 387
    },
    {
      "epoch": 1.065934065934066,
      "grad_norm": 1.434432029724121,
      "learning_rate": 5e-05,
      "loss": 1.9591,
      "step": 388
    },
    {
      "epoch": 1.0686813186813187,
      "grad_norm": 1.415040373802185,
      "learning_rate": 5e-05,
      "loss": 1.8815,
      "step": 389
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 1.4806569814682007,
      "learning_rate": 5e-05,
      "loss": 1.9325,
      "step": 390
    },
    {
      "epoch": 1.0741758241758241,
      "grad_norm": 1.4140875339508057,
      "learning_rate": 5e-05,
      "loss": 1.8543,
      "step": 391
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 1.4118571281433105,
      "learning_rate": 5e-05,
      "loss": 1.8825,
      "step": 392
    },
    {
      "epoch": 1.0796703296703296,
      "grad_norm": 1.4068641662597656,
      "learning_rate": 5e-05,
      "loss": 1.8341,
      "step": 393
    },
    {
      "epoch": 1.0824175824175823,
      "grad_norm": 1.3905977010726929,
      "learning_rate": 5e-05,
      "loss": 1.7733,
      "step": 394
    },
    {
      "epoch": 1.085164835164835,
      "grad_norm": 1.546363353729248,
      "learning_rate": 5e-05,
      "loss": 1.8426,
      "step": 395
    },
    {
      "epoch": 1.0879120879120878,
      "grad_norm": 1.4545570611953735,
      "learning_rate": 5e-05,
      "loss": 1.9017,
      "step": 396
    },
    {
      "epoch": 1.0906593406593406,
      "grad_norm": 1.411657691001892,
      "learning_rate": 5e-05,
      "loss": 1.8467,
      "step": 397
    },
    {
      "epoch": 1.0934065934065935,
      "grad_norm": 1.3879867792129517,
      "learning_rate": 5e-05,
      "loss": 1.8601,
      "step": 398
    },
    {
      "epoch": 1.0961538461538463,
      "grad_norm": 1.3900244235992432,
      "learning_rate": 5e-05,
      "loss": 1.849,
      "step": 399
    },
    {
      "epoch": 1.098901098901099,
      "grad_norm": 1.4042563438415527,
      "learning_rate": 5e-05,
      "loss": 1.8679,
      "step": 400
    },
    {
      "epoch": 1.1016483516483517,
      "grad_norm": 1.4235203266143799,
      "learning_rate": 5e-05,
      "loss": 1.8423,
      "step": 401
    },
    {
      "epoch": 1.1043956043956045,
      "grad_norm": 1.4006916284561157,
      "learning_rate": 5e-05,
      "loss": 1.811,
      "step": 402
    },
    {
      "epoch": 1.1071428571428572,
      "grad_norm": 1.379900574684143,
      "learning_rate": 5e-05,
      "loss": 1.8228,
      "step": 403
    },
    {
      "epoch": 1.10989010989011,
      "grad_norm": 1.3946508169174194,
      "learning_rate": 5e-05,
      "loss": 1.8296,
      "step": 404
    },
    {
      "epoch": 1.1126373626373627,
      "grad_norm": 1.4744532108306885,
      "learning_rate": 5e-05,
      "loss": 1.8453,
      "step": 405
    },
    {
      "epoch": 1.1153846153846154,
      "grad_norm": 1.4611108303070068,
      "learning_rate": 5e-05,
      "loss": 1.8888,
      "step": 406
    },
    {
      "epoch": 1.1181318681318682,
      "grad_norm": 1.449432134628296,
      "learning_rate": 5e-05,
      "loss": 1.7566,
      "step": 407
    },
    {
      "epoch": 1.120879120879121,
      "grad_norm": 1.4265761375427246,
      "learning_rate": 5e-05,
      "loss": 1.8555,
      "step": 408
    },
    {
      "epoch": 1.1236263736263736,
      "grad_norm": 1.407605767250061,
      "learning_rate": 5e-05,
      "loss": 1.8632,
      "step": 409
    },
    {
      "epoch": 1.1263736263736264,
      "grad_norm": 1.456616997718811,
      "learning_rate": 5e-05,
      "loss": 1.8489,
      "step": 410
    },
    {
      "epoch": 1.129120879120879,
      "grad_norm": 1.4825993776321411,
      "learning_rate": 5e-05,
      "loss": 1.9523,
      "step": 411
    },
    {
      "epoch": 1.1318681318681318,
      "grad_norm": 1.431409239768982,
      "learning_rate": 5e-05,
      "loss": 1.8791,
      "step": 412
    },
    {
      "epoch": 1.1346153846153846,
      "grad_norm": 1.408527135848999,
      "learning_rate": 5e-05,
      "loss": 1.7879,
      "step": 413
    },
    {
      "epoch": 1.1373626373626373,
      "grad_norm": 1.4621326923370361,
      "learning_rate": 5e-05,
      "loss": 1.8128,
      "step": 414
    },
    {
      "epoch": 1.14010989010989,
      "grad_norm": 1.3539048433303833,
      "learning_rate": 5e-05,
      "loss": 1.8094,
      "step": 415
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 1.4438364505767822,
      "learning_rate": 5e-05,
      "loss": 1.869,
      "step": 416
    },
    {
      "epoch": 1.1456043956043955,
      "grad_norm": 1.4298127889633179,
      "learning_rate": 5e-05,
      "loss": 1.8596,
      "step": 417
    },
    {
      "epoch": 1.1483516483516483,
      "grad_norm": 1.4717894792556763,
      "learning_rate": 5e-05,
      "loss": 1.9501,
      "step": 418
    },
    {
      "epoch": 1.151098901098901,
      "grad_norm": 1.4145112037658691,
      "learning_rate": 5e-05,
      "loss": 1.8088,
      "step": 419
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 1.4836915731430054,
      "learning_rate": 5e-05,
      "loss": 1.8033,
      "step": 420
    },
    {
      "epoch": 1.1565934065934065,
      "grad_norm": 1.4290279150009155,
      "learning_rate": 5e-05,
      "loss": 1.7983,
      "step": 421
    },
    {
      "epoch": 1.1593406593406592,
      "grad_norm": 1.4414855241775513,
      "learning_rate": 5e-05,
      "loss": 1.8545,
      "step": 422
    },
    {
      "epoch": 1.1620879120879122,
      "grad_norm": 1.4346988201141357,
      "learning_rate": 5e-05,
      "loss": 1.8396,
      "step": 423
    },
    {
      "epoch": 1.164835164835165,
      "grad_norm": 1.4018402099609375,
      "learning_rate": 5e-05,
      "loss": 1.7887,
      "step": 424
    },
    {
      "epoch": 1.1675824175824177,
      "grad_norm": 1.4072084426879883,
      "learning_rate": 5e-05,
      "loss": 1.7813,
      "step": 425
    },
    {
      "epoch": 1.1703296703296704,
      "grad_norm": 1.4251518249511719,
      "learning_rate": 5e-05,
      "loss": 1.7971,
      "step": 426
    },
    {
      "epoch": 1.1730769230769231,
      "grad_norm": 1.421057939529419,
      "learning_rate": 5e-05,
      "loss": 1.7757,
      "step": 427
    },
    {
      "epoch": 1.1758241758241759,
      "grad_norm": 1.4168102741241455,
      "learning_rate": 5e-05,
      "loss": 1.7964,
      "step": 428
    },
    {
      "epoch": 1.1785714285714286,
      "grad_norm": 1.3674674034118652,
      "learning_rate": 5e-05,
      "loss": 1.8486,
      "step": 429
    },
    {
      "epoch": 1.1813186813186813,
      "grad_norm": 1.4686448574066162,
      "learning_rate": 5e-05,
      "loss": 1.8098,
      "step": 430
    },
    {
      "epoch": 1.184065934065934,
      "grad_norm": 1.3861656188964844,
      "learning_rate": 5e-05,
      "loss": 1.757,
      "step": 431
    },
    {
      "epoch": 1.1868131868131868,
      "grad_norm": 1.4282257556915283,
      "learning_rate": 5e-05,
      "loss": 1.8162,
      "step": 432
    },
    {
      "epoch": 1.1895604395604396,
      "grad_norm": 1.4050997495651245,
      "learning_rate": 5e-05,
      "loss": 1.8192,
      "step": 433
    },
    {
      "epoch": 1.1923076923076923,
      "grad_norm": 1.4029245376586914,
      "learning_rate": 5e-05,
      "loss": 1.8552,
      "step": 434
    },
    {
      "epoch": 1.195054945054945,
      "grad_norm": 1.4355541467666626,
      "learning_rate": 5e-05,
      "loss": 1.8365,
      "step": 435
    },
    {
      "epoch": 1.1978021978021978,
      "grad_norm": 1.4086416959762573,
      "learning_rate": 5e-05,
      "loss": 1.8518,
      "step": 436
    },
    {
      "epoch": 1.2005494505494505,
      "grad_norm": 1.4056622982025146,
      "learning_rate": 5e-05,
      "loss": 1.8412,
      "step": 437
    },
    {
      "epoch": 1.2032967032967032,
      "grad_norm": 1.419624924659729,
      "learning_rate": 5e-05,
      "loss": 1.8241,
      "step": 438
    },
    {
      "epoch": 1.206043956043956,
      "grad_norm": 1.4076471328735352,
      "learning_rate": 5e-05,
      "loss": 1.8076,
      "step": 439
    },
    {
      "epoch": 1.2087912087912087,
      "grad_norm": 1.4441996812820435,
      "learning_rate": 5e-05,
      "loss": 1.7755,
      "step": 440
    },
    {
      "epoch": 1.2115384615384615,
      "grad_norm": 1.4014694690704346,
      "learning_rate": 5e-05,
      "loss": 1.7999,
      "step": 441
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 1.4283058643341064,
      "learning_rate": 5e-05,
      "loss": 1.8024,
      "step": 442
    },
    {
      "epoch": 1.2170329670329672,
      "grad_norm": 1.3808201551437378,
      "learning_rate": 5e-05,
      "loss": 1.7983,
      "step": 443
    },
    {
      "epoch": 1.2197802197802199,
      "grad_norm": 1.4183015823364258,
      "learning_rate": 5e-05,
      "loss": 1.7675,
      "step": 444
    },
    {
      "epoch": 1.2225274725274726,
      "grad_norm": 1.391979455947876,
      "learning_rate": 5e-05,
      "loss": 1.8068,
      "step": 445
    },
    {
      "epoch": 1.2252747252747254,
      "grad_norm": 1.4189105033874512,
      "learning_rate": 5e-05,
      "loss": 1.7586,
      "step": 446
    },
    {
      "epoch": 1.228021978021978,
      "grad_norm": 1.4108524322509766,
      "learning_rate": 5e-05,
      "loss": 1.8118,
      "step": 447
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 1.420425295829773,
      "learning_rate": 5e-05,
      "loss": 1.8293,
      "step": 448
    },
    {
      "epoch": 1.2335164835164836,
      "grad_norm": 1.4567275047302246,
      "learning_rate": 5e-05,
      "loss": 1.8272,
      "step": 449
    },
    {
      "epoch": 1.2362637362637363,
      "grad_norm": 1.4234107732772827,
      "learning_rate": 5e-05,
      "loss": 1.7849,
      "step": 450
    },
    {
      "epoch": 1.239010989010989,
      "grad_norm": 1.4825859069824219,
      "learning_rate": 5e-05,
      "loss": 1.8874,
      "step": 451
    },
    {
      "epoch": 1.2417582417582418,
      "grad_norm": 1.382912039756775,
      "learning_rate": 5e-05,
      "loss": 1.7541,
      "step": 452
    },
    {
      "epoch": 1.2445054945054945,
      "grad_norm": 1.3736838102340698,
      "learning_rate": 5e-05,
      "loss": 1.7487,
      "step": 453
    },
    {
      "epoch": 1.2472527472527473,
      "grad_norm": 1.3533505201339722,
      "learning_rate": 5e-05,
      "loss": 1.7762,
      "step": 454
    },
    {
      "epoch": 1.25,
      "grad_norm": 1.4056882858276367,
      "learning_rate": 5e-05,
      "loss": 1.8315,
      "step": 455
    },
    {
      "epoch": 1.2527472527472527,
      "grad_norm": 1.352007269859314,
      "learning_rate": 5e-05,
      "loss": 1.6897,
      "step": 456
    },
    {
      "epoch": 1.2554945054945055,
      "grad_norm": 1.4342315196990967,
      "learning_rate": 5e-05,
      "loss": 1.741,
      "step": 457
    },
    {
      "epoch": 1.2582417582417582,
      "grad_norm": 1.4259731769561768,
      "learning_rate": 5e-05,
      "loss": 1.8101,
      "step": 458
    },
    {
      "epoch": 1.260989010989011,
      "grad_norm": 1.392438530921936,
      "learning_rate": 5e-05,
      "loss": 1.7374,
      "step": 459
    },
    {
      "epoch": 1.2637362637362637,
      "grad_norm": 1.3751567602157593,
      "learning_rate": 5e-05,
      "loss": 1.7236,
      "step": 460
    },
    {
      "epoch": 1.2664835164835164,
      "grad_norm": 1.3968183994293213,
      "learning_rate": 5e-05,
      "loss": 1.7126,
      "step": 461
    },
    {
      "epoch": 1.2692307692307692,
      "grad_norm": 1.3863105773925781,
      "learning_rate": 5e-05,
      "loss": 1.8471,
      "step": 462
    },
    {
      "epoch": 1.271978021978022,
      "grad_norm": 1.4322186708450317,
      "learning_rate": 5e-05,
      "loss": 1.781,
      "step": 463
    },
    {
      "epoch": 1.2747252747252746,
      "grad_norm": 1.404012680053711,
      "learning_rate": 5e-05,
      "loss": 1.7567,
      "step": 464
    },
    {
      "epoch": 1.2774725274725274,
      "grad_norm": 1.4101475477218628,
      "learning_rate": 5e-05,
      "loss": 1.8098,
      "step": 465
    },
    {
      "epoch": 1.2802197802197801,
      "grad_norm": 1.366278052330017,
      "learning_rate": 5e-05,
      "loss": 1.8237,
      "step": 466
    },
    {
      "epoch": 1.2829670329670328,
      "grad_norm": 1.3855562210083008,
      "learning_rate": 5e-05,
      "loss": 1.7371,
      "step": 467
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 1.389593482017517,
      "learning_rate": 5e-05,
      "loss": 1.7599,
      "step": 468
    },
    {
      "epoch": 1.2884615384615383,
      "grad_norm": 1.4495130777359009,
      "learning_rate": 5e-05,
      "loss": 1.834,
      "step": 469
    },
    {
      "epoch": 1.2912087912087913,
      "grad_norm": 1.3942437171936035,
      "learning_rate": 5e-05,
      "loss": 1.7146,
      "step": 470
    },
    {
      "epoch": 1.293956043956044,
      "grad_norm": 1.3698192834854126,
      "learning_rate": 5e-05,
      "loss": 1.7786,
      "step": 471
    },
    {
      "epoch": 1.2967032967032968,
      "grad_norm": 1.4101547002792358,
      "learning_rate": 5e-05,
      "loss": 1.7365,
      "step": 472
    },
    {
      "epoch": 1.2994505494505495,
      "grad_norm": 1.523290991783142,
      "learning_rate": 5e-05,
      "loss": 1.7858,
      "step": 473
    },
    {
      "epoch": 1.3021978021978022,
      "grad_norm": 1.3541239500045776,
      "learning_rate": 5e-05,
      "loss": 1.8319,
      "step": 474
    },
    {
      "epoch": 1.304945054945055,
      "grad_norm": 1.4067178964614868,
      "learning_rate": 5e-05,
      "loss": 1.7804,
      "step": 475
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 1.3811776638031006,
      "learning_rate": 5e-05,
      "loss": 1.7758,
      "step": 476
    },
    {
      "epoch": 1.3104395604395604,
      "grad_norm": 1.4058114290237427,
      "learning_rate": 5e-05,
      "loss": 1.7953,
      "step": 477
    },
    {
      "epoch": 1.3131868131868132,
      "grad_norm": 1.4044265747070312,
      "learning_rate": 5e-05,
      "loss": 1.7527,
      "step": 478
    },
    {
      "epoch": 1.315934065934066,
      "grad_norm": 1.3844845294952393,
      "learning_rate": 5e-05,
      "loss": 1.7847,
      "step": 479
    },
    {
      "epoch": 1.3186813186813187,
      "grad_norm": 1.4332555532455444,
      "learning_rate": 5e-05,
      "loss": 1.7038,
      "step": 480
    },
    {
      "epoch": 1.3214285714285714,
      "grad_norm": 1.4123413562774658,
      "learning_rate": 5e-05,
      "loss": 1.7372,
      "step": 481
    },
    {
      "epoch": 1.3241758241758241,
      "grad_norm": 1.3630703687667847,
      "learning_rate": 5e-05,
      "loss": 1.7176,
      "step": 482
    },
    {
      "epoch": 1.3269230769230769,
      "grad_norm": 1.373326063156128,
      "learning_rate": 5e-05,
      "loss": 1.7243,
      "step": 483
    },
    {
      "epoch": 1.3296703296703296,
      "grad_norm": 1.4084221124649048,
      "learning_rate": 5e-05,
      "loss": 1.7426,
      "step": 484
    },
    {
      "epoch": 1.3324175824175823,
      "grad_norm": 1.4029020071029663,
      "learning_rate": 5e-05,
      "loss": 1.7243,
      "step": 485
    },
    {
      "epoch": 1.335164835164835,
      "grad_norm": 1.4089550971984863,
      "learning_rate": 5e-05,
      "loss": 1.7141,
      "step": 486
    },
    {
      "epoch": 1.337912087912088,
      "grad_norm": 1.345319151878357,
      "learning_rate": 5e-05,
      "loss": 1.6796,
      "step": 487
    },
    {
      "epoch": 1.3406593406593408,
      "grad_norm": 1.3711649179458618,
      "learning_rate": 5e-05,
      "loss": 1.7393,
      "step": 488
    },
    {
      "epoch": 1.3434065934065935,
      "grad_norm": 1.4407110214233398,
      "learning_rate": 5e-05,
      "loss": 1.7175,
      "step": 489
    },
    {
      "epoch": 1.3461538461538463,
      "grad_norm": 1.3757565021514893,
      "learning_rate": 5e-05,
      "loss": 1.7673,
      "step": 490
    },
    {
      "epoch": 1.348901098901099,
      "grad_norm": 1.450755000114441,
      "learning_rate": 5e-05,
      "loss": 1.7661,
      "step": 491
    },
    {
      "epoch": 1.3516483516483517,
      "grad_norm": 1.4235881567001343,
      "learning_rate": 5e-05,
      "loss": 1.8486,
      "step": 492
    },
    {
      "epoch": 1.3543956043956045,
      "grad_norm": 1.4050076007843018,
      "learning_rate": 5e-05,
      "loss": 1.7037,
      "step": 493
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 1.3478296995162964,
      "learning_rate": 5e-05,
      "loss": 1.6814,
      "step": 494
    },
    {
      "epoch": 1.35989010989011,
      "grad_norm": 1.3697071075439453,
      "learning_rate": 5e-05,
      "loss": 1.7278,
      "step": 495
    },
    {
      "epoch": 1.3626373626373627,
      "grad_norm": 1.3678171634674072,
      "learning_rate": 5e-05,
      "loss": 1.7286,
      "step": 496
    },
    {
      "epoch": 1.3653846153846154,
      "grad_norm": 1.3823747634887695,
      "learning_rate": 5e-05,
      "loss": 1.7542,
      "step": 497
    },
    {
      "epoch": 1.3681318681318682,
      "grad_norm": 1.3844518661499023,
      "learning_rate": 5e-05,
      "loss": 1.7645,
      "step": 498
    },
    {
      "epoch": 1.370879120879121,
      "grad_norm": 1.357337236404419,
      "learning_rate": 5e-05,
      "loss": 1.7634,
      "step": 499
    },
    {
      "epoch": 1.3736263736263736,
      "grad_norm": 1.3758798837661743,
      "learning_rate": 5e-05,
      "loss": 1.694,
      "step": 500
    },
    {
      "epoch": 1.3763736263736264,
      "grad_norm": 1.417529821395874,
      "learning_rate": 5e-05,
      "loss": 1.7618,
      "step": 501
    },
    {
      "epoch": 1.379120879120879,
      "grad_norm": 1.4276143312454224,
      "learning_rate": 5e-05,
      "loss": 1.7813,
      "step": 502
    },
    {
      "epoch": 1.3818681318681318,
      "grad_norm": 1.4048218727111816,
      "learning_rate": 5e-05,
      "loss": 1.7588,
      "step": 503
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 1.3315848112106323,
      "learning_rate": 5e-05,
      "loss": 1.7437,
      "step": 504
    },
    {
      "epoch": 1.3873626373626373,
      "grad_norm": 1.387294054031372,
      "learning_rate": 5e-05,
      "loss": 1.7903,
      "step": 505
    },
    {
      "epoch": 1.39010989010989,
      "grad_norm": 1.4195352792739868,
      "learning_rate": 5e-05,
      "loss": 1.7418,
      "step": 506
    },
    {
      "epoch": 1.3928571428571428,
      "grad_norm": 1.3375133275985718,
      "learning_rate": 5e-05,
      "loss": 1.7396,
      "step": 507
    },
    {
      "epoch": 1.3956043956043955,
      "grad_norm": 1.405215859413147,
      "learning_rate": 5e-05,
      "loss": 1.7019,
      "step": 508
    },
    {
      "epoch": 1.3983516483516483,
      "grad_norm": 1.3681893348693848,
      "learning_rate": 5e-05,
      "loss": 1.6812,
      "step": 509
    },
    {
      "epoch": 1.401098901098901,
      "grad_norm": 1.405635952949524,
      "learning_rate": 5e-05,
      "loss": 1.7333,
      "step": 510
    },
    {
      "epoch": 1.4038461538461537,
      "grad_norm": 1.3803540468215942,
      "learning_rate": 5e-05,
      "loss": 1.7066,
      "step": 511
    },
    {
      "epoch": 1.4065934065934065,
      "grad_norm": 1.337558627128601,
      "learning_rate": 5e-05,
      "loss": 1.6957,
      "step": 512
    },
    {
      "epoch": 1.4093406593406592,
      "grad_norm": 1.3768702745437622,
      "learning_rate": 5e-05,
      "loss": 1.7985,
      "step": 513
    },
    {
      "epoch": 1.412087912087912,
      "grad_norm": 1.396325945854187,
      "learning_rate": 5e-05,
      "loss": 1.7315,
      "step": 514
    },
    {
      "epoch": 1.414835164835165,
      "grad_norm": 1.3513213396072388,
      "learning_rate": 5e-05,
      "loss": 1.7086,
      "step": 515
    },
    {
      "epoch": 1.4175824175824177,
      "grad_norm": 1.3809621334075928,
      "learning_rate": 5e-05,
      "loss": 1.7604,
      "step": 516
    },
    {
      "epoch": 1.4203296703296704,
      "grad_norm": 1.379443645477295,
      "learning_rate": 5e-05,
      "loss": 1.742,
      "step": 517
    },
    {
      "epoch": 1.4230769230769231,
      "grad_norm": 1.4533429145812988,
      "learning_rate": 5e-05,
      "loss": 1.779,
      "step": 518
    },
    {
      "epoch": 1.4258241758241759,
      "grad_norm": 1.3627218008041382,
      "learning_rate": 5e-05,
      "loss": 1.7542,
      "step": 519
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 1.3945807218551636,
      "learning_rate": 5e-05,
      "loss": 1.7861,
      "step": 520
    },
    {
      "epoch": 1.4313186813186813,
      "grad_norm": 1.367820382118225,
      "learning_rate": 5e-05,
      "loss": 1.7079,
      "step": 521
    },
    {
      "epoch": 1.434065934065934,
      "grad_norm": 1.3690483570098877,
      "learning_rate": 5e-05,
      "loss": 1.6666,
      "step": 522
    },
    {
      "epoch": 1.4368131868131868,
      "grad_norm": 1.3790886402130127,
      "learning_rate": 5e-05,
      "loss": 1.7384,
      "step": 523
    },
    {
      "epoch": 1.4395604395604396,
      "grad_norm": 1.3619673252105713,
      "learning_rate": 5e-05,
      "loss": 1.6531,
      "step": 524
    },
    {
      "epoch": 1.4423076923076923,
      "grad_norm": 1.4115405082702637,
      "learning_rate": 5e-05,
      "loss": 1.7153,
      "step": 525
    },
    {
      "epoch": 1.445054945054945,
      "grad_norm": 1.3432621955871582,
      "learning_rate": 5e-05,
      "loss": 1.6794,
      "step": 526
    },
    {
      "epoch": 1.4478021978021978,
      "grad_norm": 1.4169189929962158,
      "learning_rate": 5e-05,
      "loss": 1.6853,
      "step": 527
    },
    {
      "epoch": 1.4505494505494505,
      "grad_norm": 1.3355238437652588,
      "learning_rate": 5e-05,
      "loss": 1.6487,
      "step": 528
    },
    {
      "epoch": 1.4532967032967032,
      "grad_norm": 1.3981549739837646,
      "learning_rate": 5e-05,
      "loss": 1.7351,
      "step": 529
    },
    {
      "epoch": 1.456043956043956,
      "grad_norm": 1.492950677871704,
      "learning_rate": 5e-05,
      "loss": 1.7105,
      "step": 530
    },
    {
      "epoch": 1.4587912087912087,
      "grad_norm": 1.3935256004333496,
      "learning_rate": 5e-05,
      "loss": 1.6572,
      "step": 531
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 1.3927395343780518,
      "learning_rate": 5e-05,
      "loss": 1.6751,
      "step": 532
    },
    {
      "epoch": 1.4642857142857144,
      "grad_norm": 1.3900090456008911,
      "learning_rate": 5e-05,
      "loss": 1.8285,
      "step": 533
    },
    {
      "epoch": 1.4670329670329672,
      "grad_norm": 1.3810862302780151,
      "learning_rate": 5e-05,
      "loss": 1.7522,
      "step": 534
    },
    {
      "epoch": 1.4697802197802199,
      "grad_norm": 1.403772234916687,
      "learning_rate": 5e-05,
      "loss": 1.7653,
      "step": 535
    },
    {
      "epoch": 1.4725274725274726,
      "grad_norm": 1.3676576614379883,
      "learning_rate": 5e-05,
      "loss": 1.7662,
      "step": 536
    },
    {
      "epoch": 1.4752747252747254,
      "grad_norm": 1.403173565864563,
      "learning_rate": 5e-05,
      "loss": 1.7219,
      "step": 537
    },
    {
      "epoch": 1.478021978021978,
      "grad_norm": 1.363599419593811,
      "learning_rate": 5e-05,
      "loss": 1.6682,
      "step": 538
    },
    {
      "epoch": 1.4807692307692308,
      "grad_norm": 1.3695369958877563,
      "learning_rate": 5e-05,
      "loss": 1.7623,
      "step": 539
    },
    {
      "epoch": 1.4835164835164836,
      "grad_norm": 1.3677964210510254,
      "learning_rate": 5e-05,
      "loss": 1.6861,
      "step": 540
    },
    {
      "epoch": 1.4862637362637363,
      "grad_norm": 1.3642503023147583,
      "learning_rate": 5e-05,
      "loss": 1.6356,
      "step": 541
    },
    {
      "epoch": 1.489010989010989,
      "grad_norm": 1.4312710762023926,
      "learning_rate": 5e-05,
      "loss": 1.7138,
      "step": 542
    },
    {
      "epoch": 1.4917582417582418,
      "grad_norm": 1.3985414505004883,
      "learning_rate": 5e-05,
      "loss": 1.8029,
      "step": 543
    },
    {
      "epoch": 1.4945054945054945,
      "grad_norm": 1.3809481859207153,
      "learning_rate": 5e-05,
      "loss": 1.7667,
      "step": 544
    },
    {
      "epoch": 1.4972527472527473,
      "grad_norm": 1.30148446559906,
      "learning_rate": 5e-05,
      "loss": 1.6243,
      "step": 545
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.378100037574768,
      "learning_rate": 5e-05,
      "loss": 1.654,
      "step": 546
    },
    {
      "epoch": 1.5,
      "eval_loss": 2.115718126296997,
      "eval_runtime": 98.4984,
      "eval_samples_per_second": 420.362,
      "eval_steps_per_second": 3.289,
      "step": 546
    },
    {
      "epoch": 1.5027472527472527,
      "grad_norm": 1.4027456045150757,
      "learning_rate": 5e-05,
      "loss": 1.6477,
      "step": 547
    },
    {
      "epoch": 1.5054945054945055,
      "grad_norm": 1.3793079853057861,
      "learning_rate": 5e-05,
      "loss": 1.7242,
      "step": 548
    },
    {
      "epoch": 1.5082417582417582,
      "grad_norm": 1.341597080230713,
      "learning_rate": 5e-05,
      "loss": 1.5888,
      "step": 549
    },
    {
      "epoch": 1.510989010989011,
      "grad_norm": 1.6155542135238647,
      "learning_rate": 5e-05,
      "loss": 1.733,
      "step": 550
    },
    {
      "epoch": 1.5137362637362637,
      "grad_norm": 1.3378050327301025,
      "learning_rate": 5e-05,
      "loss": 1.6567,
      "step": 551
    },
    {
      "epoch": 1.5164835164835164,
      "grad_norm": 1.36567223072052,
      "learning_rate": 5e-05,
      "loss": 1.6513,
      "step": 552
    },
    {
      "epoch": 1.5192307692307692,
      "grad_norm": 1.369423270225525,
      "learning_rate": 5e-05,
      "loss": 1.7118,
      "step": 553
    },
    {
      "epoch": 1.521978021978022,
      "grad_norm": 1.3844424486160278,
      "learning_rate": 5e-05,
      "loss": 1.6355,
      "step": 554
    },
    {
      "epoch": 1.5247252747252746,
      "grad_norm": 1.3957059383392334,
      "learning_rate": 5e-05,
      "loss": 1.629,
      "step": 555
    },
    {
      "epoch": 1.5274725274725274,
      "grad_norm": 1.409712314605713,
      "learning_rate": 5e-05,
      "loss": 1.7021,
      "step": 556
    },
    {
      "epoch": 1.5302197802197801,
      "grad_norm": 1.3630772829055786,
      "learning_rate": 5e-05,
      "loss": 1.6415,
      "step": 557
    },
    {
      "epoch": 1.5329670329670328,
      "grad_norm": 1.4567890167236328,
      "learning_rate": 5e-05,
      "loss": 1.6693,
      "step": 558
    },
    {
      "epoch": 1.5357142857142856,
      "grad_norm": 1.3791460990905762,
      "learning_rate": 5e-05,
      "loss": 1.6851,
      "step": 559
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 1.3627620935440063,
      "learning_rate": 5e-05,
      "loss": 1.6906,
      "step": 560
    },
    {
      "epoch": 1.541208791208791,
      "grad_norm": 1.3427547216415405,
      "learning_rate": 5e-05,
      "loss": 1.6521,
      "step": 561
    },
    {
      "epoch": 1.5439560439560438,
      "grad_norm": 1.375153660774231,
      "learning_rate": 5e-05,
      "loss": 1.7394,
      "step": 562
    },
    {
      "epoch": 1.5467032967032965,
      "grad_norm": 1.4206980466842651,
      "learning_rate": 5e-05,
      "loss": 1.6792,
      "step": 563
    },
    {
      "epoch": 1.5494505494505495,
      "grad_norm": 1.3719024658203125,
      "learning_rate": 5e-05,
      "loss": 1.6983,
      "step": 564
    },
    {
      "epoch": 1.5521978021978022,
      "grad_norm": 1.353804588317871,
      "learning_rate": 5e-05,
      "loss": 1.6204,
      "step": 565
    },
    {
      "epoch": 1.554945054945055,
      "grad_norm": 1.3559083938598633,
      "learning_rate": 5e-05,
      "loss": 1.6696,
      "step": 566
    },
    {
      "epoch": 1.5576923076923077,
      "grad_norm": 1.3342841863632202,
      "learning_rate": 5e-05,
      "loss": 1.6268,
      "step": 567
    },
    {
      "epoch": 1.5604395604395604,
      "grad_norm": 1.3843108415603638,
      "learning_rate": 5e-05,
      "loss": 1.6938,
      "step": 568
    },
    {
      "epoch": 1.5631868131868132,
      "grad_norm": 1.3609648942947388,
      "learning_rate": 5e-05,
      "loss": 1.6763,
      "step": 569
    },
    {
      "epoch": 1.565934065934066,
      "grad_norm": 1.3292051553726196,
      "learning_rate": 5e-05,
      "loss": 1.6835,
      "step": 570
    },
    {
      "epoch": 1.5686813186813187,
      "grad_norm": 1.3395793437957764,
      "learning_rate": 5e-05,
      "loss": 1.6556,
      "step": 571
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.3651260137557983,
      "learning_rate": 5e-05,
      "loss": 1.6116,
      "step": 572
    },
    {
      "epoch": 1.5741758241758241,
      "grad_norm": 1.3252488374710083,
      "learning_rate": 5e-05,
      "loss": 1.5902,
      "step": 573
    },
    {
      "epoch": 1.5769230769230769,
      "grad_norm": 1.3502089977264404,
      "learning_rate": 5e-05,
      "loss": 1.6008,
      "step": 574
    },
    {
      "epoch": 1.5796703296703298,
      "grad_norm": 1.3531606197357178,
      "learning_rate": 5e-05,
      "loss": 1.6333,
      "step": 575
    },
    {
      "epoch": 1.5824175824175826,
      "grad_norm": 1.3231101036071777,
      "learning_rate": 5e-05,
      "loss": 1.6685,
      "step": 576
    },
    {
      "epoch": 1.5851648351648353,
      "grad_norm": 1.3546409606933594,
      "learning_rate": 5e-05,
      "loss": 1.6194,
      "step": 577
    },
    {
      "epoch": 1.587912087912088,
      "grad_norm": 1.3522393703460693,
      "learning_rate": 5e-05,
      "loss": 1.6863,
      "step": 578
    },
    {
      "epoch": 1.5906593406593408,
      "grad_norm": 1.4723747968673706,
      "learning_rate": 5e-05,
      "loss": 1.6439,
      "step": 579
    },
    {
      "epoch": 1.5934065934065935,
      "grad_norm": 1.332892656326294,
      "learning_rate": 5e-05,
      "loss": 1.6214,
      "step": 580
    },
    {
      "epoch": 1.5961538461538463,
      "grad_norm": 1.412571907043457,
      "learning_rate": 5e-05,
      "loss": 1.6359,
      "step": 581
    },
    {
      "epoch": 1.598901098901099,
      "grad_norm": 1.3985692262649536,
      "learning_rate": 5e-05,
      "loss": 1.7565,
      "step": 582
    },
    {
      "epoch": 1.6016483516483517,
      "grad_norm": 1.3606042861938477,
      "learning_rate": 5e-05,
      "loss": 1.6137,
      "step": 583
    },
    {
      "epoch": 1.6043956043956045,
      "grad_norm": 1.3627523183822632,
      "learning_rate": 5e-05,
      "loss": 1.6242,
      "step": 584
    },
    {
      "epoch": 1.6071428571428572,
      "grad_norm": 1.317152738571167,
      "learning_rate": 5e-05,
      "loss": 1.6008,
      "step": 585
    },
    {
      "epoch": 1.60989010989011,
      "grad_norm": 1.3443416357040405,
      "learning_rate": 5e-05,
      "loss": 1.6918,
      "step": 586
    },
    {
      "epoch": 1.6126373626373627,
      "grad_norm": 1.3652070760726929,
      "learning_rate": 5e-05,
      "loss": 1.5474,
      "step": 587
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 1.3657443523406982,
      "learning_rate": 5e-05,
      "loss": 1.6375,
      "step": 588
    },
    {
      "epoch": 1.6181318681318682,
      "grad_norm": 1.400424599647522,
      "learning_rate": 5e-05,
      "loss": 1.6516,
      "step": 589
    },
    {
      "epoch": 1.620879120879121,
      "grad_norm": 1.3430229425430298,
      "learning_rate": 5e-05,
      "loss": 1.6121,
      "step": 590
    },
    {
      "epoch": 1.6236263736263736,
      "grad_norm": 1.361994743347168,
      "learning_rate": 5e-05,
      "loss": 1.7181,
      "step": 591
    },
    {
      "epoch": 1.6263736263736264,
      "grad_norm": 1.3427131175994873,
      "learning_rate": 5e-05,
      "loss": 1.5806,
      "step": 592
    },
    {
      "epoch": 1.629120879120879,
      "grad_norm": 1.3000552654266357,
      "learning_rate": 5e-05,
      "loss": 1.5908,
      "step": 593
    },
    {
      "epoch": 1.6318681318681318,
      "grad_norm": 1.3763680458068848,
      "learning_rate": 5e-05,
      "loss": 1.6897,
      "step": 594
    },
    {
      "epoch": 1.6346153846153846,
      "grad_norm": 1.337466835975647,
      "learning_rate": 5e-05,
      "loss": 1.6813,
      "step": 595
    },
    {
      "epoch": 1.6373626373626373,
      "grad_norm": 1.3593329191207886,
      "learning_rate": 5e-05,
      "loss": 1.6117,
      "step": 596
    },
    {
      "epoch": 1.64010989010989,
      "grad_norm": 1.3463891744613647,
      "learning_rate": 5e-05,
      "loss": 1.5866,
      "step": 597
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 1.3430982828140259,
      "learning_rate": 5e-05,
      "loss": 1.6564,
      "step": 598
    },
    {
      "epoch": 1.6456043956043955,
      "grad_norm": 1.3639575242996216,
      "learning_rate": 5e-05,
      "loss": 1.6506,
      "step": 599
    },
    {
      "epoch": 1.6483516483516483,
      "grad_norm": 1.3655098676681519,
      "learning_rate": 5e-05,
      "loss": 1.6197,
      "step": 600
    },
    {
      "epoch": 1.651098901098901,
      "grad_norm": 1.3629127740859985,
      "learning_rate": 5e-05,
      "loss": 1.5753,
      "step": 601
    },
    {
      "epoch": 1.6538461538461537,
      "grad_norm": 1.353515625,
      "learning_rate": 5e-05,
      "loss": 1.599,
      "step": 602
    },
    {
      "epoch": 1.6565934065934065,
      "grad_norm": 1.4255399703979492,
      "learning_rate": 5e-05,
      "loss": 1.632,
      "step": 603
    },
    {
      "epoch": 1.6593406593406592,
      "grad_norm": 1.4001637697219849,
      "learning_rate": 5e-05,
      "loss": 1.5393,
      "step": 604
    },
    {
      "epoch": 1.662087912087912,
      "grad_norm": 1.3118921518325806,
      "learning_rate": 5e-05,
      "loss": 1.5471,
      "step": 605
    },
    {
      "epoch": 1.6648351648351647,
      "grad_norm": 1.3548777103424072,
      "learning_rate": 5e-05,
      "loss": 1.7164,
      "step": 606
    },
    {
      "epoch": 1.6675824175824174,
      "grad_norm": 1.3597056865692139,
      "learning_rate": 5e-05,
      "loss": 1.6276,
      "step": 607
    },
    {
      "epoch": 1.6703296703296702,
      "grad_norm": 1.384057641029358,
      "learning_rate": 5e-05,
      "loss": 1.6757,
      "step": 608
    },
    {
      "epoch": 1.6730769230769231,
      "grad_norm": 1.3851240873336792,
      "learning_rate": 5e-05,
      "loss": 1.5424,
      "step": 609
    },
    {
      "epoch": 1.6758241758241759,
      "grad_norm": 1.4054464101791382,
      "learning_rate": 5e-05,
      "loss": 1.6879,
      "step": 610
    },
    {
      "epoch": 1.6785714285714286,
      "grad_norm": 1.3685153722763062,
      "learning_rate": 5e-05,
      "loss": 1.6652,
      "step": 611
    },
    {
      "epoch": 1.6813186813186813,
      "grad_norm": 1.35781729221344,
      "learning_rate": 5e-05,
      "loss": 1.6457,
      "step": 612
    },
    {
      "epoch": 1.684065934065934,
      "grad_norm": 1.352078914642334,
      "learning_rate": 5e-05,
      "loss": 1.6662,
      "step": 613
    },
    {
      "epoch": 1.6868131868131868,
      "grad_norm": 1.3292311429977417,
      "learning_rate": 5e-05,
      "loss": 1.5556,
      "step": 614
    },
    {
      "epoch": 1.6895604395604396,
      "grad_norm": 1.3557411432266235,
      "learning_rate": 5e-05,
      "loss": 1.619,
      "step": 615
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 1.313135027885437,
      "learning_rate": 5e-05,
      "loss": 1.5501,
      "step": 616
    },
    {
      "epoch": 1.695054945054945,
      "grad_norm": 1.3141767978668213,
      "learning_rate": 5e-05,
      "loss": 1.6075,
      "step": 617
    },
    {
      "epoch": 1.6978021978021978,
      "grad_norm": 1.3739205598831177,
      "learning_rate": 5e-05,
      "loss": 1.6229,
      "step": 618
    },
    {
      "epoch": 1.7005494505494505,
      "grad_norm": 1.4456279277801514,
      "learning_rate": 5e-05,
      "loss": 1.5674,
      "step": 619
    },
    {
      "epoch": 1.7032967032967035,
      "grad_norm": 1.354751467704773,
      "learning_rate": 5e-05,
      "loss": 1.5913,
      "step": 620
    },
    {
      "epoch": 1.7060439560439562,
      "grad_norm": 1.372889757156372,
      "learning_rate": 5e-05,
      "loss": 1.6186,
      "step": 621
    },
    {
      "epoch": 1.708791208791209,
      "grad_norm": 1.3403030633926392,
      "learning_rate": 5e-05,
      "loss": 1.5581,
      "step": 622
    },
    {
      "epoch": 1.7115384615384617,
      "grad_norm": 1.3591389656066895,
      "learning_rate": 5e-05,
      "loss": 1.6557,
      "step": 623
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 1.3725779056549072,
      "learning_rate": 5e-05,
      "loss": 1.6467,
      "step": 624
    },
    {
      "epoch": 1.7170329670329672,
      "grad_norm": 1.353481411933899,
      "learning_rate": 5e-05,
      "loss": 1.576,
      "step": 625
    },
    {
      "epoch": 1.7197802197802199,
      "grad_norm": 1.3308202028274536,
      "learning_rate": 5e-05,
      "loss": 1.6035,
      "step": 626
    },
    {
      "epoch": 1.7225274725274726,
      "grad_norm": 1.3610906600952148,
      "learning_rate": 5e-05,
      "loss": 1.6628,
      "step": 627
    },
    {
      "epoch": 1.7252747252747254,
      "grad_norm": 1.382727026939392,
      "learning_rate": 5e-05,
      "loss": 1.6571,
      "step": 628
    },
    {
      "epoch": 1.728021978021978,
      "grad_norm": 1.3388129472732544,
      "learning_rate": 5e-05,
      "loss": 1.6365,
      "step": 629
    },
    {
      "epoch": 1.7307692307692308,
      "grad_norm": 1.3086329698562622,
      "learning_rate": 5e-05,
      "loss": 1.6224,
      "step": 630
    },
    {
      "epoch": 1.7335164835164836,
      "grad_norm": 1.3416846990585327,
      "learning_rate": 5e-05,
      "loss": 1.6896,
      "step": 631
    },
    {
      "epoch": 1.7362637362637363,
      "grad_norm": 1.3359869718551636,
      "learning_rate": 5e-05,
      "loss": 1.5507,
      "step": 632
    },
    {
      "epoch": 1.739010989010989,
      "grad_norm": 1.3515206575393677,
      "learning_rate": 5e-05,
      "loss": 1.5509,
      "step": 633
    },
    {
      "epoch": 1.7417582417582418,
      "grad_norm": 1.343172311782837,
      "learning_rate": 5e-05,
      "loss": 1.5694,
      "step": 634
    },
    {
      "epoch": 1.7445054945054945,
      "grad_norm": 1.330289363861084,
      "learning_rate": 5e-05,
      "loss": 1.6174,
      "step": 635
    },
    {
      "epoch": 1.7472527472527473,
      "grad_norm": 1.3185254335403442,
      "learning_rate": 5e-05,
      "loss": 1.5342,
      "step": 636
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.3639471530914307,
      "learning_rate": 5e-05,
      "loss": 1.6189,
      "step": 637
    },
    {
      "epoch": 1.7527472527472527,
      "grad_norm": 1.36421799659729,
      "learning_rate": 5e-05,
      "loss": 1.5923,
      "step": 638
    },
    {
      "epoch": 1.7554945054945055,
      "grad_norm": 1.3845953941345215,
      "learning_rate": 5e-05,
      "loss": 1.6504,
      "step": 639
    },
    {
      "epoch": 1.7582417582417582,
      "grad_norm": 1.3984158039093018,
      "learning_rate": 5e-05,
      "loss": 1.6259,
      "step": 640
    },
    {
      "epoch": 1.760989010989011,
      "grad_norm": 1.3576668500900269,
      "learning_rate": 5e-05,
      "loss": 1.5953,
      "step": 641
    },
    {
      "epoch": 1.7637362637362637,
      "grad_norm": 1.3525739908218384,
      "learning_rate": 5e-05,
      "loss": 1.6309,
      "step": 642
    },
    {
      "epoch": 1.7664835164835164,
      "grad_norm": 1.3605042695999146,
      "learning_rate": 5e-05,
      "loss": 1.58,
      "step": 643
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 1.3632762432098389,
      "learning_rate": 5e-05,
      "loss": 1.6092,
      "step": 644
    },
    {
      "epoch": 1.771978021978022,
      "grad_norm": 1.3505353927612305,
      "learning_rate": 5e-05,
      "loss": 1.5923,
      "step": 645
    },
    {
      "epoch": 1.7747252747252746,
      "grad_norm": 1.339428186416626,
      "learning_rate": 5e-05,
      "loss": 1.5533,
      "step": 646
    },
    {
      "epoch": 1.7774725274725274,
      "grad_norm": 1.36990225315094,
      "learning_rate": 5e-05,
      "loss": 1.5087,
      "step": 647
    },
    {
      "epoch": 1.7802197802197801,
      "grad_norm": 1.350950837135315,
      "learning_rate": 5e-05,
      "loss": 1.6673,
      "step": 648
    },
    {
      "epoch": 1.7829670329670328,
      "grad_norm": 1.3318290710449219,
      "learning_rate": 5e-05,
      "loss": 1.5477,
      "step": 649
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 1.3279502391815186,
      "learning_rate": 5e-05,
      "loss": 1.6044,
      "step": 650
    },
    {
      "epoch": 1.7884615384615383,
      "grad_norm": 1.340660572052002,
      "learning_rate": 5e-05,
      "loss": 1.6098,
      "step": 651
    },
    {
      "epoch": 1.791208791208791,
      "grad_norm": 1.341886281967163,
      "learning_rate": 5e-05,
      "loss": 1.6703,
      "step": 652
    },
    {
      "epoch": 1.7939560439560438,
      "grad_norm": 1.353318691253662,
      "learning_rate": 5e-05,
      "loss": 1.6265,
      "step": 653
    },
    {
      "epoch": 1.7967032967032965,
      "grad_norm": 1.3482275009155273,
      "learning_rate": 5e-05,
      "loss": 1.5676,
      "step": 654
    },
    {
      "epoch": 1.7994505494505495,
      "grad_norm": 1.3921089172363281,
      "learning_rate": 5e-05,
      "loss": 1.6252,
      "step": 655
    },
    {
      "epoch": 1.8021978021978022,
      "grad_norm": 1.3570038080215454,
      "learning_rate": 5e-05,
      "loss": 1.6509,
      "step": 656
    },
    {
      "epoch": 1.804945054945055,
      "grad_norm": 1.348496913909912,
      "learning_rate": 5e-05,
      "loss": 1.6023,
      "step": 657
    },
    {
      "epoch": 1.8076923076923077,
      "grad_norm": 1.3583614826202393,
      "learning_rate": 5e-05,
      "loss": 1.6215,
      "step": 658
    },
    {
      "epoch": 1.8104395604395604,
      "grad_norm": 1.441090703010559,
      "learning_rate": 5e-05,
      "loss": 1.6294,
      "step": 659
    },
    {
      "epoch": 1.8131868131868132,
      "grad_norm": 1.3254880905151367,
      "learning_rate": 5e-05,
      "loss": 1.5265,
      "step": 660
    },
    {
      "epoch": 1.815934065934066,
      "grad_norm": 1.333821177482605,
      "learning_rate": 5e-05,
      "loss": 1.5949,
      "step": 661
    },
    {
      "epoch": 1.8186813186813187,
      "grad_norm": 1.3538273572921753,
      "learning_rate": 5e-05,
      "loss": 1.621,
      "step": 662
    },
    {
      "epoch": 1.8214285714285714,
      "grad_norm": 1.3463973999023438,
      "learning_rate": 5e-05,
      "loss": 1.5854,
      "step": 663
    },
    {
      "epoch": 1.8241758241758241,
      "grad_norm": 1.339087963104248,
      "learning_rate": 5e-05,
      "loss": 1.5974,
      "step": 664
    },
    {
      "epoch": 1.8269230769230769,
      "grad_norm": 1.3350050449371338,
      "learning_rate": 5e-05,
      "loss": 1.5819,
      "step": 665
    },
    {
      "epoch": 1.8296703296703298,
      "grad_norm": 1.411982774734497,
      "learning_rate": 5e-05,
      "loss": 1.629,
      "step": 666
    },
    {
      "epoch": 1.8324175824175826,
      "grad_norm": 1.341802716255188,
      "learning_rate": 5e-05,
      "loss": 1.6019,
      "step": 667
    },
    {
      "epoch": 1.8351648351648353,
      "grad_norm": 1.335489273071289,
      "learning_rate": 5e-05,
      "loss": 1.5214,
      "step": 668
    },
    {
      "epoch": 1.837912087912088,
      "grad_norm": 1.3959370851516724,
      "learning_rate": 5e-05,
      "loss": 1.6273,
      "step": 669
    },
    {
      "epoch": 1.8406593406593408,
      "grad_norm": 1.3139828443527222,
      "learning_rate": 5e-05,
      "loss": 1.5164,
      "step": 670
    },
    {
      "epoch": 1.8434065934065935,
      "grad_norm": 1.3259289264678955,
      "learning_rate": 5e-05,
      "loss": 1.5829,
      "step": 671
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 1.387221336364746,
      "learning_rate": 5e-05,
      "loss": 1.5333,
      "step": 672
    },
    {
      "epoch": 1.848901098901099,
      "grad_norm": 1.3018028736114502,
      "learning_rate": 5e-05,
      "loss": 1.551,
      "step": 673
    },
    {
      "epoch": 1.8516483516483517,
      "grad_norm": 1.4127713441848755,
      "learning_rate": 5e-05,
      "loss": 1.5326,
      "step": 674
    },
    {
      "epoch": 1.8543956043956045,
      "grad_norm": 1.3724905252456665,
      "learning_rate": 5e-05,
      "loss": 1.6165,
      "step": 675
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 1.3802012205123901,
      "learning_rate": 5e-05,
      "loss": 1.6135,
      "step": 676
    },
    {
      "epoch": 1.85989010989011,
      "grad_norm": 1.2917282581329346,
      "learning_rate": 5e-05,
      "loss": 1.5017,
      "step": 677
    },
    {
      "epoch": 1.8626373626373627,
      "grad_norm": 1.3754175901412964,
      "learning_rate": 5e-05,
      "loss": 1.5715,
      "step": 678
    },
    {
      "epoch": 1.8653846153846154,
      "grad_norm": 1.3187360763549805,
      "learning_rate": 5e-05,
      "loss": 1.6097,
      "step": 679
    },
    {
      "epoch": 1.8681318681318682,
      "grad_norm": 1.3531134128570557,
      "learning_rate": 5e-05,
      "loss": 1.5436,
      "step": 680
    },
    {
      "epoch": 1.870879120879121,
      "grad_norm": 1.3975921869277954,
      "learning_rate": 5e-05,
      "loss": 1.5553,
      "step": 681
    },
    {
      "epoch": 1.8736263736263736,
      "grad_norm": 1.3929476737976074,
      "learning_rate": 5e-05,
      "loss": 1.5437,
      "step": 682
    },
    {
      "epoch": 1.8763736263736264,
      "grad_norm": 1.3169854879379272,
      "learning_rate": 5e-05,
      "loss": 1.5373,
      "step": 683
    },
    {
      "epoch": 1.879120879120879,
      "grad_norm": 1.344276785850525,
      "learning_rate": 5e-05,
      "loss": 1.6105,
      "step": 684
    },
    {
      "epoch": 1.8818681318681318,
      "grad_norm": 1.3410825729370117,
      "learning_rate": 5e-05,
      "loss": 1.5903,
      "step": 685
    },
    {
      "epoch": 1.8846153846153846,
      "grad_norm": 1.3787612915039062,
      "learning_rate": 5e-05,
      "loss": 1.5886,
      "step": 686
    },
    {
      "epoch": 1.8873626373626373,
      "grad_norm": 1.3509433269500732,
      "learning_rate": 5e-05,
      "loss": 1.5758,
      "step": 687
    },
    {
      "epoch": 1.89010989010989,
      "grad_norm": 1.3222888708114624,
      "learning_rate": 5e-05,
      "loss": 1.5196,
      "step": 688
    },
    {
      "epoch": 1.8928571428571428,
      "grad_norm": 1.3528538942337036,
      "learning_rate": 5e-05,
      "loss": 1.5788,
      "step": 689
    },
    {
      "epoch": 1.8956043956043955,
      "grad_norm": 1.4003628492355347,
      "learning_rate": 5e-05,
      "loss": 1.597,
      "step": 690
    },
    {
      "epoch": 1.8983516483516483,
      "grad_norm": 1.3265681266784668,
      "learning_rate": 5e-05,
      "loss": 1.5425,
      "step": 691
    },
    {
      "epoch": 1.901098901098901,
      "grad_norm": 1.310156226158142,
      "learning_rate": 5e-05,
      "loss": 1.5175,
      "step": 692
    },
    {
      "epoch": 1.9038461538461537,
      "grad_norm": 1.3719863891601562,
      "learning_rate": 5e-05,
      "loss": 1.5733,
      "step": 693
    },
    {
      "epoch": 1.9065934065934065,
      "grad_norm": 1.3443418741226196,
      "learning_rate": 5e-05,
      "loss": 1.5934,
      "step": 694
    },
    {
      "epoch": 1.9093406593406592,
      "grad_norm": 1.3050074577331543,
      "learning_rate": 5e-05,
      "loss": 1.5268,
      "step": 695
    },
    {
      "epoch": 1.912087912087912,
      "grad_norm": 1.3183311223983765,
      "learning_rate": 5e-05,
      "loss": 1.5557,
      "step": 696
    },
    {
      "epoch": 1.9148351648351647,
      "grad_norm": 1.3206634521484375,
      "learning_rate": 5e-05,
      "loss": 1.5506,
      "step": 697
    },
    {
      "epoch": 1.9175824175824174,
      "grad_norm": 1.3871722221374512,
      "learning_rate": 5e-05,
      "loss": 1.5218,
      "step": 698
    },
    {
      "epoch": 1.9203296703296702,
      "grad_norm": 1.328601598739624,
      "learning_rate": 5e-05,
      "loss": 1.4895,
      "step": 699
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 1.319639801979065,
      "learning_rate": 5e-05,
      "loss": 1.5672,
      "step": 700
    },
    {
      "epoch": 1.9258241758241759,
      "grad_norm": 1.3639757633209229,
      "learning_rate": 5e-05,
      "loss": 1.4929,
      "step": 701
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 1.3198126554489136,
      "learning_rate": 5e-05,
      "loss": 1.4754,
      "step": 702
    },
    {
      "epoch": 1.9313186813186813,
      "grad_norm": 1.3883938789367676,
      "learning_rate": 5e-05,
      "loss": 1.4815,
      "step": 703
    },
    {
      "epoch": 1.934065934065934,
      "grad_norm": 1.3320571184158325,
      "learning_rate": 5e-05,
      "loss": 1.5445,
      "step": 704
    },
    {
      "epoch": 1.9368131868131868,
      "grad_norm": 1.3238390684127808,
      "learning_rate": 5e-05,
      "loss": 1.5168,
      "step": 705
    },
    {
      "epoch": 1.9395604395604396,
      "grad_norm": 1.3073385953903198,
      "learning_rate": 5e-05,
      "loss": 1.4687,
      "step": 706
    },
    {
      "epoch": 1.9423076923076923,
      "grad_norm": 1.3280302286148071,
      "learning_rate": 5e-05,
      "loss": 1.4916,
      "step": 707
    },
    {
      "epoch": 1.945054945054945,
      "grad_norm": 1.3344101905822754,
      "learning_rate": 5e-05,
      "loss": 1.5726,
      "step": 708
    },
    {
      "epoch": 1.9478021978021978,
      "grad_norm": 1.3514151573181152,
      "learning_rate": 5e-05,
      "loss": 1.5911,
      "step": 709
    },
    {
      "epoch": 1.9505494505494505,
      "grad_norm": 1.3038928508758545,
      "learning_rate": 5e-05,
      "loss": 1.5615,
      "step": 710
    },
    {
      "epoch": 1.9532967032967035,
      "grad_norm": 1.3251093626022339,
      "learning_rate": 5e-05,
      "loss": 1.5174,
      "step": 711
    },
    {
      "epoch": 1.9560439560439562,
      "grad_norm": 1.3693393468856812,
      "learning_rate": 5e-05,
      "loss": 1.4893,
      "step": 712
    },
    {
      "epoch": 1.958791208791209,
      "grad_norm": 1.3290073871612549,
      "learning_rate": 5e-05,
      "loss": 1.5336,
      "step": 713
    },
    {
      "epoch": 1.9615384615384617,
      "grad_norm": 1.326183795928955,
      "learning_rate": 5e-05,
      "loss": 1.5047,
      "step": 714
    },
    {
      "epoch": 1.9642857142857144,
      "grad_norm": 1.3393864631652832,
      "learning_rate": 5e-05,
      "loss": 1.5859,
      "step": 715
    },
    {
      "epoch": 1.9670329670329672,
      "grad_norm": 1.3135617971420288,
      "learning_rate": 5e-05,
      "loss": 1.5539,
      "step": 716
    },
    {
      "epoch": 1.9697802197802199,
      "grad_norm": 1.3113824129104614,
      "learning_rate": 5e-05,
      "loss": 1.5333,
      "step": 717
    },
    {
      "epoch": 1.9725274725274726,
      "grad_norm": 1.3680082559585571,
      "learning_rate": 5e-05,
      "loss": 1.5293,
      "step": 718
    },
    {
      "epoch": 1.9752747252747254,
      "grad_norm": 1.3542866706848145,
      "learning_rate": 5e-05,
      "loss": 1.5317,
      "step": 719
    },
    {
      "epoch": 1.978021978021978,
      "grad_norm": 1.2989307641983032,
      "learning_rate": 5e-05,
      "loss": 1.462,
      "step": 720
    },
    {
      "epoch": 1.9807692307692308,
      "grad_norm": 1.3580234050750732,
      "learning_rate": 5e-05,
      "loss": 1.5536,
      "step": 721
    },
    {
      "epoch": 1.9835164835164836,
      "grad_norm": 1.3384798765182495,
      "learning_rate": 5e-05,
      "loss": 1.4706,
      "step": 722
    },
    {
      "epoch": 1.9862637362637363,
      "grad_norm": 1.3332005739212036,
      "learning_rate": 5e-05,
      "loss": 1.555,
      "step": 723
    },
    {
      "epoch": 1.989010989010989,
      "grad_norm": 1.2863943576812744,
      "learning_rate": 5e-05,
      "loss": 1.4859,
      "step": 724
    },
    {
      "epoch": 1.9917582417582418,
      "grad_norm": 1.3213738203048706,
      "learning_rate": 5e-05,
      "loss": 1.5285,
      "step": 725
    },
    {
      "epoch": 1.9945054945054945,
      "grad_norm": 1.3058109283447266,
      "learning_rate": 5e-05,
      "loss": 1.4214,
      "step": 726
    },
    {
      "epoch": 1.9972527472527473,
      "grad_norm": 1.3660190105438232,
      "learning_rate": 5e-05,
      "loss": 1.5436,
      "step": 727
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3345131874084473,
      "learning_rate": 5e-05,
      "loss": 1.5367,
      "step": 728
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.954538106918335,
      "eval_runtime": 81.3322,
      "eval_samples_per_second": 509.085,
      "eval_steps_per_second": 3.984,
      "step": 728
    },
    {
      "epoch": 2.0027472527472527,
      "grad_norm": 1.315796136856079,
      "learning_rate": 5e-05,
      "loss": 1.4716,
      "step": 729
    },
    {
      "epoch": 2.0054945054945055,
      "grad_norm": 1.2828649282455444,
      "learning_rate": 5e-05,
      "loss": 1.3851,
      "step": 730
    },
    {
      "epoch": 2.008241758241758,
      "grad_norm": 1.3185259103775024,
      "learning_rate": 5e-05,
      "loss": 1.4151,
      "step": 731
    },
    {
      "epoch": 2.010989010989011,
      "grad_norm": 1.3098385334014893,
      "learning_rate": 5e-05,
      "loss": 1.4132,
      "step": 732
    },
    {
      "epoch": 2.0137362637362637,
      "grad_norm": 1.3168535232543945,
      "learning_rate": 5e-05,
      "loss": 1.3987,
      "step": 733
    },
    {
      "epoch": 2.0164835164835164,
      "grad_norm": 1.3475897312164307,
      "learning_rate": 5e-05,
      "loss": 1.4343,
      "step": 734
    },
    {
      "epoch": 2.019230769230769,
      "grad_norm": 1.3267464637756348,
      "learning_rate": 5e-05,
      "loss": 1.4306,
      "step": 735
    },
    {
      "epoch": 2.021978021978022,
      "grad_norm": 1.272188425064087,
      "learning_rate": 5e-05,
      "loss": 1.3929,
      "step": 736
    },
    {
      "epoch": 2.0247252747252746,
      "grad_norm": 1.3001348972320557,
      "learning_rate": 5e-05,
      "loss": 1.4756,
      "step": 737
    },
    {
      "epoch": 2.0274725274725274,
      "grad_norm": 1.3183485269546509,
      "learning_rate": 5e-05,
      "loss": 1.4429,
      "step": 738
    },
    {
      "epoch": 2.03021978021978,
      "grad_norm": 1.2820549011230469,
      "learning_rate": 5e-05,
      "loss": 1.4741,
      "step": 739
    },
    {
      "epoch": 2.032967032967033,
      "grad_norm": 1.348897933959961,
      "learning_rate": 5e-05,
      "loss": 1.4226,
      "step": 740
    },
    {
      "epoch": 2.0357142857142856,
      "grad_norm": 1.2971142530441284,
      "learning_rate": 5e-05,
      "loss": 1.4347,
      "step": 741
    },
    {
      "epoch": 2.0384615384615383,
      "grad_norm": 1.3691017627716064,
      "learning_rate": 5e-05,
      "loss": 1.4847,
      "step": 742
    },
    {
      "epoch": 2.041208791208791,
      "grad_norm": 1.3164775371551514,
      "learning_rate": 5e-05,
      "loss": 1.4633,
      "step": 743
    },
    {
      "epoch": 2.043956043956044,
      "grad_norm": 1.3017421960830688,
      "learning_rate": 5e-05,
      "loss": 1.4226,
      "step": 744
    },
    {
      "epoch": 2.0467032967032965,
      "grad_norm": 1.2984309196472168,
      "learning_rate": 5e-05,
      "loss": 1.4021,
      "step": 745
    },
    {
      "epoch": 2.0494505494505493,
      "grad_norm": 1.3153553009033203,
      "learning_rate": 5e-05,
      "loss": 1.4394,
      "step": 746
    },
    {
      "epoch": 2.052197802197802,
      "grad_norm": 1.3346918821334839,
      "learning_rate": 5e-05,
      "loss": 1.5097,
      "step": 747
    },
    {
      "epoch": 2.0549450549450547,
      "grad_norm": 1.2885555028915405,
      "learning_rate": 5e-05,
      "loss": 1.4301,
      "step": 748
    },
    {
      "epoch": 2.0576923076923075,
      "grad_norm": 1.3163599967956543,
      "learning_rate": 5e-05,
      "loss": 1.4333,
      "step": 749
    },
    {
      "epoch": 2.0604395604395602,
      "grad_norm": 1.3289945125579834,
      "learning_rate": 5e-05,
      "loss": 1.397,
      "step": 750
    },
    {
      "epoch": 2.063186813186813,
      "grad_norm": 1.3193801641464233,
      "learning_rate": 5e-05,
      "loss": 1.4024,
      "step": 751
    },
    {
      "epoch": 2.065934065934066,
      "grad_norm": 1.334618330001831,
      "learning_rate": 5e-05,
      "loss": 1.4768,
      "step": 752
    },
    {
      "epoch": 2.068681318681319,
      "grad_norm": 1.3069005012512207,
      "learning_rate": 5e-05,
      "loss": 1.4637,
      "step": 753
    },
    {
      "epoch": 2.0714285714285716,
      "grad_norm": 1.3173259496688843,
      "learning_rate": 5e-05,
      "loss": 1.4756,
      "step": 754
    },
    {
      "epoch": 2.0741758241758244,
      "grad_norm": 1.2852495908737183,
      "learning_rate": 5e-05,
      "loss": 1.3291,
      "step": 755
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 1.3646012544631958,
      "learning_rate": 5e-05,
      "loss": 1.4577,
      "step": 756
    },
    {
      "epoch": 2.07967032967033,
      "grad_norm": 1.3659964799880981,
      "learning_rate": 5e-05,
      "loss": 1.4151,
      "step": 757
    },
    {
      "epoch": 2.0824175824175826,
      "grad_norm": 1.3129523992538452,
      "learning_rate": 5e-05,
      "loss": 1.3929,
      "step": 758
    },
    {
      "epoch": 2.0851648351648353,
      "grad_norm": 1.3072792291641235,
      "learning_rate": 5e-05,
      "loss": 1.3957,
      "step": 759
    },
    {
      "epoch": 2.087912087912088,
      "grad_norm": 1.323655128479004,
      "learning_rate": 5e-05,
      "loss": 1.3971,
      "step": 760
    },
    {
      "epoch": 2.090659340659341,
      "grad_norm": 1.3413864374160767,
      "learning_rate": 5e-05,
      "loss": 1.4986,
      "step": 761
    },
    {
      "epoch": 2.0934065934065935,
      "grad_norm": 1.3241790533065796,
      "learning_rate": 5e-05,
      "loss": 1.5022,
      "step": 762
    },
    {
      "epoch": 2.0961538461538463,
      "grad_norm": 1.305673599243164,
      "learning_rate": 5e-05,
      "loss": 1.4605,
      "step": 763
    },
    {
      "epoch": 2.098901098901099,
      "grad_norm": 1.3303697109222412,
      "learning_rate": 5e-05,
      "loss": 1.3911,
      "step": 764
    },
    {
      "epoch": 2.1016483516483517,
      "grad_norm": 1.3367358446121216,
      "learning_rate": 5e-05,
      "loss": 1.4912,
      "step": 765
    },
    {
      "epoch": 2.1043956043956045,
      "grad_norm": 1.270965576171875,
      "learning_rate": 5e-05,
      "loss": 1.4444,
      "step": 766
    },
    {
      "epoch": 2.107142857142857,
      "grad_norm": 1.3126580715179443,
      "learning_rate": 5e-05,
      "loss": 1.4742,
      "step": 767
    },
    {
      "epoch": 2.10989010989011,
      "grad_norm": 1.3473926782608032,
      "learning_rate": 5e-05,
      "loss": 1.4599,
      "step": 768
    },
    {
      "epoch": 2.1126373626373627,
      "grad_norm": 1.2847285270690918,
      "learning_rate": 5e-05,
      "loss": 1.3882,
      "step": 769
    },
    {
      "epoch": 2.1153846153846154,
      "grad_norm": 1.3146511316299438,
      "learning_rate": 5e-05,
      "loss": 1.4219,
      "step": 770
    },
    {
      "epoch": 2.118131868131868,
      "grad_norm": 1.3318673372268677,
      "learning_rate": 5e-05,
      "loss": 1.3771,
      "step": 771
    },
    {
      "epoch": 2.120879120879121,
      "grad_norm": 1.3439126014709473,
      "learning_rate": 5e-05,
      "loss": 1.4362,
      "step": 772
    },
    {
      "epoch": 2.1236263736263736,
      "grad_norm": 1.2842645645141602,
      "learning_rate": 5e-05,
      "loss": 1.4415,
      "step": 773
    },
    {
      "epoch": 2.1263736263736264,
      "grad_norm": 1.2941837310791016,
      "learning_rate": 5e-05,
      "loss": 1.4283,
      "step": 774
    },
    {
      "epoch": 2.129120879120879,
      "grad_norm": 1.323481559753418,
      "learning_rate": 5e-05,
      "loss": 1.4394,
      "step": 775
    },
    {
      "epoch": 2.131868131868132,
      "grad_norm": 1.2734895944595337,
      "learning_rate": 5e-05,
      "loss": 1.3879,
      "step": 776
    },
    {
      "epoch": 2.1346153846153846,
      "grad_norm": 1.2664309740066528,
      "learning_rate": 5e-05,
      "loss": 1.3589,
      "step": 777
    },
    {
      "epoch": 2.1373626373626373,
      "grad_norm": 1.2938532829284668,
      "learning_rate": 5e-05,
      "loss": 1.407,
      "step": 778
    },
    {
      "epoch": 2.14010989010989,
      "grad_norm": 1.287401556968689,
      "learning_rate": 5e-05,
      "loss": 1.4107,
      "step": 779
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 1.33916175365448,
      "learning_rate": 5e-05,
      "loss": 1.4409,
      "step": 780
    },
    {
      "epoch": 2.1456043956043955,
      "grad_norm": 1.3155021667480469,
      "learning_rate": 5e-05,
      "loss": 1.4627,
      "step": 781
    },
    {
      "epoch": 2.1483516483516483,
      "grad_norm": 1.3075273036956787,
      "learning_rate": 5e-05,
      "loss": 1.4145,
      "step": 782
    },
    {
      "epoch": 2.151098901098901,
      "grad_norm": 1.2915935516357422,
      "learning_rate": 5e-05,
      "loss": 1.4356,
      "step": 783
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 1.402058720588684,
      "learning_rate": 5e-05,
      "loss": 1.4485,
      "step": 784
    },
    {
      "epoch": 2.1565934065934065,
      "grad_norm": 1.3323464393615723,
      "learning_rate": 5e-05,
      "loss": 1.4545,
      "step": 785
    },
    {
      "epoch": 2.159340659340659,
      "grad_norm": 1.3306952714920044,
      "learning_rate": 5e-05,
      "loss": 1.3686,
      "step": 786
    },
    {
      "epoch": 2.162087912087912,
      "grad_norm": 1.327041745185852,
      "learning_rate": 5e-05,
      "loss": 1.4349,
      "step": 787
    },
    {
      "epoch": 2.1648351648351647,
      "grad_norm": 1.2470214366912842,
      "learning_rate": 5e-05,
      "loss": 1.38,
      "step": 788
    },
    {
      "epoch": 2.1675824175824174,
      "grad_norm": 1.3037207126617432,
      "learning_rate": 5e-05,
      "loss": 1.4218,
      "step": 789
    },
    {
      "epoch": 2.17032967032967,
      "grad_norm": 1.3220652341842651,
      "learning_rate": 5e-05,
      "loss": 1.4204,
      "step": 790
    },
    {
      "epoch": 2.173076923076923,
      "grad_norm": 1.3193516731262207,
      "learning_rate": 5e-05,
      "loss": 1.4018,
      "step": 791
    },
    {
      "epoch": 2.1758241758241756,
      "grad_norm": 1.325196385383606,
      "learning_rate": 5e-05,
      "loss": 1.4338,
      "step": 792
    },
    {
      "epoch": 2.1785714285714284,
      "grad_norm": 1.275253415107727,
      "learning_rate": 5e-05,
      "loss": 1.3485,
      "step": 793
    },
    {
      "epoch": 2.181318681318681,
      "grad_norm": 1.3013345003128052,
      "learning_rate": 5e-05,
      "loss": 1.4358,
      "step": 794
    },
    {
      "epoch": 2.1840659340659343,
      "grad_norm": 1.334696888923645,
      "learning_rate": 5e-05,
      "loss": 1.4065,
      "step": 795
    },
    {
      "epoch": 2.186813186813187,
      "grad_norm": 1.3205056190490723,
      "learning_rate": 5e-05,
      "loss": 1.3683,
      "step": 796
    },
    {
      "epoch": 2.1895604395604398,
      "grad_norm": 1.2774642705917358,
      "learning_rate": 5e-05,
      "loss": 1.4106,
      "step": 797
    },
    {
      "epoch": 2.1923076923076925,
      "grad_norm": 1.3695629835128784,
      "learning_rate": 5e-05,
      "loss": 1.3591,
      "step": 798
    },
    {
      "epoch": 2.1950549450549453,
      "grad_norm": 1.3135969638824463,
      "learning_rate": 5e-05,
      "loss": 1.4258,
      "step": 799
    },
    {
      "epoch": 2.197802197802198,
      "grad_norm": 1.3034483194351196,
      "learning_rate": 5e-05,
      "loss": 1.4006,
      "step": 800
    },
    {
      "epoch": 2.2005494505494507,
      "grad_norm": 1.2914865016937256,
      "learning_rate": 5e-05,
      "loss": 1.4599,
      "step": 801
    },
    {
      "epoch": 2.2032967032967035,
      "grad_norm": 1.2998790740966797,
      "learning_rate": 5e-05,
      "loss": 1.3561,
      "step": 802
    },
    {
      "epoch": 2.206043956043956,
      "grad_norm": 1.3107832670211792,
      "learning_rate": 5e-05,
      "loss": 1.36,
      "step": 803
    },
    {
      "epoch": 2.208791208791209,
      "grad_norm": 1.3519591093063354,
      "learning_rate": 5e-05,
      "loss": 1.4306,
      "step": 804
    },
    {
      "epoch": 2.2115384615384617,
      "grad_norm": 1.3529455661773682,
      "learning_rate": 5e-05,
      "loss": 1.3908,
      "step": 805
    },
    {
      "epoch": 2.2142857142857144,
      "grad_norm": 1.3169516324996948,
      "learning_rate": 5e-05,
      "loss": 1.3976,
      "step": 806
    },
    {
      "epoch": 2.217032967032967,
      "grad_norm": 1.3755677938461304,
      "learning_rate": 5e-05,
      "loss": 1.4692,
      "step": 807
    },
    {
      "epoch": 2.21978021978022,
      "grad_norm": 1.3362139463424683,
      "learning_rate": 5e-05,
      "loss": 1.4258,
      "step": 808
    },
    {
      "epoch": 2.2225274725274726,
      "grad_norm": 1.328106164932251,
      "learning_rate": 5e-05,
      "loss": 1.4466,
      "step": 809
    },
    {
      "epoch": 2.2252747252747254,
      "grad_norm": 1.326109766960144,
      "learning_rate": 5e-05,
      "loss": 1.4235,
      "step": 810
    },
    {
      "epoch": 2.228021978021978,
      "grad_norm": 1.3724699020385742,
      "learning_rate": 5e-05,
      "loss": 1.3613,
      "step": 811
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 1.3319302797317505,
      "learning_rate": 5e-05,
      "loss": 1.4384,
      "step": 812
    },
    {
      "epoch": 2.2335164835164836,
      "grad_norm": 1.3194271326065063,
      "learning_rate": 5e-05,
      "loss": 1.4129,
      "step": 813
    },
    {
      "epoch": 2.2362637362637363,
      "grad_norm": 1.305160641670227,
      "learning_rate": 5e-05,
      "loss": 1.3549,
      "step": 814
    },
    {
      "epoch": 2.239010989010989,
      "grad_norm": 1.323040246963501,
      "learning_rate": 5e-05,
      "loss": 1.3904,
      "step": 815
    },
    {
      "epoch": 2.241758241758242,
      "grad_norm": 1.323970913887024,
      "learning_rate": 5e-05,
      "loss": 1.3833,
      "step": 816
    },
    {
      "epoch": 2.2445054945054945,
      "grad_norm": 1.3141322135925293,
      "learning_rate": 5e-05,
      "loss": 1.4231,
      "step": 817
    },
    {
      "epoch": 2.2472527472527473,
      "grad_norm": 1.3110321760177612,
      "learning_rate": 5e-05,
      "loss": 1.432,
      "step": 818
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.3411279916763306,
      "learning_rate": 5e-05,
      "loss": 1.4391,
      "step": 819
    },
    {
      "epoch": 2.2527472527472527,
      "grad_norm": 1.331864833831787,
      "learning_rate": 5e-05,
      "loss": 1.3584,
      "step": 820
    },
    {
      "epoch": 2.2554945054945055,
      "grad_norm": 1.2935656309127808,
      "learning_rate": 5e-05,
      "loss": 1.3925,
      "step": 821
    },
    {
      "epoch": 2.258241758241758,
      "grad_norm": 1.2984535694122314,
      "learning_rate": 5e-05,
      "loss": 1.3521,
      "step": 822
    },
    {
      "epoch": 2.260989010989011,
      "grad_norm": 1.3328953981399536,
      "learning_rate": 5e-05,
      "loss": 1.4581,
      "step": 823
    },
    {
      "epoch": 2.2637362637362637,
      "grad_norm": 1.3397198915481567,
      "learning_rate": 5e-05,
      "loss": 1.3681,
      "step": 824
    },
    {
      "epoch": 2.2664835164835164,
      "grad_norm": 1.3088083267211914,
      "learning_rate": 5e-05,
      "loss": 1.4234,
      "step": 825
    },
    {
      "epoch": 2.269230769230769,
      "grad_norm": 1.3056776523590088,
      "learning_rate": 5e-05,
      "loss": 1.4178,
      "step": 826
    },
    {
      "epoch": 2.271978021978022,
      "grad_norm": 1.351029872894287,
      "learning_rate": 5e-05,
      "loss": 1.3117,
      "step": 827
    },
    {
      "epoch": 2.2747252747252746,
      "grad_norm": 1.2952734231948853,
      "learning_rate": 5e-05,
      "loss": 1.3436,
      "step": 828
    },
    {
      "epoch": 2.2774725274725274,
      "grad_norm": 1.28357994556427,
      "learning_rate": 5e-05,
      "loss": 1.3582,
      "step": 829
    },
    {
      "epoch": 2.28021978021978,
      "grad_norm": 1.2726918458938599,
      "learning_rate": 5e-05,
      "loss": 1.347,
      "step": 830
    },
    {
      "epoch": 2.282967032967033,
      "grad_norm": 1.3372844457626343,
      "learning_rate": 5e-05,
      "loss": 1.3874,
      "step": 831
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 1.2925779819488525,
      "learning_rate": 5e-05,
      "loss": 1.338,
      "step": 832
    },
    {
      "epoch": 2.2884615384615383,
      "grad_norm": 1.2922916412353516,
      "learning_rate": 5e-05,
      "loss": 1.3536,
      "step": 833
    },
    {
      "epoch": 2.291208791208791,
      "grad_norm": 1.298819661140442,
      "learning_rate": 5e-05,
      "loss": 1.4326,
      "step": 834
    },
    {
      "epoch": 2.293956043956044,
      "grad_norm": 1.3744784593582153,
      "learning_rate": 5e-05,
      "loss": 1.4306,
      "step": 835
    },
    {
      "epoch": 2.2967032967032965,
      "grad_norm": 1.3578368425369263,
      "learning_rate": 5e-05,
      "loss": 1.4244,
      "step": 836
    },
    {
      "epoch": 2.2994505494505493,
      "grad_norm": 1.2928228378295898,
      "learning_rate": 5e-05,
      "loss": 1.3348,
      "step": 837
    },
    {
      "epoch": 2.302197802197802,
      "grad_norm": 1.319601058959961,
      "learning_rate": 5e-05,
      "loss": 1.3602,
      "step": 838
    },
    {
      "epoch": 2.3049450549450547,
      "grad_norm": 1.3206130266189575,
      "learning_rate": 5e-05,
      "loss": 1.3536,
      "step": 839
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 1.3647592067718506,
      "learning_rate": 5e-05,
      "loss": 1.4272,
      "step": 840
    },
    {
      "epoch": 2.3104395604395602,
      "grad_norm": 1.32209050655365,
      "learning_rate": 5e-05,
      "loss": 1.3696,
      "step": 841
    },
    {
      "epoch": 2.313186813186813,
      "grad_norm": 1.282860517501831,
      "learning_rate": 5e-05,
      "loss": 1.4132,
      "step": 842
    },
    {
      "epoch": 2.3159340659340657,
      "grad_norm": 1.3159857988357544,
      "learning_rate": 5e-05,
      "loss": 1.3808,
      "step": 843
    },
    {
      "epoch": 2.3186813186813184,
      "grad_norm": 1.3147728443145752,
      "learning_rate": 5e-05,
      "loss": 1.3225,
      "step": 844
    },
    {
      "epoch": 2.3214285714285716,
      "grad_norm": 1.323182463645935,
      "learning_rate": 5e-05,
      "loss": 1.4169,
      "step": 845
    },
    {
      "epoch": 2.3241758241758244,
      "grad_norm": 1.3315256834030151,
      "learning_rate": 5e-05,
      "loss": 1.3941,
      "step": 846
    },
    {
      "epoch": 2.326923076923077,
      "grad_norm": 1.3205779790878296,
      "learning_rate": 5e-05,
      "loss": 1.3495,
      "step": 847
    },
    {
      "epoch": 2.32967032967033,
      "grad_norm": 1.3534189462661743,
      "learning_rate": 5e-05,
      "loss": 1.4603,
      "step": 848
    },
    {
      "epoch": 2.3324175824175826,
      "grad_norm": 1.3125042915344238,
      "learning_rate": 5e-05,
      "loss": 1.3928,
      "step": 849
    },
    {
      "epoch": 2.3351648351648353,
      "grad_norm": 1.3057174682617188,
      "learning_rate": 5e-05,
      "loss": 1.4435,
      "step": 850
    },
    {
      "epoch": 2.337912087912088,
      "grad_norm": 1.3239012956619263,
      "learning_rate": 5e-05,
      "loss": 1.3715,
      "step": 851
    },
    {
      "epoch": 2.340659340659341,
      "grad_norm": 1.295697569847107,
      "learning_rate": 5e-05,
      "loss": 1.3608,
      "step": 852
    },
    {
      "epoch": 2.3434065934065935,
      "grad_norm": 1.3129074573516846,
      "learning_rate": 5e-05,
      "loss": 1.3548,
      "step": 853
    },
    {
      "epoch": 2.3461538461538463,
      "grad_norm": 1.320739984512329,
      "learning_rate": 5e-05,
      "loss": 1.3715,
      "step": 854
    },
    {
      "epoch": 2.348901098901099,
      "grad_norm": 1.2922844886779785,
      "learning_rate": 5e-05,
      "loss": 1.3514,
      "step": 855
    },
    {
      "epoch": 2.3516483516483517,
      "grad_norm": 1.3155494928359985,
      "learning_rate": 5e-05,
      "loss": 1.4536,
      "step": 856
    },
    {
      "epoch": 2.3543956043956045,
      "grad_norm": 1.3173236846923828,
      "learning_rate": 5e-05,
      "loss": 1.4229,
      "step": 857
    },
    {
      "epoch": 2.357142857142857,
      "grad_norm": 1.3081567287445068,
      "learning_rate": 5e-05,
      "loss": 1.3795,
      "step": 858
    },
    {
      "epoch": 2.35989010989011,
      "grad_norm": 1.3185911178588867,
      "learning_rate": 5e-05,
      "loss": 1.3719,
      "step": 859
    },
    {
      "epoch": 2.3626373626373627,
      "grad_norm": 1.3589588403701782,
      "learning_rate": 5e-05,
      "loss": 1.3662,
      "step": 860
    },
    {
      "epoch": 2.3653846153846154,
      "grad_norm": 1.3231605291366577,
      "learning_rate": 5e-05,
      "loss": 1.3762,
      "step": 861
    },
    {
      "epoch": 2.368131868131868,
      "grad_norm": 1.318847417831421,
      "learning_rate": 5e-05,
      "loss": 1.441,
      "step": 862
    },
    {
      "epoch": 2.370879120879121,
      "grad_norm": 1.3184151649475098,
      "learning_rate": 5e-05,
      "loss": 1.3521,
      "step": 863
    },
    {
      "epoch": 2.3736263736263736,
      "grad_norm": 1.2885407209396362,
      "learning_rate": 5e-05,
      "loss": 1.3454,
      "step": 864
    },
    {
      "epoch": 2.3763736263736264,
      "grad_norm": 1.3391897678375244,
      "learning_rate": 5e-05,
      "loss": 1.3159,
      "step": 865
    },
    {
      "epoch": 2.379120879120879,
      "grad_norm": 1.3252198696136475,
      "learning_rate": 5e-05,
      "loss": 1.3709,
      "step": 866
    },
    {
      "epoch": 2.381868131868132,
      "grad_norm": 1.2981842756271362,
      "learning_rate": 5e-05,
      "loss": 1.3655,
      "step": 867
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 1.3081300258636475,
      "learning_rate": 5e-05,
      "loss": 1.3373,
      "step": 868
    },
    {
      "epoch": 2.3873626373626373,
      "grad_norm": 1.3082942962646484,
      "learning_rate": 5e-05,
      "loss": 1.3547,
      "step": 869
    },
    {
      "epoch": 2.39010989010989,
      "grad_norm": 1.3420895338058472,
      "learning_rate": 5e-05,
      "loss": 1.3565,
      "step": 870
    },
    {
      "epoch": 2.392857142857143,
      "grad_norm": 1.29417085647583,
      "learning_rate": 5e-05,
      "loss": 1.3934,
      "step": 871
    },
    {
      "epoch": 2.3956043956043955,
      "grad_norm": 1.3736916780471802,
      "learning_rate": 5e-05,
      "loss": 1.3533,
      "step": 872
    },
    {
      "epoch": 2.3983516483516483,
      "grad_norm": 1.3853782415390015,
      "learning_rate": 5e-05,
      "loss": 1.4335,
      "step": 873
    },
    {
      "epoch": 2.401098901098901,
      "grad_norm": 1.264515995979309,
      "learning_rate": 5e-05,
      "loss": 1.2963,
      "step": 874
    },
    {
      "epoch": 2.4038461538461537,
      "grad_norm": 1.2906718254089355,
      "learning_rate": 5e-05,
      "loss": 1.3359,
      "step": 875
    },
    {
      "epoch": 2.4065934065934065,
      "grad_norm": 1.3171089887619019,
      "learning_rate": 5e-05,
      "loss": 1.372,
      "step": 876
    },
    {
      "epoch": 2.409340659340659,
      "grad_norm": 1.311485767364502,
      "learning_rate": 5e-05,
      "loss": 1.3204,
      "step": 877
    },
    {
      "epoch": 2.412087912087912,
      "grad_norm": 1.2774829864501953,
      "learning_rate": 5e-05,
      "loss": 1.3247,
      "step": 878
    },
    {
      "epoch": 2.4148351648351647,
      "grad_norm": 1.3282588720321655,
      "learning_rate": 5e-05,
      "loss": 1.3747,
      "step": 879
    },
    {
      "epoch": 2.4175824175824174,
      "grad_norm": 1.3397858142852783,
      "learning_rate": 5e-05,
      "loss": 1.3806,
      "step": 880
    },
    {
      "epoch": 2.42032967032967,
      "grad_norm": 1.2763034105300903,
      "learning_rate": 5e-05,
      "loss": 1.2927,
      "step": 881
    },
    {
      "epoch": 2.423076923076923,
      "grad_norm": 1.286757230758667,
      "learning_rate": 5e-05,
      "loss": 1.3035,
      "step": 882
    },
    {
      "epoch": 2.4258241758241756,
      "grad_norm": 1.346250295639038,
      "learning_rate": 5e-05,
      "loss": 1.4101,
      "step": 883
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 1.3364745378494263,
      "learning_rate": 5e-05,
      "loss": 1.3337,
      "step": 884
    },
    {
      "epoch": 2.4313186813186816,
      "grad_norm": 1.2955888509750366,
      "learning_rate": 5e-05,
      "loss": 1.3727,
      "step": 885
    },
    {
      "epoch": 2.4340659340659343,
      "grad_norm": 1.3050360679626465,
      "learning_rate": 5e-05,
      "loss": 1.367,
      "step": 886
    },
    {
      "epoch": 2.436813186813187,
      "grad_norm": 1.3256412744522095,
      "learning_rate": 5e-05,
      "loss": 1.3493,
      "step": 887
    },
    {
      "epoch": 2.4395604395604398,
      "grad_norm": 1.3807326555252075,
      "learning_rate": 5e-05,
      "loss": 1.3915,
      "step": 888
    },
    {
      "epoch": 2.4423076923076925,
      "grad_norm": 1.339819073677063,
      "learning_rate": 5e-05,
      "loss": 1.3847,
      "step": 889
    },
    {
      "epoch": 2.4450549450549453,
      "grad_norm": 1.2922471761703491,
      "learning_rate": 5e-05,
      "loss": 1.3594,
      "step": 890
    },
    {
      "epoch": 2.447802197802198,
      "grad_norm": 1.3758305311203003,
      "learning_rate": 5e-05,
      "loss": 1.3721,
      "step": 891
    },
    {
      "epoch": 2.4505494505494507,
      "grad_norm": 1.3840203285217285,
      "learning_rate": 5e-05,
      "loss": 1.4384,
      "step": 892
    },
    {
      "epoch": 2.4532967032967035,
      "grad_norm": 1.2881388664245605,
      "learning_rate": 5e-05,
      "loss": 1.2977,
      "step": 893
    },
    {
      "epoch": 2.456043956043956,
      "grad_norm": 1.3026882410049438,
      "learning_rate": 5e-05,
      "loss": 1.3817,
      "step": 894
    },
    {
      "epoch": 2.458791208791209,
      "grad_norm": 1.344905138015747,
      "learning_rate": 5e-05,
      "loss": 1.405,
      "step": 895
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 1.3098149299621582,
      "learning_rate": 5e-05,
      "loss": 1.3462,
      "step": 896
    },
    {
      "epoch": 2.4642857142857144,
      "grad_norm": 1.2548491954803467,
      "learning_rate": 5e-05,
      "loss": 1.323,
      "step": 897
    },
    {
      "epoch": 2.467032967032967,
      "grad_norm": 1.31222403049469,
      "learning_rate": 5e-05,
      "loss": 1.42,
      "step": 898
    },
    {
      "epoch": 2.46978021978022,
      "grad_norm": 1.3492634296417236,
      "learning_rate": 5e-05,
      "loss": 1.3306,
      "step": 899
    },
    {
      "epoch": 2.4725274725274726,
      "grad_norm": 1.3304742574691772,
      "learning_rate": 5e-05,
      "loss": 1.349,
      "step": 900
    },
    {
      "epoch": 2.4752747252747254,
      "grad_norm": 1.3048230409622192,
      "learning_rate": 5e-05,
      "loss": 1.3921,
      "step": 901
    },
    {
      "epoch": 2.478021978021978,
      "grad_norm": 1.2968143224716187,
      "learning_rate": 5e-05,
      "loss": 1.3457,
      "step": 902
    },
    {
      "epoch": 2.480769230769231,
      "grad_norm": 1.2936043739318848,
      "learning_rate": 5e-05,
      "loss": 1.427,
      "step": 903
    },
    {
      "epoch": 2.4835164835164836,
      "grad_norm": 1.2933539152145386,
      "learning_rate": 5e-05,
      "loss": 1.407,
      "step": 904
    },
    {
      "epoch": 2.4862637362637363,
      "grad_norm": 1.2971360683441162,
      "learning_rate": 5e-05,
      "loss": 1.3463,
      "step": 905
    },
    {
      "epoch": 2.489010989010989,
      "grad_norm": 1.288578748703003,
      "learning_rate": 5e-05,
      "loss": 1.3266,
      "step": 906
    },
    {
      "epoch": 2.491758241758242,
      "grad_norm": 1.2801061868667603,
      "learning_rate": 5e-05,
      "loss": 1.3812,
      "step": 907
    },
    {
      "epoch": 2.4945054945054945,
      "grad_norm": 1.2886395454406738,
      "learning_rate": 5e-05,
      "loss": 1.3221,
      "step": 908
    },
    {
      "epoch": 2.4972527472527473,
      "grad_norm": 1.2720705270767212,
      "learning_rate": 5e-05,
      "loss": 1.3474,
      "step": 909
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.3324912786483765,
      "learning_rate": 5e-05,
      "loss": 1.289,
      "step": 910
    },
    {
      "epoch": 2.5,
      "eval_loss": 1.9168306589126587,
      "eval_runtime": 101.0714,
      "eval_samples_per_second": 409.661,
      "eval_steps_per_second": 3.206,
      "step": 910
    },
    {
      "epoch": 2.5027472527472527,
      "grad_norm": 1.3666412830352783,
      "learning_rate": 5e-05,
      "loss": 1.4401,
      "step": 911
    },
    {
      "epoch": 2.5054945054945055,
      "grad_norm": 1.2782138586044312,
      "learning_rate": 5e-05,
      "loss": 1.36,
      "step": 912
    },
    {
      "epoch": 2.508241758241758,
      "grad_norm": 1.300771713256836,
      "learning_rate": 5e-05,
      "loss": 1.3373,
      "step": 913
    },
    {
      "epoch": 2.510989010989011,
      "grad_norm": 1.3138340711593628,
      "learning_rate": 5e-05,
      "loss": 1.3307,
      "step": 914
    },
    {
      "epoch": 2.5137362637362637,
      "grad_norm": 1.3454757928848267,
      "learning_rate": 5e-05,
      "loss": 1.3958,
      "step": 915
    },
    {
      "epoch": 2.5164835164835164,
      "grad_norm": 1.2974106073379517,
      "learning_rate": 5e-05,
      "loss": 1.339,
      "step": 916
    },
    {
      "epoch": 2.519230769230769,
      "grad_norm": 1.3039288520812988,
      "learning_rate": 5e-05,
      "loss": 1.2651,
      "step": 917
    },
    {
      "epoch": 2.521978021978022,
      "grad_norm": 1.2630516290664673,
      "learning_rate": 5e-05,
      "loss": 1.2951,
      "step": 918
    },
    {
      "epoch": 2.5247252747252746,
      "grad_norm": 1.362013339996338,
      "learning_rate": 5e-05,
      "loss": 1.396,
      "step": 919
    },
    {
      "epoch": 2.5274725274725274,
      "grad_norm": 1.2981503009796143,
      "learning_rate": 5e-05,
      "loss": 1.3641,
      "step": 920
    },
    {
      "epoch": 2.53021978021978,
      "grad_norm": 1.3193185329437256,
      "learning_rate": 5e-05,
      "loss": 1.3489,
      "step": 921
    },
    {
      "epoch": 2.532967032967033,
      "grad_norm": 1.3059749603271484,
      "learning_rate": 5e-05,
      "loss": 1.3409,
      "step": 922
    },
    {
      "epoch": 2.5357142857142856,
      "grad_norm": 1.3447952270507812,
      "learning_rate": 5e-05,
      "loss": 1.3221,
      "step": 923
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 1.3217792510986328,
      "learning_rate": 5e-05,
      "loss": 1.3586,
      "step": 924
    },
    {
      "epoch": 2.541208791208791,
      "grad_norm": 1.3583836555480957,
      "learning_rate": 5e-05,
      "loss": 1.351,
      "step": 925
    },
    {
      "epoch": 2.543956043956044,
      "grad_norm": 1.3584531545639038,
      "learning_rate": 5e-05,
      "loss": 1.3004,
      "step": 926
    },
    {
      "epoch": 2.5467032967032965,
      "grad_norm": 1.338019847869873,
      "learning_rate": 5e-05,
      "loss": 1.3304,
      "step": 927
    },
    {
      "epoch": 2.5494505494505493,
      "grad_norm": 1.3167253732681274,
      "learning_rate": 5e-05,
      "loss": 1.3538,
      "step": 928
    },
    {
      "epoch": 2.552197802197802,
      "grad_norm": 1.3140376806259155,
      "learning_rate": 5e-05,
      "loss": 1.3398,
      "step": 929
    },
    {
      "epoch": 2.5549450549450547,
      "grad_norm": 1.2686489820480347,
      "learning_rate": 5e-05,
      "loss": 1.3028,
      "step": 930
    },
    {
      "epoch": 2.5576923076923075,
      "grad_norm": 1.263027548789978,
      "learning_rate": 5e-05,
      "loss": 1.293,
      "step": 931
    },
    {
      "epoch": 2.5604395604395602,
      "grad_norm": 1.3015995025634766,
      "learning_rate": 5e-05,
      "loss": 1.3765,
      "step": 932
    },
    {
      "epoch": 2.563186813186813,
      "grad_norm": 1.3335119485855103,
      "learning_rate": 5e-05,
      "loss": 1.3139,
      "step": 933
    },
    {
      "epoch": 2.5659340659340657,
      "grad_norm": 1.3064124584197998,
      "learning_rate": 5e-05,
      "loss": 1.3342,
      "step": 934
    },
    {
      "epoch": 2.5686813186813184,
      "grad_norm": 1.310696005821228,
      "learning_rate": 5e-05,
      "loss": 1.3731,
      "step": 935
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.333640456199646,
      "learning_rate": 5e-05,
      "loss": 1.317,
      "step": 936
    },
    {
      "epoch": 2.574175824175824,
      "grad_norm": 1.3254016637802124,
      "learning_rate": 5e-05,
      "loss": 1.3372,
      "step": 937
    },
    {
      "epoch": 2.5769230769230766,
      "grad_norm": 1.3255727291107178,
      "learning_rate": 5e-05,
      "loss": 1.2876,
      "step": 938
    },
    {
      "epoch": 2.57967032967033,
      "grad_norm": 1.3218961954116821,
      "learning_rate": 5e-05,
      "loss": 1.3584,
      "step": 939
    },
    {
      "epoch": 2.5824175824175826,
      "grad_norm": 1.3278576135635376,
      "learning_rate": 5e-05,
      "loss": 1.3198,
      "step": 940
    },
    {
      "epoch": 2.5851648351648353,
      "grad_norm": 1.3314262628555298,
      "learning_rate": 5e-05,
      "loss": 1.3281,
      "step": 941
    },
    {
      "epoch": 2.587912087912088,
      "grad_norm": 1.2845242023468018,
      "learning_rate": 5e-05,
      "loss": 1.3843,
      "step": 942
    },
    {
      "epoch": 2.590659340659341,
      "grad_norm": 1.267682671546936,
      "learning_rate": 5e-05,
      "loss": 1.3037,
      "step": 943
    },
    {
      "epoch": 2.5934065934065935,
      "grad_norm": 1.2788149118423462,
      "learning_rate": 5e-05,
      "loss": 1.3256,
      "step": 944
    },
    {
      "epoch": 2.5961538461538463,
      "grad_norm": 1.2968981266021729,
      "learning_rate": 5e-05,
      "loss": 1.2841,
      "step": 945
    },
    {
      "epoch": 2.598901098901099,
      "grad_norm": 1.2574032545089722,
      "learning_rate": 5e-05,
      "loss": 1.2627,
      "step": 946
    },
    {
      "epoch": 2.6016483516483517,
      "grad_norm": 1.2793904542922974,
      "learning_rate": 5e-05,
      "loss": 1.2133,
      "step": 947
    },
    {
      "epoch": 2.6043956043956045,
      "grad_norm": 1.2982558012008667,
      "learning_rate": 5e-05,
      "loss": 1.3409,
      "step": 948
    },
    {
      "epoch": 2.607142857142857,
      "grad_norm": 1.340785264968872,
      "learning_rate": 5e-05,
      "loss": 1.3755,
      "step": 949
    },
    {
      "epoch": 2.60989010989011,
      "grad_norm": 1.289219617843628,
      "learning_rate": 5e-05,
      "loss": 1.3325,
      "step": 950
    },
    {
      "epoch": 2.6126373626373627,
      "grad_norm": 1.317821979522705,
      "learning_rate": 5e-05,
      "loss": 1.3699,
      "step": 951
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 1.327810525894165,
      "learning_rate": 5e-05,
      "loss": 1.368,
      "step": 952
    },
    {
      "epoch": 2.618131868131868,
      "grad_norm": 1.3306699991226196,
      "learning_rate": 5e-05,
      "loss": 1.3382,
      "step": 953
    },
    {
      "epoch": 2.620879120879121,
      "grad_norm": 1.354367971420288,
      "learning_rate": 5e-05,
      "loss": 1.3699,
      "step": 954
    },
    {
      "epoch": 2.6236263736263736,
      "grad_norm": 1.3068525791168213,
      "learning_rate": 5e-05,
      "loss": 1.3557,
      "step": 955
    },
    {
      "epoch": 2.6263736263736264,
      "grad_norm": 1.300050973892212,
      "learning_rate": 5e-05,
      "loss": 1.3068,
      "step": 956
    },
    {
      "epoch": 2.629120879120879,
      "grad_norm": 1.3127416372299194,
      "learning_rate": 5e-05,
      "loss": 1.2667,
      "step": 957
    },
    {
      "epoch": 2.631868131868132,
      "grad_norm": 1.2938507795333862,
      "learning_rate": 5e-05,
      "loss": 1.3126,
      "step": 958
    },
    {
      "epoch": 2.6346153846153846,
      "grad_norm": 1.2858465909957886,
      "learning_rate": 5e-05,
      "loss": 1.2788,
      "step": 959
    },
    {
      "epoch": 2.6373626373626373,
      "grad_norm": 1.3300515413284302,
      "learning_rate": 5e-05,
      "loss": 1.2743,
      "step": 960
    },
    {
      "epoch": 2.64010989010989,
      "grad_norm": 1.2818535566329956,
      "learning_rate": 5e-05,
      "loss": 1.2949,
      "step": 961
    },
    {
      "epoch": 2.642857142857143,
      "grad_norm": 1.307767391204834,
      "learning_rate": 5e-05,
      "loss": 1.3294,
      "step": 962
    },
    {
      "epoch": 2.6456043956043955,
      "grad_norm": 1.3161571025848389,
      "learning_rate": 5e-05,
      "loss": 1.3901,
      "step": 963
    },
    {
      "epoch": 2.6483516483516483,
      "grad_norm": 1.2998358011245728,
      "learning_rate": 5e-05,
      "loss": 1.3086,
      "step": 964
    },
    {
      "epoch": 2.651098901098901,
      "grad_norm": 1.3512895107269287,
      "learning_rate": 5e-05,
      "loss": 1.3275,
      "step": 965
    },
    {
      "epoch": 2.6538461538461537,
      "grad_norm": 1.2898184061050415,
      "learning_rate": 5e-05,
      "loss": 1.2687,
      "step": 966
    },
    {
      "epoch": 2.6565934065934065,
      "grad_norm": 1.3111636638641357,
      "learning_rate": 5e-05,
      "loss": 1.3209,
      "step": 967
    },
    {
      "epoch": 2.659340659340659,
      "grad_norm": 1.3043686151504517,
      "learning_rate": 5e-05,
      "loss": 1.3302,
      "step": 968
    },
    {
      "epoch": 2.662087912087912,
      "grad_norm": 1.3546665906906128,
      "learning_rate": 5e-05,
      "loss": 1.3446,
      "step": 969
    },
    {
      "epoch": 2.6648351648351647,
      "grad_norm": 1.3262605667114258,
      "learning_rate": 5e-05,
      "loss": 1.3682,
      "step": 970
    },
    {
      "epoch": 2.6675824175824174,
      "grad_norm": 1.3188836574554443,
      "learning_rate": 5e-05,
      "loss": 1.2374,
      "step": 971
    },
    {
      "epoch": 2.67032967032967,
      "grad_norm": 1.3403621912002563,
      "learning_rate": 5e-05,
      "loss": 1.3934,
      "step": 972
    },
    {
      "epoch": 2.6730769230769234,
      "grad_norm": 1.3457517623901367,
      "learning_rate": 5e-05,
      "loss": 1.3723,
      "step": 973
    },
    {
      "epoch": 2.675824175824176,
      "grad_norm": 1.2894057035446167,
      "learning_rate": 5e-05,
      "loss": 1.3034,
      "step": 974
    },
    {
      "epoch": 2.678571428571429,
      "grad_norm": 1.2993601560592651,
      "learning_rate": 5e-05,
      "loss": 1.3451,
      "step": 975
    },
    {
      "epoch": 2.6813186813186816,
      "grad_norm": 1.277099370956421,
      "learning_rate": 5e-05,
      "loss": 1.3383,
      "step": 976
    },
    {
      "epoch": 2.6840659340659343,
      "grad_norm": 1.2869025468826294,
      "learning_rate": 5e-05,
      "loss": 1.3012,
      "step": 977
    },
    {
      "epoch": 2.686813186813187,
      "grad_norm": 1.3096164464950562,
      "learning_rate": 5e-05,
      "loss": 1.2676,
      "step": 978
    },
    {
      "epoch": 2.6895604395604398,
      "grad_norm": 1.327897310256958,
      "learning_rate": 5e-05,
      "loss": 1.3048,
      "step": 979
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 1.281611442565918,
      "learning_rate": 5e-05,
      "loss": 1.2959,
      "step": 980
    },
    {
      "epoch": 2.6950549450549453,
      "grad_norm": 1.366746187210083,
      "learning_rate": 5e-05,
      "loss": 1.327,
      "step": 981
    },
    {
      "epoch": 2.697802197802198,
      "grad_norm": 1.331482172012329,
      "learning_rate": 5e-05,
      "loss": 1.317,
      "step": 982
    },
    {
      "epoch": 2.7005494505494507,
      "grad_norm": 1.2892342805862427,
      "learning_rate": 5e-05,
      "loss": 1.3136,
      "step": 983
    },
    {
      "epoch": 2.7032967032967035,
      "grad_norm": 1.3533103466033936,
      "learning_rate": 5e-05,
      "loss": 1.3782,
      "step": 984
    },
    {
      "epoch": 2.706043956043956,
      "grad_norm": 1.2781933546066284,
      "learning_rate": 5e-05,
      "loss": 1.2863,
      "step": 985
    },
    {
      "epoch": 2.708791208791209,
      "grad_norm": 1.3047215938568115,
      "learning_rate": 5e-05,
      "loss": 1.3359,
      "step": 986
    },
    {
      "epoch": 2.7115384615384617,
      "grad_norm": 1.3423686027526855,
      "learning_rate": 5e-05,
      "loss": 1.3712,
      "step": 987
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 1.3208202123641968,
      "learning_rate": 5e-05,
      "loss": 1.3133,
      "step": 988
    },
    {
      "epoch": 2.717032967032967,
      "grad_norm": 1.3026849031448364,
      "learning_rate": 5e-05,
      "loss": 1.2867,
      "step": 989
    },
    {
      "epoch": 2.71978021978022,
      "grad_norm": 1.346487283706665,
      "learning_rate": 5e-05,
      "loss": 1.2541,
      "step": 990
    },
    {
      "epoch": 2.7225274725274726,
      "grad_norm": 1.3595988750457764,
      "learning_rate": 5e-05,
      "loss": 1.2941,
      "step": 991
    },
    {
      "epoch": 2.7252747252747254,
      "grad_norm": 1.3320096731185913,
      "learning_rate": 5e-05,
      "loss": 1.3272,
      "step": 992
    },
    {
      "epoch": 2.728021978021978,
      "grad_norm": 1.3053120374679565,
      "learning_rate": 5e-05,
      "loss": 1.3225,
      "step": 993
    },
    {
      "epoch": 2.730769230769231,
      "grad_norm": 1.2715691328048706,
      "learning_rate": 5e-05,
      "loss": 1.2599,
      "step": 994
    },
    {
      "epoch": 2.7335164835164836,
      "grad_norm": 1.2845743894577026,
      "learning_rate": 5e-05,
      "loss": 1.2853,
      "step": 995
    },
    {
      "epoch": 2.7362637362637363,
      "grad_norm": 1.3240925073623657,
      "learning_rate": 5e-05,
      "loss": 1.291,
      "step": 996
    },
    {
      "epoch": 2.739010989010989,
      "grad_norm": 1.3105813264846802,
      "learning_rate": 5e-05,
      "loss": 1.295,
      "step": 997
    },
    {
      "epoch": 2.741758241758242,
      "grad_norm": 1.3865883350372314,
      "learning_rate": 5e-05,
      "loss": 1.3551,
      "step": 998
    },
    {
      "epoch": 2.7445054945054945,
      "grad_norm": 1.2928346395492554,
      "learning_rate": 5e-05,
      "loss": 1.3018,
      "step": 999
    },
    {
      "epoch": 2.7472527472527473,
      "grad_norm": 1.333199143409729,
      "learning_rate": 5e-05,
      "loss": 1.2557,
      "step": 1000
    },
    {
      "epoch": 2.75,
      "grad_norm": 1.28989577293396,
      "learning_rate": 5e-05,
      "loss": 1.276,
      "step": 1001
    },
    {
      "epoch": 2.7527472527472527,
      "grad_norm": 1.2505173683166504,
      "learning_rate": 5e-05,
      "loss": 1.2476,
      "step": 1002
    },
    {
      "epoch": 2.7554945054945055,
      "grad_norm": 1.3702027797698975,
      "learning_rate": 5e-05,
      "loss": 1.3352,
      "step": 1003
    },
    {
      "epoch": 2.758241758241758,
      "grad_norm": 1.3533083200454712,
      "learning_rate": 5e-05,
      "loss": 1.3204,
      "step": 1004
    },
    {
      "epoch": 2.760989010989011,
      "grad_norm": 1.3342344760894775,
      "learning_rate": 5e-05,
      "loss": 1.3041,
      "step": 1005
    },
    {
      "epoch": 2.7637362637362637,
      "grad_norm": 1.2983263731002808,
      "learning_rate": 5e-05,
      "loss": 1.3248,
      "step": 1006
    },
    {
      "epoch": 2.7664835164835164,
      "grad_norm": 1.3139055967330933,
      "learning_rate": 5e-05,
      "loss": 1.3486,
      "step": 1007
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 1.3485677242279053,
      "learning_rate": 5e-05,
      "loss": 1.3244,
      "step": 1008
    },
    {
      "epoch": 2.771978021978022,
      "grad_norm": 1.911234974861145,
      "learning_rate": 5e-05,
      "loss": 1.2591,
      "step": 1009
    },
    {
      "epoch": 2.7747252747252746,
      "grad_norm": 1.3302513360977173,
      "learning_rate": 5e-05,
      "loss": 1.3061,
      "step": 1010
    },
    {
      "epoch": 2.7774725274725274,
      "grad_norm": 1.315368890762329,
      "learning_rate": 5e-05,
      "loss": 1.26,
      "step": 1011
    },
    {
      "epoch": 2.78021978021978,
      "grad_norm": 1.2961121797561646,
      "learning_rate": 5e-05,
      "loss": 1.2744,
      "step": 1012
    },
    {
      "epoch": 2.782967032967033,
      "grad_norm": 1.3004957437515259,
      "learning_rate": 5e-05,
      "loss": 1.2749,
      "step": 1013
    },
    {
      "epoch": 2.7857142857142856,
      "grad_norm": 1.3135217428207397,
      "learning_rate": 5e-05,
      "loss": 1.3008,
      "step": 1014
    },
    {
      "epoch": 2.7884615384615383,
      "grad_norm": 1.2832911014556885,
      "learning_rate": 5e-05,
      "loss": 1.2585,
      "step": 1015
    },
    {
      "epoch": 2.791208791208791,
      "grad_norm": 1.3425335884094238,
      "learning_rate": 5e-05,
      "loss": 1.2563,
      "step": 1016
    },
    {
      "epoch": 2.793956043956044,
      "grad_norm": 1.3344250917434692,
      "learning_rate": 5e-05,
      "loss": 1.2753,
      "step": 1017
    },
    {
      "epoch": 2.7967032967032965,
      "grad_norm": 1.3028833866119385,
      "learning_rate": 5e-05,
      "loss": 1.2625,
      "step": 1018
    },
    {
      "epoch": 2.7994505494505493,
      "grad_norm": 1.322034239768982,
      "learning_rate": 5e-05,
      "loss": 1.2538,
      "step": 1019
    },
    {
      "epoch": 2.802197802197802,
      "grad_norm": 1.286875605583191,
      "learning_rate": 5e-05,
      "loss": 1.2478,
      "step": 1020
    },
    {
      "epoch": 2.8049450549450547,
      "grad_norm": 1.2989308834075928,
      "learning_rate": 5e-05,
      "loss": 1.2982,
      "step": 1021
    },
    {
      "epoch": 2.8076923076923075,
      "grad_norm": 1.3281742334365845,
      "learning_rate": 5e-05,
      "loss": 1.3222,
      "step": 1022
    },
    {
      "epoch": 2.8104395604395602,
      "grad_norm": 1.3146170377731323,
      "learning_rate": 5e-05,
      "loss": 1.3078,
      "step": 1023
    },
    {
      "epoch": 2.813186813186813,
      "grad_norm": 1.351733684539795,
      "learning_rate": 5e-05,
      "loss": 1.3278,
      "step": 1024
    },
    {
      "epoch": 2.8159340659340657,
      "grad_norm": 1.3094154596328735,
      "learning_rate": 5e-05,
      "loss": 1.2756,
      "step": 1025
    },
    {
      "epoch": 2.8186813186813184,
      "grad_norm": 1.2628586292266846,
      "learning_rate": 5e-05,
      "loss": 1.3064,
      "step": 1026
    },
    {
      "epoch": 2.821428571428571,
      "grad_norm": 1.2507685422897339,
      "learning_rate": 5e-05,
      "loss": 1.2447,
      "step": 1027
    },
    {
      "epoch": 2.824175824175824,
      "grad_norm": 1.3060449361801147,
      "learning_rate": 5e-05,
      "loss": 1.3215,
      "step": 1028
    },
    {
      "epoch": 2.8269230769230766,
      "grad_norm": 1.3305611610412598,
      "learning_rate": 5e-05,
      "loss": 1.3367,
      "step": 1029
    },
    {
      "epoch": 2.82967032967033,
      "grad_norm": 1.2869383096694946,
      "learning_rate": 5e-05,
      "loss": 1.2376,
      "step": 1030
    },
    {
      "epoch": 2.8324175824175826,
      "grad_norm": 1.31687593460083,
      "learning_rate": 5e-05,
      "loss": 1.3546,
      "step": 1031
    },
    {
      "epoch": 2.8351648351648353,
      "grad_norm": 1.276097297668457,
      "learning_rate": 5e-05,
      "loss": 1.2821,
      "step": 1032
    },
    {
      "epoch": 2.837912087912088,
      "grad_norm": 1.3217034339904785,
      "learning_rate": 5e-05,
      "loss": 1.3194,
      "step": 1033
    },
    {
      "epoch": 2.840659340659341,
      "grad_norm": 1.3196747303009033,
      "learning_rate": 5e-05,
      "loss": 1.3745,
      "step": 1034
    },
    {
      "epoch": 2.8434065934065935,
      "grad_norm": 1.2867738008499146,
      "learning_rate": 5e-05,
      "loss": 1.2495,
      "step": 1035
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 1.2907651662826538,
      "learning_rate": 5e-05,
      "loss": 1.2681,
      "step": 1036
    },
    {
      "epoch": 2.848901098901099,
      "grad_norm": 1.3339364528656006,
      "learning_rate": 5e-05,
      "loss": 1.3793,
      "step": 1037
    },
    {
      "epoch": 2.8516483516483517,
      "grad_norm": 1.277271032333374,
      "learning_rate": 5e-05,
      "loss": 1.2396,
      "step": 1038
    },
    {
      "epoch": 2.8543956043956045,
      "grad_norm": 1.3342645168304443,
      "learning_rate": 5e-05,
      "loss": 1.3382,
      "step": 1039
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 1.2702949047088623,
      "learning_rate": 5e-05,
      "loss": 1.2333,
      "step": 1040
    },
    {
      "epoch": 2.85989010989011,
      "grad_norm": 1.2755382061004639,
      "learning_rate": 5e-05,
      "loss": 1.2288,
      "step": 1041
    },
    {
      "epoch": 2.8626373626373627,
      "grad_norm": 1.3327306509017944,
      "learning_rate": 5e-05,
      "loss": 1.2542,
      "step": 1042
    },
    {
      "epoch": 2.8653846153846154,
      "grad_norm": 1.2966320514678955,
      "learning_rate": 5e-05,
      "loss": 1.2974,
      "step": 1043
    },
    {
      "epoch": 2.868131868131868,
      "grad_norm": 1.328001618385315,
      "learning_rate": 5e-05,
      "loss": 1.3309,
      "step": 1044
    },
    {
      "epoch": 2.870879120879121,
      "grad_norm": 1.3645116090774536,
      "learning_rate": 5e-05,
      "loss": 1.2324,
      "step": 1045
    },
    {
      "epoch": 2.8736263736263736,
      "grad_norm": 1.3100640773773193,
      "learning_rate": 5e-05,
      "loss": 1.3151,
      "step": 1046
    },
    {
      "epoch": 2.8763736263736264,
      "grad_norm": 1.3242069482803345,
      "learning_rate": 5e-05,
      "loss": 1.2796,
      "step": 1047
    },
    {
      "epoch": 2.879120879120879,
      "grad_norm": 1.2891169786453247,
      "learning_rate": 5e-05,
      "loss": 1.244,
      "step": 1048
    },
    {
      "epoch": 2.881868131868132,
      "grad_norm": 1.28204345703125,
      "learning_rate": 5e-05,
      "loss": 1.2651,
      "step": 1049
    },
    {
      "epoch": 2.8846153846153846,
      "grad_norm": 1.325221061706543,
      "learning_rate": 5e-05,
      "loss": 1.3608,
      "step": 1050
    },
    {
      "epoch": 2.8873626373626373,
      "grad_norm": 1.3032898902893066,
      "learning_rate": 5e-05,
      "loss": 1.3076,
      "step": 1051
    },
    {
      "epoch": 2.89010989010989,
      "grad_norm": 1.3289867639541626,
      "learning_rate": 5e-05,
      "loss": 1.3022,
      "step": 1052
    },
    {
      "epoch": 2.892857142857143,
      "grad_norm": 1.3007735013961792,
      "learning_rate": 5e-05,
      "loss": 1.2903,
      "step": 1053
    },
    {
      "epoch": 2.8956043956043955,
      "grad_norm": 1.279517650604248,
      "learning_rate": 5e-05,
      "loss": 1.2381,
      "step": 1054
    },
    {
      "epoch": 2.8983516483516483,
      "grad_norm": 1.2976418733596802,
      "learning_rate": 5e-05,
      "loss": 1.2985,
      "step": 1055
    },
    {
      "epoch": 2.901098901098901,
      "grad_norm": 1.2916606664657593,
      "learning_rate": 5e-05,
      "loss": 1.2976,
      "step": 1056
    },
    {
      "epoch": 2.9038461538461537,
      "grad_norm": 1.3255568742752075,
      "learning_rate": 5e-05,
      "loss": 1.3054,
      "step": 1057
    },
    {
      "epoch": 2.9065934065934065,
      "grad_norm": 1.2852636575698853,
      "learning_rate": 5e-05,
      "loss": 1.2595,
      "step": 1058
    },
    {
      "epoch": 2.909340659340659,
      "grad_norm": 1.2877118587493896,
      "learning_rate": 5e-05,
      "loss": 1.2577,
      "step": 1059
    },
    {
      "epoch": 2.912087912087912,
      "grad_norm": 1.313249111175537,
      "learning_rate": 5e-05,
      "loss": 1.2711,
      "step": 1060
    },
    {
      "epoch": 2.9148351648351647,
      "grad_norm": 1.270853877067566,
      "learning_rate": 5e-05,
      "loss": 1.2059,
      "step": 1061
    },
    {
      "epoch": 2.9175824175824174,
      "grad_norm": 1.3220211267471313,
      "learning_rate": 5e-05,
      "loss": 1.2592,
      "step": 1062
    },
    {
      "epoch": 2.92032967032967,
      "grad_norm": 1.3385343551635742,
      "learning_rate": 5e-05,
      "loss": 1.2559,
      "step": 1063
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 1.282649040222168,
      "learning_rate": 5e-05,
      "loss": 1.2771,
      "step": 1064
    },
    {
      "epoch": 2.925824175824176,
      "grad_norm": 1.2957239151000977,
      "learning_rate": 5e-05,
      "loss": 1.2897,
      "step": 1065
    },
    {
      "epoch": 2.928571428571429,
      "grad_norm": 1.2919037342071533,
      "learning_rate": 5e-05,
      "loss": 1.2453,
      "step": 1066
    },
    {
      "epoch": 2.9313186813186816,
      "grad_norm": 1.3344125747680664,
      "learning_rate": 5e-05,
      "loss": 1.2185,
      "step": 1067
    },
    {
      "epoch": 2.9340659340659343,
      "grad_norm": 1.3759443759918213,
      "learning_rate": 5e-05,
      "loss": 1.3376,
      "step": 1068
    },
    {
      "epoch": 2.936813186813187,
      "grad_norm": 1.329750895500183,
      "learning_rate": 5e-05,
      "loss": 1.2984,
      "step": 1069
    },
    {
      "epoch": 2.9395604395604398,
      "grad_norm": 1.3114286661148071,
      "learning_rate": 5e-05,
      "loss": 1.234,
      "step": 1070
    },
    {
      "epoch": 2.9423076923076925,
      "grad_norm": 1.2878409624099731,
      "learning_rate": 5e-05,
      "loss": 1.2651,
      "step": 1071
    },
    {
      "epoch": 2.9450549450549453,
      "grad_norm": 1.3237463235855103,
      "learning_rate": 5e-05,
      "loss": 1.2613,
      "step": 1072
    },
    {
      "epoch": 2.947802197802198,
      "grad_norm": 1.2871898412704468,
      "learning_rate": 5e-05,
      "loss": 1.3442,
      "step": 1073
    },
    {
      "epoch": 2.9505494505494507,
      "grad_norm": 1.2851760387420654,
      "learning_rate": 5e-05,
      "loss": 1.2294,
      "step": 1074
    },
    {
      "epoch": 2.9532967032967035,
      "grad_norm": 1.2837162017822266,
      "learning_rate": 5e-05,
      "loss": 1.2436,
      "step": 1075
    },
    {
      "epoch": 2.956043956043956,
      "grad_norm": 1.2839308977127075,
      "learning_rate": 5e-05,
      "loss": 1.2869,
      "step": 1076
    },
    {
      "epoch": 2.958791208791209,
      "grad_norm": 1.2804114818572998,
      "learning_rate": 5e-05,
      "loss": 1.2818,
      "step": 1077
    },
    {
      "epoch": 2.9615384615384617,
      "grad_norm": 1.3658826351165771,
      "learning_rate": 5e-05,
      "loss": 1.2789,
      "step": 1078
    },
    {
      "epoch": 2.9642857142857144,
      "grad_norm": 1.2973301410675049,
      "learning_rate": 5e-05,
      "loss": 1.3391,
      "step": 1079
    },
    {
      "epoch": 2.967032967032967,
      "grad_norm": 1.2915842533111572,
      "learning_rate": 5e-05,
      "loss": 1.2471,
      "step": 1080
    },
    {
      "epoch": 2.96978021978022,
      "grad_norm": 1.3204467296600342,
      "learning_rate": 5e-05,
      "loss": 1.2649,
      "step": 1081
    },
    {
      "epoch": 2.9725274725274726,
      "grad_norm": 1.3138540983200073,
      "learning_rate": 5e-05,
      "loss": 1.3478,
      "step": 1082
    },
    {
      "epoch": 2.9752747252747254,
      "grad_norm": 1.346406102180481,
      "learning_rate": 5e-05,
      "loss": 1.2203,
      "step": 1083
    },
    {
      "epoch": 2.978021978021978,
      "grad_norm": 1.2956947088241577,
      "learning_rate": 5e-05,
      "loss": 1.2726,
      "step": 1084
    },
    {
      "epoch": 2.980769230769231,
      "grad_norm": 1.28515625,
      "learning_rate": 5e-05,
      "loss": 1.2906,
      "step": 1085
    },
    {
      "epoch": 2.9835164835164836,
      "grad_norm": 1.295153260231018,
      "learning_rate": 5e-05,
      "loss": 1.1659,
      "step": 1086
    },
    {
      "epoch": 2.9862637362637363,
      "grad_norm": 1.3729740381240845,
      "learning_rate": 5e-05,
      "loss": 1.3273,
      "step": 1087
    },
    {
      "epoch": 2.989010989010989,
      "grad_norm": 1.3615696430206299,
      "learning_rate": 5e-05,
      "loss": 1.3891,
      "step": 1088
    },
    {
      "epoch": 2.991758241758242,
      "grad_norm": 1.3386424779891968,
      "learning_rate": 5e-05,
      "loss": 1.2692,
      "step": 1089
    },
    {
      "epoch": 2.9945054945054945,
      "grad_norm": 1.323781132698059,
      "learning_rate": 5e-05,
      "loss": 1.1955,
      "step": 1090
    },
    {
      "epoch": 2.9972527472527473,
      "grad_norm": 1.3018701076507568,
      "learning_rate": 5e-05,
      "loss": 1.2718,
      "step": 1091
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.310959815979004,
      "learning_rate": 5e-05,
      "loss": 1.2996,
      "step": 1092
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.8338487148284912,
      "eval_runtime": 81.1564,
      "eval_samples_per_second": 510.188,
      "eval_steps_per_second": 3.992,
      "step": 1092
    },
    {
      "epoch": 3.0027472527472527,
      "grad_norm": 1.2747812271118164,
      "learning_rate": 5e-05,
      "loss": 1.2083,
      "step": 1093
    },
    {
      "epoch": 3.0054945054945055,
      "grad_norm": 1.277924656867981,
      "learning_rate": 5e-05,
      "loss": 1.1892,
      "step": 1094
    },
    {
      "epoch": 3.008241758241758,
      "grad_norm": 1.2671196460723877,
      "learning_rate": 5e-05,
      "loss": 1.1441,
      "step": 1095
    },
    {
      "epoch": 3.010989010989011,
      "grad_norm": 1.2531250715255737,
      "learning_rate": 5e-05,
      "loss": 1.2147,
      "step": 1096
    },
    {
      "epoch": 3.0137362637362637,
      "grad_norm": 1.321777105331421,
      "learning_rate": 5e-05,
      "loss": 1.2271,
      "step": 1097
    },
    {
      "epoch": 3.0164835164835164,
      "grad_norm": 1.278085708618164,
      "learning_rate": 5e-05,
      "loss": 1.2235,
      "step": 1098
    },
    {
      "epoch": 3.019230769230769,
      "grad_norm": 1.2603925466537476,
      "learning_rate": 5e-05,
      "loss": 1.1592,
      "step": 1099
    },
    {
      "epoch": 3.021978021978022,
      "grad_norm": 1.274228572845459,
      "learning_rate": 5e-05,
      "loss": 1.2173,
      "step": 1100
    },
    {
      "epoch": 3.0247252747252746,
      "grad_norm": 1.254544734954834,
      "learning_rate": 5e-05,
      "loss": 1.1446,
      "step": 1101
    },
    {
      "epoch": 3.0274725274725274,
      "grad_norm": 1.3107653856277466,
      "learning_rate": 5e-05,
      "loss": 1.2305,
      "step": 1102
    },
    {
      "epoch": 3.03021978021978,
      "grad_norm": 1.2946927547454834,
      "learning_rate": 5e-05,
      "loss": 1.2393,
      "step": 1103
    },
    {
      "epoch": 3.032967032967033,
      "grad_norm": 1.2713027000427246,
      "learning_rate": 5e-05,
      "loss": 1.1879,
      "step": 1104
    },
    {
      "epoch": 3.0357142857142856,
      "grad_norm": 1.311596155166626,
      "learning_rate": 5e-05,
      "loss": 1.222,
      "step": 1105
    },
    {
      "epoch": 3.0384615384615383,
      "grad_norm": 1.2838093042373657,
      "learning_rate": 5e-05,
      "loss": 1.2251,
      "step": 1106
    },
    {
      "epoch": 3.041208791208791,
      "grad_norm": 1.2601344585418701,
      "learning_rate": 5e-05,
      "loss": 1.182,
      "step": 1107
    },
    {
      "epoch": 3.043956043956044,
      "grad_norm": 1.3106014728546143,
      "learning_rate": 5e-05,
      "loss": 1.1109,
      "step": 1108
    },
    {
      "epoch": 3.0467032967032965,
      "grad_norm": 1.3220715522766113,
      "learning_rate": 5e-05,
      "loss": 1.2306,
      "step": 1109
    },
    {
      "epoch": 3.0494505494505493,
      "grad_norm": 1.289973497390747,
      "learning_rate": 5e-05,
      "loss": 1.2351,
      "step": 1110
    },
    {
      "epoch": 3.052197802197802,
      "grad_norm": 1.297473669052124,
      "learning_rate": 5e-05,
      "loss": 1.2186,
      "step": 1111
    },
    {
      "epoch": 3.0549450549450547,
      "grad_norm": 1.2716035842895508,
      "learning_rate": 5e-05,
      "loss": 1.1613,
      "step": 1112
    },
    {
      "epoch": 3.0576923076923075,
      "grad_norm": 1.2899950742721558,
      "learning_rate": 5e-05,
      "loss": 1.1921,
      "step": 1113
    },
    {
      "epoch": 3.0604395604395602,
      "grad_norm": 1.29507315158844,
      "learning_rate": 5e-05,
      "loss": 1.1925,
      "step": 1114
    },
    {
      "epoch": 3.063186813186813,
      "grad_norm": 1.2852020263671875,
      "learning_rate": 5e-05,
      "loss": 1.2262,
      "step": 1115
    },
    {
      "epoch": 3.065934065934066,
      "grad_norm": 1.2699252367019653,
      "learning_rate": 5e-05,
      "loss": 1.2148,
      "step": 1116
    },
    {
      "epoch": 3.068681318681319,
      "grad_norm": 1.3688178062438965,
      "learning_rate": 5e-05,
      "loss": 1.1813,
      "step": 1117
    },
    {
      "epoch": 3.0714285714285716,
      "grad_norm": 1.310247540473938,
      "learning_rate": 5e-05,
      "loss": 1.2114,
      "step": 1118
    },
    {
      "epoch": 3.0741758241758244,
      "grad_norm": 1.2596937417984009,
      "learning_rate": 5e-05,
      "loss": 1.1428,
      "step": 1119
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 1.2923922538757324,
      "learning_rate": 5e-05,
      "loss": 1.2084,
      "step": 1120
    },
    {
      "epoch": 3.07967032967033,
      "grad_norm": 1.3170547485351562,
      "learning_rate": 5e-05,
      "loss": 1.185,
      "step": 1121
    },
    {
      "epoch": 3.0824175824175826,
      "grad_norm": 1.2677377462387085,
      "learning_rate": 5e-05,
      "loss": 1.1687,
      "step": 1122
    },
    {
      "epoch": 3.0851648351648353,
      "grad_norm": 1.2475471496582031,
      "learning_rate": 5e-05,
      "loss": 1.1524,
      "step": 1123
    },
    {
      "epoch": 3.087912087912088,
      "grad_norm": 1.277322769165039,
      "learning_rate": 5e-05,
      "loss": 1.1468,
      "step": 1124
    },
    {
      "epoch": 3.090659340659341,
      "grad_norm": 1.2547357082366943,
      "learning_rate": 5e-05,
      "loss": 1.1604,
      "step": 1125
    },
    {
      "epoch": 3.0934065934065935,
      "grad_norm": 1.2657666206359863,
      "learning_rate": 5e-05,
      "loss": 1.2415,
      "step": 1126
    },
    {
      "epoch": 3.0961538461538463,
      "grad_norm": 1.275014042854309,
      "learning_rate": 5e-05,
      "loss": 1.148,
      "step": 1127
    },
    {
      "epoch": 3.098901098901099,
      "grad_norm": 1.2905857563018799,
      "learning_rate": 5e-05,
      "loss": 1.2211,
      "step": 1128
    },
    {
      "epoch": 3.1016483516483517,
      "grad_norm": 1.26399564743042,
      "learning_rate": 5e-05,
      "loss": 1.2522,
      "step": 1129
    },
    {
      "epoch": 3.1043956043956045,
      "grad_norm": 1.2655881643295288,
      "learning_rate": 5e-05,
      "loss": 1.1266,
      "step": 1130
    },
    {
      "epoch": 3.107142857142857,
      "grad_norm": 1.3564083576202393,
      "learning_rate": 5e-05,
      "loss": 1.2647,
      "step": 1131
    },
    {
      "epoch": 3.10989010989011,
      "grad_norm": 1.3196064233779907,
      "learning_rate": 5e-05,
      "loss": 1.2366,
      "step": 1132
    },
    {
      "epoch": 3.1126373626373627,
      "grad_norm": 1.2895175218582153,
      "learning_rate": 5e-05,
      "loss": 1.1312,
      "step": 1133
    },
    {
      "epoch": 3.1153846153846154,
      "grad_norm": 1.29789400100708,
      "learning_rate": 5e-05,
      "loss": 1.1842,
      "step": 1134
    },
    {
      "epoch": 3.118131868131868,
      "grad_norm": 1.2956717014312744,
      "learning_rate": 5e-05,
      "loss": 1.1544,
      "step": 1135
    },
    {
      "epoch": 3.120879120879121,
      "grad_norm": 1.2662785053253174,
      "learning_rate": 5e-05,
      "loss": 1.2173,
      "step": 1136
    },
    {
      "epoch": 3.1236263736263736,
      "grad_norm": 1.3005709648132324,
      "learning_rate": 5e-05,
      "loss": 1.137,
      "step": 1137
    },
    {
      "epoch": 3.1263736263736264,
      "grad_norm": 1.2751426696777344,
      "learning_rate": 5e-05,
      "loss": 1.1557,
      "step": 1138
    },
    {
      "epoch": 3.129120879120879,
      "grad_norm": 1.2890766859054565,
      "learning_rate": 5e-05,
      "loss": 1.1478,
      "step": 1139
    },
    {
      "epoch": 3.131868131868132,
      "grad_norm": 1.2824190855026245,
      "learning_rate": 5e-05,
      "loss": 1.2677,
      "step": 1140
    },
    {
      "epoch": 3.1346153846153846,
      "grad_norm": 1.2945773601531982,
      "learning_rate": 5e-05,
      "loss": 1.2463,
      "step": 1141
    },
    {
      "epoch": 3.1373626373626373,
      "grad_norm": 1.2690989971160889,
      "learning_rate": 5e-05,
      "loss": 1.0982,
      "step": 1142
    },
    {
      "epoch": 3.14010989010989,
      "grad_norm": 1.3171087503433228,
      "learning_rate": 5e-05,
      "loss": 1.2313,
      "step": 1143
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 1.2625532150268555,
      "learning_rate": 5e-05,
      "loss": 1.1723,
      "step": 1144
    },
    {
      "epoch": 3.1456043956043955,
      "grad_norm": 1.2826721668243408,
      "learning_rate": 5e-05,
      "loss": 1.1435,
      "step": 1145
    },
    {
      "epoch": 3.1483516483516483,
      "grad_norm": 1.2701056003570557,
      "learning_rate": 5e-05,
      "loss": 1.0931,
      "step": 1146
    },
    {
      "epoch": 3.151098901098901,
      "grad_norm": 1.2451130151748657,
      "learning_rate": 5e-05,
      "loss": 1.1107,
      "step": 1147
    },
    {
      "epoch": 3.1538461538461537,
      "grad_norm": 1.2882647514343262,
      "learning_rate": 5e-05,
      "loss": 1.179,
      "step": 1148
    },
    {
      "epoch": 3.1565934065934065,
      "grad_norm": 1.2810580730438232,
      "learning_rate": 5e-05,
      "loss": 1.1557,
      "step": 1149
    },
    {
      "epoch": 3.159340659340659,
      "grad_norm": 1.2477208375930786,
      "learning_rate": 5e-05,
      "loss": 1.142,
      "step": 1150
    },
    {
      "epoch": 3.162087912087912,
      "grad_norm": 1.2431896924972534,
      "learning_rate": 5e-05,
      "loss": 1.1154,
      "step": 1151
    },
    {
      "epoch": 3.1648351648351647,
      "grad_norm": 1.2628960609436035,
      "learning_rate": 5e-05,
      "loss": 1.1749,
      "step": 1152
    },
    {
      "epoch": 3.1675824175824174,
      "grad_norm": 1.2902592420578003,
      "learning_rate": 5e-05,
      "loss": 1.1741,
      "step": 1153
    },
    {
      "epoch": 3.17032967032967,
      "grad_norm": 1.3205888271331787,
      "learning_rate": 5e-05,
      "loss": 1.2317,
      "step": 1154
    },
    {
      "epoch": 3.173076923076923,
      "grad_norm": 1.2807267904281616,
      "learning_rate": 5e-05,
      "loss": 1.1399,
      "step": 1155
    },
    {
      "epoch": 3.1758241758241756,
      "grad_norm": 1.2910901308059692,
      "learning_rate": 5e-05,
      "loss": 1.1551,
      "step": 1156
    },
    {
      "epoch": 3.1785714285714284,
      "grad_norm": 1.2493990659713745,
      "learning_rate": 5e-05,
      "loss": 1.1505,
      "step": 1157
    },
    {
      "epoch": 3.181318681318681,
      "grad_norm": 1.29677152633667,
      "learning_rate": 5e-05,
      "loss": 1.1578,
      "step": 1158
    },
    {
      "epoch": 3.1840659340659343,
      "grad_norm": 1.2931963205337524,
      "learning_rate": 5e-05,
      "loss": 1.2067,
      "step": 1159
    },
    {
      "epoch": 3.186813186813187,
      "grad_norm": 1.3341457843780518,
      "learning_rate": 5e-05,
      "loss": 1.2588,
      "step": 1160
    },
    {
      "epoch": 3.1895604395604398,
      "grad_norm": 1.278855800628662,
      "learning_rate": 5e-05,
      "loss": 1.2347,
      "step": 1161
    },
    {
      "epoch": 3.1923076923076925,
      "grad_norm": 1.3166824579238892,
      "learning_rate": 5e-05,
      "loss": 1.2098,
      "step": 1162
    },
    {
      "epoch": 3.1950549450549453,
      "grad_norm": 1.2885220050811768,
      "learning_rate": 5e-05,
      "loss": 1.1931,
      "step": 1163
    },
    {
      "epoch": 3.197802197802198,
      "grad_norm": 1.2620073556900024,
      "learning_rate": 5e-05,
      "loss": 1.1485,
      "step": 1164
    },
    {
      "epoch": 3.2005494505494507,
      "grad_norm": 1.2970151901245117,
      "learning_rate": 5e-05,
      "loss": 1.1496,
      "step": 1165
    },
    {
      "epoch": 3.2032967032967035,
      "grad_norm": 1.3196215629577637,
      "learning_rate": 5e-05,
      "loss": 1.2077,
      "step": 1166
    },
    {
      "epoch": 3.206043956043956,
      "grad_norm": 1.273790717124939,
      "learning_rate": 5e-05,
      "loss": 1.1455,
      "step": 1167
    },
    {
      "epoch": 3.208791208791209,
      "grad_norm": 1.2777457237243652,
      "learning_rate": 5e-05,
      "loss": 1.1153,
      "step": 1168
    },
    {
      "epoch": 3.2115384615384617,
      "grad_norm": 1.330108404159546,
      "learning_rate": 5e-05,
      "loss": 1.1801,
      "step": 1169
    },
    {
      "epoch": 3.2142857142857144,
      "grad_norm": 1.343876838684082,
      "learning_rate": 5e-05,
      "loss": 1.1895,
      "step": 1170
    },
    {
      "epoch": 3.217032967032967,
      "grad_norm": 1.3220006227493286,
      "learning_rate": 5e-05,
      "loss": 1.151,
      "step": 1171
    },
    {
      "epoch": 3.21978021978022,
      "grad_norm": 1.2742558717727661,
      "learning_rate": 5e-05,
      "loss": 1.1247,
      "step": 1172
    },
    {
      "epoch": 3.2225274725274726,
      "grad_norm": 1.3449522256851196,
      "learning_rate": 5e-05,
      "loss": 1.262,
      "step": 1173
    },
    {
      "epoch": 3.2252747252747254,
      "grad_norm": 1.265472173690796,
      "learning_rate": 5e-05,
      "loss": 1.1179,
      "step": 1174
    },
    {
      "epoch": 3.228021978021978,
      "grad_norm": 1.2873969078063965,
      "learning_rate": 5e-05,
      "loss": 1.1466,
      "step": 1175
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 1.3045661449432373,
      "learning_rate": 5e-05,
      "loss": 1.1923,
      "step": 1176
    },
    {
      "epoch": 3.2335164835164836,
      "grad_norm": 1.2800936698913574,
      "learning_rate": 5e-05,
      "loss": 1.199,
      "step": 1177
    },
    {
      "epoch": 3.2362637362637363,
      "grad_norm": 1.297263264656067,
      "learning_rate": 5e-05,
      "loss": 1.1394,
      "step": 1178
    },
    {
      "epoch": 3.239010989010989,
      "grad_norm": 1.2822692394256592,
      "learning_rate": 5e-05,
      "loss": 1.1432,
      "step": 1179
    },
    {
      "epoch": 3.241758241758242,
      "grad_norm": 1.3182764053344727,
      "learning_rate": 5e-05,
      "loss": 1.1829,
      "step": 1180
    },
    {
      "epoch": 3.2445054945054945,
      "grad_norm": 1.2679027318954468,
      "learning_rate": 5e-05,
      "loss": 1.1438,
      "step": 1181
    },
    {
      "epoch": 3.2472527472527473,
      "grad_norm": 1.2868367433547974,
      "learning_rate": 5e-05,
      "loss": 1.1787,
      "step": 1182
    },
    {
      "epoch": 3.25,
      "grad_norm": 1.2926243543624878,
      "learning_rate": 5e-05,
      "loss": 1.1752,
      "step": 1183
    },
    {
      "epoch": 3.2527472527472527,
      "grad_norm": 1.2812120914459229,
      "learning_rate": 5e-05,
      "loss": 1.1302,
      "step": 1184
    },
    {
      "epoch": 3.2554945054945055,
      "grad_norm": 1.2948085069656372,
      "learning_rate": 5e-05,
      "loss": 1.1822,
      "step": 1185
    },
    {
      "epoch": 3.258241758241758,
      "grad_norm": 1.3108303546905518,
      "learning_rate": 5e-05,
      "loss": 1.1863,
      "step": 1186
    },
    {
      "epoch": 3.260989010989011,
      "grad_norm": 1.2919063568115234,
      "learning_rate": 5e-05,
      "loss": 1.1432,
      "step": 1187
    },
    {
      "epoch": 3.2637362637362637,
      "grad_norm": 1.25138258934021,
      "learning_rate": 5e-05,
      "loss": 1.1544,
      "step": 1188
    },
    {
      "epoch": 3.2664835164835164,
      "grad_norm": 1.2851024866104126,
      "learning_rate": 5e-05,
      "loss": 1.2206,
      "step": 1189
    },
    {
      "epoch": 3.269230769230769,
      "grad_norm": 1.2755217552185059,
      "learning_rate": 5e-05,
      "loss": 1.1417,
      "step": 1190
    },
    {
      "epoch": 3.271978021978022,
      "grad_norm": 1.3040831089019775,
      "learning_rate": 5e-05,
      "loss": 1.1266,
      "step": 1191
    },
    {
      "epoch": 3.2747252747252746,
      "grad_norm": 1.2834256887435913,
      "learning_rate": 5e-05,
      "loss": 1.1702,
      "step": 1192
    },
    {
      "epoch": 3.2774725274725274,
      "grad_norm": 1.2722046375274658,
      "learning_rate": 5e-05,
      "loss": 1.1201,
      "step": 1193
    },
    {
      "epoch": 3.28021978021978,
      "grad_norm": 1.308569073677063,
      "learning_rate": 5e-05,
      "loss": 1.1712,
      "step": 1194
    },
    {
      "epoch": 3.282967032967033,
      "grad_norm": 1.3326174020767212,
      "learning_rate": 5e-05,
      "loss": 1.1669,
      "step": 1195
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 1.3270790576934814,
      "learning_rate": 5e-05,
      "loss": 1.176,
      "step": 1196
    },
    {
      "epoch": 3.2884615384615383,
      "grad_norm": 1.289947509765625,
      "learning_rate": 5e-05,
      "loss": 1.1367,
      "step": 1197
    },
    {
      "epoch": 3.291208791208791,
      "grad_norm": 1.2675786018371582,
      "learning_rate": 5e-05,
      "loss": 1.1096,
      "step": 1198
    },
    {
      "epoch": 3.293956043956044,
      "grad_norm": 1.2796183824539185,
      "learning_rate": 5e-05,
      "loss": 1.1542,
      "step": 1199
    },
    {
      "epoch": 3.2967032967032965,
      "grad_norm": 1.2770862579345703,
      "learning_rate": 5e-05,
      "loss": 1.1611,
      "step": 1200
    },
    {
      "epoch": 3.2994505494505493,
      "grad_norm": 1.3034782409667969,
      "learning_rate": 5e-05,
      "loss": 1.1204,
      "step": 1201
    },
    {
      "epoch": 3.302197802197802,
      "grad_norm": 1.3109970092773438,
      "learning_rate": 5e-05,
      "loss": 1.1887,
      "step": 1202
    },
    {
      "epoch": 3.3049450549450547,
      "grad_norm": 1.3005956411361694,
      "learning_rate": 5e-05,
      "loss": 1.1394,
      "step": 1203
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 1.3197731971740723,
      "learning_rate": 5e-05,
      "loss": 1.1471,
      "step": 1204
    },
    {
      "epoch": 3.3104395604395602,
      "grad_norm": 1.3250163793563843,
      "learning_rate": 5e-05,
      "loss": 1.152,
      "step": 1205
    },
    {
      "epoch": 3.313186813186813,
      "grad_norm": 1.2755998373031616,
      "learning_rate": 5e-05,
      "loss": 1.2226,
      "step": 1206
    },
    {
      "epoch": 3.3159340659340657,
      "grad_norm": 1.2709228992462158,
      "learning_rate": 5e-05,
      "loss": 1.1209,
      "step": 1207
    },
    {
      "epoch": 3.3186813186813184,
      "grad_norm": 1.2571543455123901,
      "learning_rate": 5e-05,
      "loss": 0.995,
      "step": 1208
    },
    {
      "epoch": 3.3214285714285716,
      "grad_norm": 1.316471815109253,
      "learning_rate": 5e-05,
      "loss": 1.1816,
      "step": 1209
    },
    {
      "epoch": 3.3241758241758244,
      "grad_norm": 1.2781932353973389,
      "learning_rate": 5e-05,
      "loss": 1.1341,
      "step": 1210
    },
    {
      "epoch": 3.326923076923077,
      "grad_norm": 1.340444564819336,
      "learning_rate": 5e-05,
      "loss": 1.1904,
      "step": 1211
    },
    {
      "epoch": 3.32967032967033,
      "grad_norm": 1.3199130296707153,
      "learning_rate": 5e-05,
      "loss": 1.2282,
      "step": 1212
    },
    {
      "epoch": 3.3324175824175826,
      "grad_norm": 1.3506948947906494,
      "learning_rate": 5e-05,
      "loss": 1.2107,
      "step": 1213
    },
    {
      "epoch": 3.3351648351648353,
      "grad_norm": 1.2933166027069092,
      "learning_rate": 5e-05,
      "loss": 1.1618,
      "step": 1214
    },
    {
      "epoch": 3.337912087912088,
      "grad_norm": 1.236792802810669,
      "learning_rate": 5e-05,
      "loss": 1.1259,
      "step": 1215
    },
    {
      "epoch": 3.340659340659341,
      "grad_norm": 1.2730251550674438,
      "learning_rate": 5e-05,
      "loss": 1.1266,
      "step": 1216
    },
    {
      "epoch": 3.3434065934065935,
      "grad_norm": 1.2627583742141724,
      "learning_rate": 5e-05,
      "loss": 1.1019,
      "step": 1217
    },
    {
      "epoch": 3.3461538461538463,
      "grad_norm": 1.301766276359558,
      "learning_rate": 5e-05,
      "loss": 1.2178,
      "step": 1218
    },
    {
      "epoch": 3.348901098901099,
      "grad_norm": 1.3315366506576538,
      "learning_rate": 5e-05,
      "loss": 1.1549,
      "step": 1219
    },
    {
      "epoch": 3.3516483516483517,
      "grad_norm": 1.2756495475769043,
      "learning_rate": 5e-05,
      "loss": 1.0933,
      "step": 1220
    },
    {
      "epoch": 3.3543956043956045,
      "grad_norm": 1.2695798873901367,
      "learning_rate": 5e-05,
      "loss": 1.0965,
      "step": 1221
    },
    {
      "epoch": 3.357142857142857,
      "grad_norm": 1.2656750679016113,
      "learning_rate": 5e-05,
      "loss": 1.1639,
      "step": 1222
    },
    {
      "epoch": 3.35989010989011,
      "grad_norm": 1.2644519805908203,
      "learning_rate": 5e-05,
      "loss": 1.1004,
      "step": 1223
    },
    {
      "epoch": 3.3626373626373627,
      "grad_norm": 1.288326621055603,
      "learning_rate": 5e-05,
      "loss": 1.154,
      "step": 1224
    },
    {
      "epoch": 3.3653846153846154,
      "grad_norm": 1.2358837127685547,
      "learning_rate": 5e-05,
      "loss": 1.1302,
      "step": 1225
    },
    {
      "epoch": 3.368131868131868,
      "grad_norm": 1.2607733011245728,
      "learning_rate": 5e-05,
      "loss": 1.1548,
      "step": 1226
    },
    {
      "epoch": 3.370879120879121,
      "grad_norm": 1.319496989250183,
      "learning_rate": 5e-05,
      "loss": 1.2581,
      "step": 1227
    },
    {
      "epoch": 3.3736263736263736,
      "grad_norm": 1.2467703819274902,
      "learning_rate": 5e-05,
      "loss": 1.1057,
      "step": 1228
    },
    {
      "epoch": 3.3763736263736264,
      "grad_norm": 1.2967664003372192,
      "learning_rate": 5e-05,
      "loss": 1.1577,
      "step": 1229
    },
    {
      "epoch": 3.379120879120879,
      "grad_norm": 1.309699535369873,
      "learning_rate": 5e-05,
      "loss": 1.1803,
      "step": 1230
    },
    {
      "epoch": 3.381868131868132,
      "grad_norm": 1.2733478546142578,
      "learning_rate": 5e-05,
      "loss": 1.0998,
      "step": 1231
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 1.2864704132080078,
      "learning_rate": 5e-05,
      "loss": 1.1678,
      "step": 1232
    },
    {
      "epoch": 3.3873626373626373,
      "grad_norm": 1.3059324026107788,
      "learning_rate": 5e-05,
      "loss": 1.2182,
      "step": 1233
    },
    {
      "epoch": 3.39010989010989,
      "grad_norm": 1.2922576665878296,
      "learning_rate": 5e-05,
      "loss": 1.1402,
      "step": 1234
    },
    {
      "epoch": 3.392857142857143,
      "grad_norm": 1.3234503269195557,
      "learning_rate": 5e-05,
      "loss": 1.1349,
      "step": 1235
    },
    {
      "epoch": 3.3956043956043955,
      "grad_norm": 1.2802733182907104,
      "learning_rate": 5e-05,
      "loss": 1.1906,
      "step": 1236
    },
    {
      "epoch": 3.3983516483516483,
      "grad_norm": 1.2880735397338867,
      "learning_rate": 5e-05,
      "loss": 1.1671,
      "step": 1237
    },
    {
      "epoch": 3.401098901098901,
      "grad_norm": 1.2959668636322021,
      "learning_rate": 5e-05,
      "loss": 1.1299,
      "step": 1238
    },
    {
      "epoch": 3.4038461538461537,
      "grad_norm": 1.3450685739517212,
      "learning_rate": 5e-05,
      "loss": 1.1777,
      "step": 1239
    },
    {
      "epoch": 3.4065934065934065,
      "grad_norm": 1.2840887308120728,
      "learning_rate": 5e-05,
      "loss": 1.1814,
      "step": 1240
    },
    {
      "epoch": 3.409340659340659,
      "grad_norm": 1.3164424896240234,
      "learning_rate": 5e-05,
      "loss": 1.1429,
      "step": 1241
    },
    {
      "epoch": 3.412087912087912,
      "grad_norm": 1.261170744895935,
      "learning_rate": 5e-05,
      "loss": 1.0571,
      "step": 1242
    },
    {
      "epoch": 3.4148351648351647,
      "grad_norm": 1.2799571752548218,
      "learning_rate": 5e-05,
      "loss": 1.1573,
      "step": 1243
    },
    {
      "epoch": 3.4175824175824174,
      "grad_norm": 1.2789555788040161,
      "learning_rate": 5e-05,
      "loss": 1.1061,
      "step": 1244
    },
    {
      "epoch": 3.42032967032967,
      "grad_norm": 1.3018913269042969,
      "learning_rate": 5e-05,
      "loss": 1.1506,
      "step": 1245
    },
    {
      "epoch": 3.423076923076923,
      "grad_norm": 1.2619514465332031,
      "learning_rate": 5e-05,
      "loss": 1.1136,
      "step": 1246
    },
    {
      "epoch": 3.4258241758241756,
      "grad_norm": 1.2708959579467773,
      "learning_rate": 5e-05,
      "loss": 1.1304,
      "step": 1247
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 1.3161700963974,
      "learning_rate": 5e-05,
      "loss": 1.1864,
      "step": 1248
    },
    {
      "epoch": 3.4313186813186816,
      "grad_norm": 1.3643795251846313,
      "learning_rate": 5e-05,
      "loss": 1.1948,
      "step": 1249
    },
    {
      "epoch": 3.4340659340659343,
      "grad_norm": 1.311071515083313,
      "learning_rate": 5e-05,
      "loss": 1.1088,
      "step": 1250
    },
    {
      "epoch": 3.436813186813187,
      "grad_norm": 1.301107406616211,
      "learning_rate": 5e-05,
      "loss": 1.1462,
      "step": 1251
    },
    {
      "epoch": 3.4395604395604398,
      "grad_norm": 1.3284848928451538,
      "learning_rate": 5e-05,
      "loss": 1.1385,
      "step": 1252
    },
    {
      "epoch": 3.4423076923076925,
      "grad_norm": 1.3165843486785889,
      "learning_rate": 5e-05,
      "loss": 1.1631,
      "step": 1253
    },
    {
      "epoch": 3.4450549450549453,
      "grad_norm": 1.265071153640747,
      "learning_rate": 5e-05,
      "loss": 1.1273,
      "step": 1254
    },
    {
      "epoch": 3.447802197802198,
      "grad_norm": 1.3400778770446777,
      "learning_rate": 5e-05,
      "loss": 1.1565,
      "step": 1255
    },
    {
      "epoch": 3.4505494505494507,
      "grad_norm": 1.317195177078247,
      "learning_rate": 5e-05,
      "loss": 1.1814,
      "step": 1256
    },
    {
      "epoch": 3.4532967032967035,
      "grad_norm": 1.3039153814315796,
      "learning_rate": 5e-05,
      "loss": 1.1218,
      "step": 1257
    },
    {
      "epoch": 3.456043956043956,
      "grad_norm": 1.35982084274292,
      "learning_rate": 5e-05,
      "loss": 1.191,
      "step": 1258
    },
    {
      "epoch": 3.458791208791209,
      "grad_norm": 1.2914097309112549,
      "learning_rate": 5e-05,
      "loss": 1.1892,
      "step": 1259
    },
    {
      "epoch": 3.4615384615384617,
      "grad_norm": 1.3404613733291626,
      "learning_rate": 5e-05,
      "loss": 1.1798,
      "step": 1260
    },
    {
      "epoch": 3.4642857142857144,
      "grad_norm": 1.2999179363250732,
      "learning_rate": 5e-05,
      "loss": 1.1663,
      "step": 1261
    },
    {
      "epoch": 3.467032967032967,
      "grad_norm": 1.3068445920944214,
      "learning_rate": 5e-05,
      "loss": 1.1516,
      "step": 1262
    },
    {
      "epoch": 3.46978021978022,
      "grad_norm": 1.3616610765457153,
      "learning_rate": 5e-05,
      "loss": 1.1371,
      "step": 1263
    },
    {
      "epoch": 3.4725274725274726,
      "grad_norm": 1.2822366952896118,
      "learning_rate": 5e-05,
      "loss": 1.1281,
      "step": 1264
    },
    {
      "epoch": 3.4752747252747254,
      "grad_norm": 1.3339178562164307,
      "learning_rate": 5e-05,
      "loss": 1.1496,
      "step": 1265
    },
    {
      "epoch": 3.478021978021978,
      "grad_norm": 1.2831860780715942,
      "learning_rate": 5e-05,
      "loss": 1.1376,
      "step": 1266
    },
    {
      "epoch": 3.480769230769231,
      "grad_norm": 1.2979246377944946,
      "learning_rate": 5e-05,
      "loss": 1.1284,
      "step": 1267
    },
    {
      "epoch": 3.4835164835164836,
      "grad_norm": 1.3035845756530762,
      "learning_rate": 5e-05,
      "loss": 1.1361,
      "step": 1268
    },
    {
      "epoch": 3.4862637362637363,
      "grad_norm": 1.285367488861084,
      "learning_rate": 5e-05,
      "loss": 1.1569,
      "step": 1269
    },
    {
      "epoch": 3.489010989010989,
      "grad_norm": 1.2509804964065552,
      "learning_rate": 5e-05,
      "loss": 1.0681,
      "step": 1270
    },
    {
      "epoch": 3.491758241758242,
      "grad_norm": 1.3191007375717163,
      "learning_rate": 5e-05,
      "loss": 1.2036,
      "step": 1271
    },
    {
      "epoch": 3.4945054945054945,
      "grad_norm": 1.2911632061004639,
      "learning_rate": 5e-05,
      "loss": 1.1307,
      "step": 1272
    },
    {
      "epoch": 3.4972527472527473,
      "grad_norm": 1.3058058023452759,
      "learning_rate": 5e-05,
      "loss": 1.0924,
      "step": 1273
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.3040300607681274,
      "learning_rate": 5e-05,
      "loss": 1.1344,
      "step": 1274
    },
    {
      "epoch": 3.5,
      "eval_loss": 1.8506718873977661,
      "eval_runtime": 100.4201,
      "eval_samples_per_second": 412.318,
      "eval_steps_per_second": 3.226,
      "step": 1274
    },
    {
      "epoch": 3.5027472527472527,
      "grad_norm": 1.274717926979065,
      "learning_rate": 5e-05,
      "loss": 1.1572,
      "step": 1275
    },
    {
      "epoch": 3.5054945054945055,
      "grad_norm": 1.295401930809021,
      "learning_rate": 5e-05,
      "loss": 1.1629,
      "step": 1276
    },
    {
      "epoch": 3.508241758241758,
      "grad_norm": 1.2776857614517212,
      "learning_rate": 5e-05,
      "loss": 1.0363,
      "step": 1277
    },
    {
      "epoch": 3.510989010989011,
      "grad_norm": 1.2660307884216309,
      "learning_rate": 5e-05,
      "loss": 1.0545,
      "step": 1278
    },
    {
      "epoch": 3.5137362637362637,
      "grad_norm": 1.3284912109375,
      "learning_rate": 5e-05,
      "loss": 1.1965,
      "step": 1279
    },
    {
      "epoch": 3.5164835164835164,
      "grad_norm": 1.2845810651779175,
      "learning_rate": 5e-05,
      "loss": 1.125,
      "step": 1280
    },
    {
      "epoch": 3.519230769230769,
      "grad_norm": 1.3025745153427124,
      "learning_rate": 5e-05,
      "loss": 1.1701,
      "step": 1281
    },
    {
      "epoch": 3.521978021978022,
      "grad_norm": 1.2834709882736206,
      "learning_rate": 5e-05,
      "loss": 1.1492,
      "step": 1282
    },
    {
      "epoch": 3.5247252747252746,
      "grad_norm": 1.3025463819503784,
      "learning_rate": 5e-05,
      "loss": 1.1389,
      "step": 1283
    },
    {
      "epoch": 3.5274725274725274,
      "grad_norm": 1.290366291999817,
      "learning_rate": 5e-05,
      "loss": 1.1063,
      "step": 1284
    },
    {
      "epoch": 3.53021978021978,
      "grad_norm": 1.2925188541412354,
      "learning_rate": 5e-05,
      "loss": 1.1169,
      "step": 1285
    },
    {
      "epoch": 3.532967032967033,
      "grad_norm": 1.309967279434204,
      "learning_rate": 5e-05,
      "loss": 1.1587,
      "step": 1286
    },
    {
      "epoch": 3.5357142857142856,
      "grad_norm": 1.3404520750045776,
      "learning_rate": 5e-05,
      "loss": 1.1034,
      "step": 1287
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 1.295522928237915,
      "learning_rate": 5e-05,
      "loss": 1.1122,
      "step": 1288
    },
    {
      "epoch": 3.541208791208791,
      "grad_norm": 1.2782334089279175,
      "learning_rate": 5e-05,
      "loss": 1.1347,
      "step": 1289
    },
    {
      "epoch": 3.543956043956044,
      "grad_norm": 1.3022785186767578,
      "learning_rate": 5e-05,
      "loss": 1.1181,
      "step": 1290
    },
    {
      "epoch": 3.5467032967032965,
      "grad_norm": 1.2672725915908813,
      "learning_rate": 5e-05,
      "loss": 1.104,
      "step": 1291
    },
    {
      "epoch": 3.5494505494505493,
      "grad_norm": 1.2957572937011719,
      "learning_rate": 5e-05,
      "loss": 1.1426,
      "step": 1292
    },
    {
      "epoch": 3.552197802197802,
      "grad_norm": 1.3305184841156006,
      "learning_rate": 5e-05,
      "loss": 1.1542,
      "step": 1293
    },
    {
      "epoch": 3.5549450549450547,
      "grad_norm": 1.3086357116699219,
      "learning_rate": 5e-05,
      "loss": 1.1408,
      "step": 1294
    },
    {
      "epoch": 3.5576923076923075,
      "grad_norm": 1.2944635152816772,
      "learning_rate": 5e-05,
      "loss": 1.1017,
      "step": 1295
    },
    {
      "epoch": 3.5604395604395602,
      "grad_norm": 1.295140027999878,
      "learning_rate": 5e-05,
      "loss": 1.1544,
      "step": 1296
    },
    {
      "epoch": 3.563186813186813,
      "grad_norm": 1.276590347290039,
      "learning_rate": 5e-05,
      "loss": 1.0755,
      "step": 1297
    },
    {
      "epoch": 3.5659340659340657,
      "grad_norm": 1.3152412176132202,
      "learning_rate": 5e-05,
      "loss": 1.125,
      "step": 1298
    },
    {
      "epoch": 3.5686813186813184,
      "grad_norm": 1.2795509099960327,
      "learning_rate": 5e-05,
      "loss": 1.1044,
      "step": 1299
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 1.2881629467010498,
      "learning_rate": 5e-05,
      "loss": 1.0905,
      "step": 1300
    },
    {
      "epoch": 3.574175824175824,
      "grad_norm": 1.367072343826294,
      "learning_rate": 5e-05,
      "loss": 1.1231,
      "step": 1301
    },
    {
      "epoch": 3.5769230769230766,
      "grad_norm": 1.289896845817566,
      "learning_rate": 5e-05,
      "loss": 1.1578,
      "step": 1302
    },
    {
      "epoch": 3.57967032967033,
      "grad_norm": 1.2891656160354614,
      "learning_rate": 5e-05,
      "loss": 1.0961,
      "step": 1303
    },
    {
      "epoch": 3.5824175824175826,
      "grad_norm": 1.3328999280929565,
      "learning_rate": 5e-05,
      "loss": 1.1416,
      "step": 1304
    },
    {
      "epoch": 3.5851648351648353,
      "grad_norm": 1.3703844547271729,
      "learning_rate": 5e-05,
      "loss": 1.0952,
      "step": 1305
    },
    {
      "epoch": 3.587912087912088,
      "grad_norm": 1.3040579557418823,
      "learning_rate": 5e-05,
      "loss": 1.1371,
      "step": 1306
    },
    {
      "epoch": 3.590659340659341,
      "grad_norm": 1.2733125686645508,
      "learning_rate": 5e-05,
      "loss": 1.1375,
      "step": 1307
    },
    {
      "epoch": 3.5934065934065935,
      "grad_norm": 1.3021138906478882,
      "learning_rate": 5e-05,
      "loss": 1.1911,
      "step": 1308
    },
    {
      "epoch": 3.5961538461538463,
      "grad_norm": 1.2798357009887695,
      "learning_rate": 5e-05,
      "loss": 1.0536,
      "step": 1309
    },
    {
      "epoch": 3.598901098901099,
      "grad_norm": 1.3494511842727661,
      "learning_rate": 5e-05,
      "loss": 1.169,
      "step": 1310
    },
    {
      "epoch": 3.6016483516483517,
      "grad_norm": 1.2380868196487427,
      "learning_rate": 5e-05,
      "loss": 0.9794,
      "step": 1311
    },
    {
      "epoch": 3.6043956043956045,
      "grad_norm": 1.284157156944275,
      "learning_rate": 5e-05,
      "loss": 1.1372,
      "step": 1312
    },
    {
      "epoch": 3.607142857142857,
      "grad_norm": 1.2686210870742798,
      "learning_rate": 5e-05,
      "loss": 1.0974,
      "step": 1313
    },
    {
      "epoch": 3.60989010989011,
      "grad_norm": 1.277738094329834,
      "learning_rate": 5e-05,
      "loss": 1.1099,
      "step": 1314
    },
    {
      "epoch": 3.6126373626373627,
      "grad_norm": 1.249037504196167,
      "learning_rate": 5e-05,
      "loss": 1.0194,
      "step": 1315
    },
    {
      "epoch": 3.6153846153846154,
      "grad_norm": 1.2749378681182861,
      "learning_rate": 5e-05,
      "loss": 1.1432,
      "step": 1316
    },
    {
      "epoch": 3.618131868131868,
      "grad_norm": 1.3326719999313354,
      "learning_rate": 5e-05,
      "loss": 1.1591,
      "step": 1317
    },
    {
      "epoch": 3.620879120879121,
      "grad_norm": 1.2733922004699707,
      "learning_rate": 5e-05,
      "loss": 1.1325,
      "step": 1318
    },
    {
      "epoch": 3.6236263736263736,
      "grad_norm": 1.2747095823287964,
      "learning_rate": 5e-05,
      "loss": 1.0878,
      "step": 1319
    },
    {
      "epoch": 3.6263736263736264,
      "grad_norm": 1.3040452003479004,
      "learning_rate": 5e-05,
      "loss": 1.2079,
      "step": 1320
    },
    {
      "epoch": 3.629120879120879,
      "grad_norm": 1.3248090744018555,
      "learning_rate": 5e-05,
      "loss": 1.1042,
      "step": 1321
    },
    {
      "epoch": 3.631868131868132,
      "grad_norm": 1.3380755186080933,
      "learning_rate": 5e-05,
      "loss": 1.2136,
      "step": 1322
    },
    {
      "epoch": 3.6346153846153846,
      "grad_norm": 1.2572965621948242,
      "learning_rate": 5e-05,
      "loss": 1.0719,
      "step": 1323
    },
    {
      "epoch": 3.6373626373626373,
      "grad_norm": 1.3129299879074097,
      "learning_rate": 5e-05,
      "loss": 1.1309,
      "step": 1324
    },
    {
      "epoch": 3.64010989010989,
      "grad_norm": 1.3100427389144897,
      "learning_rate": 5e-05,
      "loss": 1.1802,
      "step": 1325
    },
    {
      "epoch": 3.642857142857143,
      "grad_norm": 1.2608745098114014,
      "learning_rate": 5e-05,
      "loss": 1.0375,
      "step": 1326
    },
    {
      "epoch": 3.6456043956043955,
      "grad_norm": 1.3125547170639038,
      "learning_rate": 5e-05,
      "loss": 1.1896,
      "step": 1327
    },
    {
      "epoch": 3.6483516483516483,
      "grad_norm": 1.266224980354309,
      "learning_rate": 5e-05,
      "loss": 1.1216,
      "step": 1328
    },
    {
      "epoch": 3.651098901098901,
      "grad_norm": 1.3032029867172241,
      "learning_rate": 5e-05,
      "loss": 1.1803,
      "step": 1329
    },
    {
      "epoch": 3.6538461538461537,
      "grad_norm": 1.2565723657608032,
      "learning_rate": 5e-05,
      "loss": 1.0637,
      "step": 1330
    },
    {
      "epoch": 3.6565934065934065,
      "grad_norm": 1.2562127113342285,
      "learning_rate": 5e-05,
      "loss": 1.1194,
      "step": 1331
    },
    {
      "epoch": 3.659340659340659,
      "grad_norm": 1.3005141019821167,
      "learning_rate": 5e-05,
      "loss": 1.0933,
      "step": 1332
    },
    {
      "epoch": 3.662087912087912,
      "grad_norm": 1.286194086074829,
      "learning_rate": 5e-05,
      "loss": 1.121,
      "step": 1333
    },
    {
      "epoch": 3.6648351648351647,
      "grad_norm": 1.2984570264816284,
      "learning_rate": 5e-05,
      "loss": 1.13,
      "step": 1334
    },
    {
      "epoch": 3.6675824175824174,
      "grad_norm": 1.2850635051727295,
      "learning_rate": 5e-05,
      "loss": 1.1139,
      "step": 1335
    },
    {
      "epoch": 3.67032967032967,
      "grad_norm": 1.2843902111053467,
      "learning_rate": 5e-05,
      "loss": 1.0827,
      "step": 1336
    },
    {
      "epoch": 3.6730769230769234,
      "grad_norm": 1.3255345821380615,
      "learning_rate": 5e-05,
      "loss": 1.1366,
      "step": 1337
    },
    {
      "epoch": 3.675824175824176,
      "grad_norm": 1.31159508228302,
      "learning_rate": 5e-05,
      "loss": 1.0725,
      "step": 1338
    },
    {
      "epoch": 3.678571428571429,
      "grad_norm": 1.3810124397277832,
      "learning_rate": 5e-05,
      "loss": 1.1481,
      "step": 1339
    },
    {
      "epoch": 3.6813186813186816,
      "grad_norm": 1.2864227294921875,
      "learning_rate": 5e-05,
      "loss": 1.0666,
      "step": 1340
    },
    {
      "epoch": 3.6840659340659343,
      "grad_norm": 1.3282281160354614,
      "learning_rate": 5e-05,
      "loss": 1.0955,
      "step": 1341
    },
    {
      "epoch": 3.686813186813187,
      "grad_norm": 1.3147757053375244,
      "learning_rate": 5e-05,
      "loss": 1.1563,
      "step": 1342
    },
    {
      "epoch": 3.6895604395604398,
      "grad_norm": 1.288922667503357,
      "learning_rate": 5e-05,
      "loss": 1.137,
      "step": 1343
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 1.2658709287643433,
      "learning_rate": 5e-05,
      "loss": 1.0807,
      "step": 1344
    },
    {
      "epoch": 3.6950549450549453,
      "grad_norm": 1.2569741010665894,
      "learning_rate": 5e-05,
      "loss": 1.0133,
      "step": 1345
    },
    {
      "epoch": 3.697802197802198,
      "grad_norm": 1.319078803062439,
      "learning_rate": 5e-05,
      "loss": 1.1421,
      "step": 1346
    },
    {
      "epoch": 3.7005494505494507,
      "grad_norm": 1.3394808769226074,
      "learning_rate": 5e-05,
      "loss": 1.1058,
      "step": 1347
    },
    {
      "epoch": 3.7032967032967035,
      "grad_norm": 1.2743808031082153,
      "learning_rate": 5e-05,
      "loss": 1.0387,
      "step": 1348
    },
    {
      "epoch": 3.706043956043956,
      "grad_norm": 1.3325918912887573,
      "learning_rate": 5e-05,
      "loss": 1.1418,
      "step": 1349
    },
    {
      "epoch": 3.708791208791209,
      "grad_norm": 1.3121048212051392,
      "learning_rate": 5e-05,
      "loss": 1.0653,
      "step": 1350
    },
    {
      "epoch": 3.7115384615384617,
      "grad_norm": 1.3171485662460327,
      "learning_rate": 5e-05,
      "loss": 1.1138,
      "step": 1351
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 1.313768982887268,
      "learning_rate": 5e-05,
      "loss": 1.181,
      "step": 1352
    },
    {
      "epoch": 3.717032967032967,
      "grad_norm": 1.2719404697418213,
      "learning_rate": 5e-05,
      "loss": 1.0819,
      "step": 1353
    },
    {
      "epoch": 3.71978021978022,
      "grad_norm": 1.2836531400680542,
      "learning_rate": 5e-05,
      "loss": 1.0813,
      "step": 1354
    },
    {
      "epoch": 3.7225274725274726,
      "grad_norm": 1.311470866203308,
      "learning_rate": 5e-05,
      "loss": 1.1027,
      "step": 1355
    },
    {
      "epoch": 3.7252747252747254,
      "grad_norm": 1.2990225553512573,
      "learning_rate": 5e-05,
      "loss": 1.1562,
      "step": 1356
    },
    {
      "epoch": 3.728021978021978,
      "grad_norm": 1.2542551755905151,
      "learning_rate": 5e-05,
      "loss": 1.0274,
      "step": 1357
    },
    {
      "epoch": 3.730769230769231,
      "grad_norm": 1.2509561777114868,
      "learning_rate": 5e-05,
      "loss": 1.0619,
      "step": 1358
    },
    {
      "epoch": 3.7335164835164836,
      "grad_norm": 1.3113062381744385,
      "learning_rate": 5e-05,
      "loss": 1.1762,
      "step": 1359
    },
    {
      "epoch": 3.7362637362637363,
      "grad_norm": 1.2688666582107544,
      "learning_rate": 5e-05,
      "loss": 1.0644,
      "step": 1360
    },
    {
      "epoch": 3.739010989010989,
      "grad_norm": 1.302237868309021,
      "learning_rate": 5e-05,
      "loss": 1.1007,
      "step": 1361
    },
    {
      "epoch": 3.741758241758242,
      "grad_norm": 1.307663917541504,
      "learning_rate": 5e-05,
      "loss": 1.0924,
      "step": 1362
    },
    {
      "epoch": 3.7445054945054945,
      "grad_norm": 1.3690121173858643,
      "learning_rate": 5e-05,
      "loss": 1.1515,
      "step": 1363
    },
    {
      "epoch": 3.7472527472527473,
      "grad_norm": 1.2696490287780762,
      "learning_rate": 5e-05,
      "loss": 1.084,
      "step": 1364
    },
    {
      "epoch": 3.75,
      "grad_norm": 1.2988357543945312,
      "learning_rate": 5e-05,
      "loss": 1.1378,
      "step": 1365
    },
    {
      "epoch": 3.7527472527472527,
      "grad_norm": 1.2702510356903076,
      "learning_rate": 5e-05,
      "loss": 1.1083,
      "step": 1366
    },
    {
      "epoch": 3.7554945054945055,
      "grad_norm": 1.2660256624221802,
      "learning_rate": 5e-05,
      "loss": 1.0352,
      "step": 1367
    },
    {
      "epoch": 3.758241758241758,
      "grad_norm": 1.3111847639083862,
      "learning_rate": 5e-05,
      "loss": 1.1051,
      "step": 1368
    },
    {
      "epoch": 3.760989010989011,
      "grad_norm": 1.2817264795303345,
      "learning_rate": 5e-05,
      "loss": 1.0615,
      "step": 1369
    },
    {
      "epoch": 3.7637362637362637,
      "grad_norm": 1.2680268287658691,
      "learning_rate": 5e-05,
      "loss": 1.0257,
      "step": 1370
    },
    {
      "epoch": 3.7664835164835164,
      "grad_norm": 1.2965723276138306,
      "learning_rate": 5e-05,
      "loss": 1.0489,
      "step": 1371
    },
    {
      "epoch": 3.769230769230769,
      "grad_norm": 1.333963394165039,
      "learning_rate": 5e-05,
      "loss": 1.1161,
      "step": 1372
    },
    {
      "epoch": 3.771978021978022,
      "grad_norm": 1.2731608152389526,
      "learning_rate": 5e-05,
      "loss": 1.084,
      "step": 1373
    },
    {
      "epoch": 3.7747252747252746,
      "grad_norm": 1.29671049118042,
      "learning_rate": 5e-05,
      "loss": 1.0981,
      "step": 1374
    },
    {
      "epoch": 3.7774725274725274,
      "grad_norm": 1.3343251943588257,
      "learning_rate": 5e-05,
      "loss": 1.1008,
      "step": 1375
    },
    {
      "epoch": 3.78021978021978,
      "grad_norm": 1.3037803173065186,
      "learning_rate": 5e-05,
      "loss": 1.1352,
      "step": 1376
    },
    {
      "epoch": 3.782967032967033,
      "grad_norm": 1.2644977569580078,
      "learning_rate": 5e-05,
      "loss": 1.0728,
      "step": 1377
    },
    {
      "epoch": 3.7857142857142856,
      "grad_norm": 1.3134876489639282,
      "learning_rate": 5e-05,
      "loss": 1.1166,
      "step": 1378
    },
    {
      "epoch": 3.7884615384615383,
      "grad_norm": 1.3015624284744263,
      "learning_rate": 5e-05,
      "loss": 1.1513,
      "step": 1379
    },
    {
      "epoch": 3.791208791208791,
      "grad_norm": 1.3440126180648804,
      "learning_rate": 5e-05,
      "loss": 1.1642,
      "step": 1380
    },
    {
      "epoch": 3.793956043956044,
      "grad_norm": 1.2549186944961548,
      "learning_rate": 5e-05,
      "loss": 1.0756,
      "step": 1381
    },
    {
      "epoch": 3.7967032967032965,
      "grad_norm": 1.2756638526916504,
      "learning_rate": 5e-05,
      "loss": 1.0924,
      "step": 1382
    },
    {
      "epoch": 3.7994505494505493,
      "grad_norm": 1.3158514499664307,
      "learning_rate": 5e-05,
      "loss": 1.0649,
      "step": 1383
    },
    {
      "epoch": 3.802197802197802,
      "grad_norm": 1.2770237922668457,
      "learning_rate": 5e-05,
      "loss": 1.0865,
      "step": 1384
    },
    {
      "epoch": 3.8049450549450547,
      "grad_norm": 1.2829852104187012,
      "learning_rate": 5e-05,
      "loss": 1.1297,
      "step": 1385
    },
    {
      "epoch": 3.8076923076923075,
      "grad_norm": 1.306114912033081,
      "learning_rate": 5e-05,
      "loss": 1.1211,
      "step": 1386
    },
    {
      "epoch": 3.8104395604395602,
      "grad_norm": 1.2876194715499878,
      "learning_rate": 5e-05,
      "loss": 1.0681,
      "step": 1387
    },
    {
      "epoch": 3.813186813186813,
      "grad_norm": 1.2781871557235718,
      "learning_rate": 5e-05,
      "loss": 1.1245,
      "step": 1388
    },
    {
      "epoch": 3.8159340659340657,
      "grad_norm": 1.232285499572754,
      "learning_rate": 5e-05,
      "loss": 1.0414,
      "step": 1389
    },
    {
      "epoch": 3.8186813186813184,
      "grad_norm": 1.3295300006866455,
      "learning_rate": 5e-05,
      "loss": 1.1206,
      "step": 1390
    },
    {
      "epoch": 3.821428571428571,
      "grad_norm": 1.2726106643676758,
      "learning_rate": 5e-05,
      "loss": 1.0743,
      "step": 1391
    },
    {
      "epoch": 3.824175824175824,
      "grad_norm": 1.282985806465149,
      "learning_rate": 5e-05,
      "loss": 1.0754,
      "step": 1392
    },
    {
      "epoch": 3.8269230769230766,
      "grad_norm": 1.2781951427459717,
      "learning_rate": 5e-05,
      "loss": 1.0006,
      "step": 1393
    },
    {
      "epoch": 3.82967032967033,
      "grad_norm": 1.3382914066314697,
      "learning_rate": 5e-05,
      "loss": 1.1285,
      "step": 1394
    },
    {
      "epoch": 3.8324175824175826,
      "grad_norm": 1.2822880744934082,
      "learning_rate": 5e-05,
      "loss": 1.0588,
      "step": 1395
    },
    {
      "epoch": 3.8351648351648353,
      "grad_norm": 1.2727221250534058,
      "learning_rate": 5e-05,
      "loss": 1.0862,
      "step": 1396
    },
    {
      "epoch": 3.837912087912088,
      "grad_norm": 1.3286691904067993,
      "learning_rate": 5e-05,
      "loss": 1.1421,
      "step": 1397
    },
    {
      "epoch": 3.840659340659341,
      "grad_norm": 1.2925046682357788,
      "learning_rate": 5e-05,
      "loss": 1.1216,
      "step": 1398
    },
    {
      "epoch": 3.8434065934065935,
      "grad_norm": 1.2589107751846313,
      "learning_rate": 5e-05,
      "loss": 1.0854,
      "step": 1399
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 1.30439293384552,
      "learning_rate": 5e-05,
      "loss": 1.1393,
      "step": 1400
    },
    {
      "epoch": 3.848901098901099,
      "grad_norm": 1.3020116090774536,
      "learning_rate": 5e-05,
      "loss": 1.1298,
      "step": 1401
    },
    {
      "epoch": 3.8516483516483517,
      "grad_norm": 1.2751752138137817,
      "learning_rate": 5e-05,
      "loss": 1.109,
      "step": 1402
    },
    {
      "epoch": 3.8543956043956045,
      "grad_norm": 1.2853549718856812,
      "learning_rate": 5e-05,
      "loss": 1.1184,
      "step": 1403
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 1.337883472442627,
      "learning_rate": 5e-05,
      "loss": 1.0779,
      "step": 1404
    },
    {
      "epoch": 3.85989010989011,
      "grad_norm": 1.2541429996490479,
      "learning_rate": 5e-05,
      "loss": 1.1262,
      "step": 1405
    },
    {
      "epoch": 3.8626373626373627,
      "grad_norm": 1.2558348178863525,
      "learning_rate": 5e-05,
      "loss": 1.0855,
      "step": 1406
    },
    {
      "epoch": 3.8653846153846154,
      "grad_norm": 1.264084815979004,
      "learning_rate": 5e-05,
      "loss": 1.0356,
      "step": 1407
    },
    {
      "epoch": 3.868131868131868,
      "grad_norm": 1.2749121189117432,
      "learning_rate": 5e-05,
      "loss": 1.0739,
      "step": 1408
    },
    {
      "epoch": 3.870879120879121,
      "grad_norm": 1.3085298538208008,
      "learning_rate": 5e-05,
      "loss": 1.114,
      "step": 1409
    },
    {
      "epoch": 3.8736263736263736,
      "grad_norm": 1.2588592767715454,
      "learning_rate": 5e-05,
      "loss": 1.0733,
      "step": 1410
    },
    {
      "epoch": 3.8763736263736264,
      "grad_norm": 1.371078610420227,
      "learning_rate": 5e-05,
      "loss": 1.1715,
      "step": 1411
    },
    {
      "epoch": 3.879120879120879,
      "grad_norm": 1.315018892288208,
      "learning_rate": 5e-05,
      "loss": 1.1088,
      "step": 1412
    },
    {
      "epoch": 3.881868131868132,
      "grad_norm": 1.3109831809997559,
      "learning_rate": 5e-05,
      "loss": 1.1048,
      "step": 1413
    },
    {
      "epoch": 3.8846153846153846,
      "grad_norm": 1.323296308517456,
      "learning_rate": 5e-05,
      "loss": 1.1608,
      "step": 1414
    },
    {
      "epoch": 3.8873626373626373,
      "grad_norm": 1.3143671751022339,
      "learning_rate": 5e-05,
      "loss": 1.0742,
      "step": 1415
    },
    {
      "epoch": 3.89010989010989,
      "grad_norm": 1.2887942790985107,
      "learning_rate": 5e-05,
      "loss": 1.0816,
      "step": 1416
    },
    {
      "epoch": 3.892857142857143,
      "grad_norm": 1.2558866739273071,
      "learning_rate": 5e-05,
      "loss": 1.0683,
      "step": 1417
    },
    {
      "epoch": 3.8956043956043955,
      "grad_norm": 1.3049123287200928,
      "learning_rate": 5e-05,
      "loss": 1.066,
      "step": 1418
    },
    {
      "epoch": 3.8983516483516483,
      "grad_norm": 1.2740840911865234,
      "learning_rate": 5e-05,
      "loss": 1.0454,
      "step": 1419
    },
    {
      "epoch": 3.901098901098901,
      "grad_norm": 1.335769534111023,
      "learning_rate": 5e-05,
      "loss": 1.1113,
      "step": 1420
    },
    {
      "epoch": 3.9038461538461537,
      "grad_norm": 1.312728762626648,
      "learning_rate": 5e-05,
      "loss": 1.0383,
      "step": 1421
    },
    {
      "epoch": 3.9065934065934065,
      "grad_norm": 1.2860825061798096,
      "learning_rate": 5e-05,
      "loss": 1.0488,
      "step": 1422
    },
    {
      "epoch": 3.909340659340659,
      "grad_norm": 1.3171619176864624,
      "learning_rate": 5e-05,
      "loss": 1.0984,
      "step": 1423
    },
    {
      "epoch": 3.912087912087912,
      "grad_norm": 1.3102600574493408,
      "learning_rate": 5e-05,
      "loss": 1.0719,
      "step": 1424
    },
    {
      "epoch": 3.9148351648351647,
      "grad_norm": 1.2411415576934814,
      "learning_rate": 5e-05,
      "loss": 0.9848,
      "step": 1425
    },
    {
      "epoch": 3.9175824175824174,
      "grad_norm": 1.2965061664581299,
      "learning_rate": 5e-05,
      "loss": 1.1456,
      "step": 1426
    },
    {
      "epoch": 3.92032967032967,
      "grad_norm": 1.3122869729995728,
      "learning_rate": 5e-05,
      "loss": 1.0908,
      "step": 1427
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 1.2534763813018799,
      "learning_rate": 5e-05,
      "loss": 1.0644,
      "step": 1428
    },
    {
      "epoch": 3.925824175824176,
      "grad_norm": 1.2600091695785522,
      "learning_rate": 5e-05,
      "loss": 1.0385,
      "step": 1429
    },
    {
      "epoch": 3.928571428571429,
      "grad_norm": 1.3406645059585571,
      "learning_rate": 5e-05,
      "loss": 1.1189,
      "step": 1430
    },
    {
      "epoch": 3.9313186813186816,
      "grad_norm": 1.3049993515014648,
      "learning_rate": 5e-05,
      "loss": 1.0886,
      "step": 1431
    },
    {
      "epoch": 3.9340659340659343,
      "grad_norm": 1.2638942003250122,
      "learning_rate": 5e-05,
      "loss": 1.0917,
      "step": 1432
    },
    {
      "epoch": 3.936813186813187,
      "grad_norm": 1.2673548460006714,
      "learning_rate": 5e-05,
      "loss": 1.0717,
      "step": 1433
    },
    {
      "epoch": 3.9395604395604398,
      "grad_norm": 1.3121535778045654,
      "learning_rate": 5e-05,
      "loss": 1.103,
      "step": 1434
    },
    {
      "epoch": 3.9423076923076925,
      "grad_norm": 1.2767984867095947,
      "learning_rate": 5e-05,
      "loss": 1.015,
      "step": 1435
    },
    {
      "epoch": 3.9450549450549453,
      "grad_norm": 1.2834163904190063,
      "learning_rate": 5e-05,
      "loss": 1.0357,
      "step": 1436
    },
    {
      "epoch": 3.947802197802198,
      "grad_norm": 1.3029475212097168,
      "learning_rate": 5e-05,
      "loss": 1.0596,
      "step": 1437
    },
    {
      "epoch": 3.9505494505494507,
      "grad_norm": 1.2636202573776245,
      "learning_rate": 5e-05,
      "loss": 1.0563,
      "step": 1438
    },
    {
      "epoch": 3.9532967032967035,
      "grad_norm": 1.297680377960205,
      "learning_rate": 5e-05,
      "loss": 1.0738,
      "step": 1439
    },
    {
      "epoch": 3.956043956043956,
      "grad_norm": 1.3480008840560913,
      "learning_rate": 5e-05,
      "loss": 1.1049,
      "step": 1440
    },
    {
      "epoch": 3.958791208791209,
      "grad_norm": 1.3156496286392212,
      "learning_rate": 5e-05,
      "loss": 1.0847,
      "step": 1441
    },
    {
      "epoch": 3.9615384615384617,
      "grad_norm": 1.2930930852890015,
      "learning_rate": 5e-05,
      "loss": 1.0699,
      "step": 1442
    },
    {
      "epoch": 3.9642857142857144,
      "grad_norm": 1.2790049314498901,
      "learning_rate": 5e-05,
      "loss": 1.0835,
      "step": 1443
    },
    {
      "epoch": 3.967032967032967,
      "grad_norm": 1.3154516220092773,
      "learning_rate": 5e-05,
      "loss": 1.1427,
      "step": 1444
    },
    {
      "epoch": 3.96978021978022,
      "grad_norm": 1.2985817193984985,
      "learning_rate": 5e-05,
      "loss": 1.0414,
      "step": 1445
    },
    {
      "epoch": 3.9725274725274726,
      "grad_norm": 1.3208060264587402,
      "learning_rate": 5e-05,
      "loss": 1.148,
      "step": 1446
    },
    {
      "epoch": 3.9752747252747254,
      "grad_norm": 1.3037108182907104,
      "learning_rate": 5e-05,
      "loss": 1.1276,
      "step": 1447
    },
    {
      "epoch": 3.978021978021978,
      "grad_norm": 1.2699968814849854,
      "learning_rate": 5e-05,
      "loss": 1.0086,
      "step": 1448
    },
    {
      "epoch": 3.980769230769231,
      "grad_norm": 1.3339829444885254,
      "learning_rate": 5e-05,
      "loss": 1.0712,
      "step": 1449
    },
    {
      "epoch": 3.9835164835164836,
      "grad_norm": 1.269389033317566,
      "learning_rate": 5e-05,
      "loss": 1.0275,
      "step": 1450
    },
    {
      "epoch": 3.9862637362637363,
      "grad_norm": 1.289036512374878,
      "learning_rate": 5e-05,
      "loss": 1.061,
      "step": 1451
    },
    {
      "epoch": 3.989010989010989,
      "grad_norm": 1.2848312854766846,
      "learning_rate": 5e-05,
      "loss": 1.1135,
      "step": 1452
    },
    {
      "epoch": 3.991758241758242,
      "grad_norm": 1.2721102237701416,
      "learning_rate": 5e-05,
      "loss": 1.0526,
      "step": 1453
    },
    {
      "epoch": 3.9945054945054945,
      "grad_norm": 1.30924391746521,
      "learning_rate": 5e-05,
      "loss": 1.0835,
      "step": 1454
    },
    {
      "epoch": 3.9972527472527473,
      "grad_norm": 1.3322380781173706,
      "learning_rate": 5e-05,
      "loss": 1.1472,
      "step": 1455
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.2856004238128662,
      "learning_rate": 5e-05,
      "loss": 1.0844,
      "step": 1456
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.8317958116531372,
      "eval_runtime": 82.3747,
      "eval_samples_per_second": 502.642,
      "eval_steps_per_second": 3.933,
      "step": 1456
    },
    {
      "epoch": 4.002747252747253,
      "grad_norm": 1.2593761682510376,
      "learning_rate": 5e-05,
      "loss": 0.9963,
      "step": 1457
    },
    {
      "epoch": 4.0054945054945055,
      "grad_norm": 1.2857367992401123,
      "learning_rate": 5e-05,
      "loss": 1.0453,
      "step": 1458
    },
    {
      "epoch": 4.008241758241758,
      "grad_norm": 1.2862118482589722,
      "learning_rate": 5e-05,
      "loss": 1.0594,
      "step": 1459
    },
    {
      "epoch": 4.010989010989011,
      "grad_norm": 1.3094651699066162,
      "learning_rate": 5e-05,
      "loss": 1.0875,
      "step": 1460
    },
    {
      "epoch": 4.013736263736264,
      "grad_norm": 1.276760220527649,
      "learning_rate": 5e-05,
      "loss": 0.9928,
      "step": 1461
    },
    {
      "epoch": 4.016483516483516,
      "grad_norm": 1.2068721055984497,
      "learning_rate": 5e-05,
      "loss": 0.9886,
      "step": 1462
    },
    {
      "epoch": 4.019230769230769,
      "grad_norm": 1.2881958484649658,
      "learning_rate": 5e-05,
      "loss": 1.0581,
      "step": 1463
    },
    {
      "epoch": 4.021978021978022,
      "grad_norm": 1.294987440109253,
      "learning_rate": 5e-05,
      "loss": 1.042,
      "step": 1464
    },
    {
      "epoch": 4.024725274725275,
      "grad_norm": 1.2522975206375122,
      "learning_rate": 5e-05,
      "loss": 1.004,
      "step": 1465
    },
    {
      "epoch": 4.027472527472527,
      "grad_norm": 1.3152726888656616,
      "learning_rate": 5e-05,
      "loss": 1.0575,
      "step": 1466
    },
    {
      "epoch": 4.03021978021978,
      "grad_norm": 1.3083525896072388,
      "learning_rate": 5e-05,
      "loss": 1.0912,
      "step": 1467
    },
    {
      "epoch": 4.032967032967033,
      "grad_norm": 1.2901901006698608,
      "learning_rate": 5e-05,
      "loss": 1.0009,
      "step": 1468
    },
    {
      "epoch": 4.035714285714286,
      "grad_norm": 1.2959522008895874,
      "learning_rate": 5e-05,
      "loss": 1.0378,
      "step": 1469
    },
    {
      "epoch": 4.038461538461538,
      "grad_norm": 1.3278422355651855,
      "learning_rate": 5e-05,
      "loss": 1.0569,
      "step": 1470
    },
    {
      "epoch": 4.041208791208791,
      "grad_norm": 1.2799171209335327,
      "learning_rate": 5e-05,
      "loss": 1.0269,
      "step": 1471
    },
    {
      "epoch": 4.043956043956044,
      "grad_norm": 1.240535020828247,
      "learning_rate": 5e-05,
      "loss": 0.9911,
      "step": 1472
    },
    {
      "epoch": 4.0467032967032965,
      "grad_norm": 1.2822366952896118,
      "learning_rate": 5e-05,
      "loss": 1.0151,
      "step": 1473
    },
    {
      "epoch": 4.049450549450549,
      "grad_norm": 1.2684828042984009,
      "learning_rate": 5e-05,
      "loss": 0.9894,
      "step": 1474
    },
    {
      "epoch": 4.052197802197802,
      "grad_norm": 1.2594279050827026,
      "learning_rate": 5e-05,
      "loss": 1.0,
      "step": 1475
    },
    {
      "epoch": 4.054945054945055,
      "grad_norm": 1.2375704050064087,
      "learning_rate": 5e-05,
      "loss": 0.9816,
      "step": 1476
    },
    {
      "epoch": 4.0576923076923075,
      "grad_norm": 1.2591842412948608,
      "learning_rate": 5e-05,
      "loss": 0.9963,
      "step": 1477
    },
    {
      "epoch": 4.06043956043956,
      "grad_norm": 1.3211880922317505,
      "learning_rate": 5e-05,
      "loss": 1.0844,
      "step": 1478
    },
    {
      "epoch": 4.063186813186813,
      "grad_norm": 1.2480659484863281,
      "learning_rate": 5e-05,
      "loss": 0.9606,
      "step": 1479
    },
    {
      "epoch": 4.065934065934066,
      "grad_norm": 1.2743916511535645,
      "learning_rate": 5e-05,
      "loss": 0.9723,
      "step": 1480
    },
    {
      "epoch": 4.068681318681318,
      "grad_norm": 1.282933235168457,
      "learning_rate": 5e-05,
      "loss": 1.034,
      "step": 1481
    },
    {
      "epoch": 4.071428571428571,
      "grad_norm": 1.3130033016204834,
      "learning_rate": 5e-05,
      "loss": 1.0451,
      "step": 1482
    },
    {
      "epoch": 4.074175824175824,
      "grad_norm": 1.2898424863815308,
      "learning_rate": 5e-05,
      "loss": 1.0853,
      "step": 1483
    },
    {
      "epoch": 4.076923076923077,
      "grad_norm": 1.2803339958190918,
      "learning_rate": 5e-05,
      "loss": 1.0558,
      "step": 1484
    },
    {
      "epoch": 4.079670329670329,
      "grad_norm": 1.2586408853530884,
      "learning_rate": 5e-05,
      "loss": 0.9454,
      "step": 1485
    },
    {
      "epoch": 4.082417582417582,
      "grad_norm": 1.3080437183380127,
      "learning_rate": 5e-05,
      "loss": 1.0425,
      "step": 1486
    },
    {
      "epoch": 4.085164835164835,
      "grad_norm": 1.2882264852523804,
      "learning_rate": 5e-05,
      "loss": 1.0083,
      "step": 1487
    },
    {
      "epoch": 4.087912087912088,
      "grad_norm": 1.281165599822998,
      "learning_rate": 5e-05,
      "loss": 0.9758,
      "step": 1488
    },
    {
      "epoch": 4.09065934065934,
      "grad_norm": 1.2818937301635742,
      "learning_rate": 5e-05,
      "loss": 1.0418,
      "step": 1489
    },
    {
      "epoch": 4.093406593406593,
      "grad_norm": 1.2405574321746826,
      "learning_rate": 5e-05,
      "loss": 0.9488,
      "step": 1490
    },
    {
      "epoch": 4.096153846153846,
      "grad_norm": 1.2525359392166138,
      "learning_rate": 5e-05,
      "loss": 0.929,
      "step": 1491
    },
    {
      "epoch": 4.0989010989010985,
      "grad_norm": 1.260404109954834,
      "learning_rate": 5e-05,
      "loss": 1.0184,
      "step": 1492
    },
    {
      "epoch": 4.101648351648351,
      "grad_norm": 1.2628610134124756,
      "learning_rate": 5e-05,
      "loss": 0.9718,
      "step": 1493
    },
    {
      "epoch": 4.104395604395604,
      "grad_norm": 1.3039016723632812,
      "learning_rate": 5e-05,
      "loss": 0.9997,
      "step": 1494
    },
    {
      "epoch": 4.107142857142857,
      "grad_norm": 1.2780053615570068,
      "learning_rate": 5e-05,
      "loss": 0.9643,
      "step": 1495
    },
    {
      "epoch": 4.1098901098901095,
      "grad_norm": 1.2566359043121338,
      "learning_rate": 5e-05,
      "loss": 1.0019,
      "step": 1496
    },
    {
      "epoch": 4.112637362637362,
      "grad_norm": 1.2988228797912598,
      "learning_rate": 5e-05,
      "loss": 1.0059,
      "step": 1497
    },
    {
      "epoch": 4.115384615384615,
      "grad_norm": 1.2826192378997803,
      "learning_rate": 5e-05,
      "loss": 0.9898,
      "step": 1498
    },
    {
      "epoch": 4.118131868131868,
      "grad_norm": 1.3141255378723145,
      "learning_rate": 5e-05,
      "loss": 1.041,
      "step": 1499
    },
    {
      "epoch": 4.1208791208791204,
      "grad_norm": 1.2901602983474731,
      "learning_rate": 5e-05,
      "loss": 0.9832,
      "step": 1500
    },
    {
      "epoch": 4.123626373626374,
      "grad_norm": 1.293958306312561,
      "learning_rate": 5e-05,
      "loss": 1.0208,
      "step": 1501
    },
    {
      "epoch": 4.126373626373626,
      "grad_norm": 1.2675777673721313,
      "learning_rate": 5e-05,
      "loss": 0.9367,
      "step": 1502
    },
    {
      "epoch": 4.1291208791208796,
      "grad_norm": 1.3007267713546753,
      "learning_rate": 5e-05,
      "loss": 1.0209,
      "step": 1503
    },
    {
      "epoch": 4.131868131868132,
      "grad_norm": 1.2606067657470703,
      "learning_rate": 5e-05,
      "loss": 0.9692,
      "step": 1504
    },
    {
      "epoch": 4.134615384615385,
      "grad_norm": 1.277374505996704,
      "learning_rate": 5e-05,
      "loss": 0.9991,
      "step": 1505
    },
    {
      "epoch": 4.137362637362638,
      "grad_norm": 1.2471078634262085,
      "learning_rate": 5e-05,
      "loss": 0.9755,
      "step": 1506
    },
    {
      "epoch": 4.1401098901098905,
      "grad_norm": 1.259961485862732,
      "learning_rate": 5e-05,
      "loss": 1.0008,
      "step": 1507
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 1.2236607074737549,
      "learning_rate": 5e-05,
      "loss": 0.9639,
      "step": 1508
    },
    {
      "epoch": 4.145604395604396,
      "grad_norm": 1.2659344673156738,
      "learning_rate": 5e-05,
      "loss": 1.0182,
      "step": 1509
    },
    {
      "epoch": 4.148351648351649,
      "grad_norm": 1.2930971384048462,
      "learning_rate": 5e-05,
      "loss": 1.035,
      "step": 1510
    },
    {
      "epoch": 4.1510989010989015,
      "grad_norm": 1.2589459419250488,
      "learning_rate": 5e-05,
      "loss": 1.0617,
      "step": 1511
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 1.2996656894683838,
      "learning_rate": 5e-05,
      "loss": 1.0379,
      "step": 1512
    },
    {
      "epoch": 4.156593406593407,
      "grad_norm": 1.2980462312698364,
      "learning_rate": 5e-05,
      "loss": 1.0502,
      "step": 1513
    },
    {
      "epoch": 4.15934065934066,
      "grad_norm": 1.3199456930160522,
      "learning_rate": 5e-05,
      "loss": 1.019,
      "step": 1514
    },
    {
      "epoch": 4.162087912087912,
      "grad_norm": 1.2977135181427002,
      "learning_rate": 5e-05,
      "loss": 1.0559,
      "step": 1515
    },
    {
      "epoch": 4.164835164835165,
      "grad_norm": 1.2966245412826538,
      "learning_rate": 5e-05,
      "loss": 1.0342,
      "step": 1516
    },
    {
      "epoch": 4.167582417582418,
      "grad_norm": 1.2568986415863037,
      "learning_rate": 5e-05,
      "loss": 0.9586,
      "step": 1517
    },
    {
      "epoch": 4.170329670329671,
      "grad_norm": 1.3126959800720215,
      "learning_rate": 5e-05,
      "loss": 1.027,
      "step": 1518
    },
    {
      "epoch": 4.173076923076923,
      "grad_norm": 1.2814544439315796,
      "learning_rate": 5e-05,
      "loss": 0.9793,
      "step": 1519
    },
    {
      "epoch": 4.175824175824176,
      "grad_norm": 1.3154534101486206,
      "learning_rate": 5e-05,
      "loss": 0.9932,
      "step": 1520
    },
    {
      "epoch": 4.178571428571429,
      "grad_norm": 1.2931400537490845,
      "learning_rate": 5e-05,
      "loss": 0.9769,
      "step": 1521
    },
    {
      "epoch": 4.181318681318682,
      "grad_norm": 1.2691093683242798,
      "learning_rate": 5e-05,
      "loss": 0.9729,
      "step": 1522
    },
    {
      "epoch": 4.184065934065934,
      "grad_norm": 1.3278483152389526,
      "learning_rate": 5e-05,
      "loss": 1.0416,
      "step": 1523
    },
    {
      "epoch": 4.186813186813187,
      "grad_norm": 1.2842448949813843,
      "learning_rate": 5e-05,
      "loss": 0.9773,
      "step": 1524
    },
    {
      "epoch": 4.18956043956044,
      "grad_norm": 1.2868685722351074,
      "learning_rate": 5e-05,
      "loss": 1.0472,
      "step": 1525
    },
    {
      "epoch": 4.1923076923076925,
      "grad_norm": 1.3303542137145996,
      "learning_rate": 5e-05,
      "loss": 1.0857,
      "step": 1526
    },
    {
      "epoch": 4.195054945054945,
      "grad_norm": 1.2363277673721313,
      "learning_rate": 5e-05,
      "loss": 0.9803,
      "step": 1527
    },
    {
      "epoch": 4.197802197802198,
      "grad_norm": 1.2535691261291504,
      "learning_rate": 5e-05,
      "loss": 0.963,
      "step": 1528
    },
    {
      "epoch": 4.200549450549451,
      "grad_norm": 1.2429956197738647,
      "learning_rate": 5e-05,
      "loss": 0.9882,
      "step": 1529
    },
    {
      "epoch": 4.2032967032967035,
      "grad_norm": 1.2815001010894775,
      "learning_rate": 5e-05,
      "loss": 0.9751,
      "step": 1530
    },
    {
      "epoch": 4.206043956043956,
      "grad_norm": 1.3203191757202148,
      "learning_rate": 5e-05,
      "loss": 0.9816,
      "step": 1531
    },
    {
      "epoch": 4.208791208791209,
      "grad_norm": 1.3043731451034546,
      "learning_rate": 5e-05,
      "loss": 1.1213,
      "step": 1532
    },
    {
      "epoch": 4.211538461538462,
      "grad_norm": 1.3341546058654785,
      "learning_rate": 5e-05,
      "loss": 1.0497,
      "step": 1533
    },
    {
      "epoch": 4.214285714285714,
      "grad_norm": 1.3336337804794312,
      "learning_rate": 5e-05,
      "loss": 1.0601,
      "step": 1534
    },
    {
      "epoch": 4.217032967032967,
      "grad_norm": 1.3118736743927002,
      "learning_rate": 5e-05,
      "loss": 1.0103,
      "step": 1535
    },
    {
      "epoch": 4.21978021978022,
      "grad_norm": 1.270075798034668,
      "learning_rate": 5e-05,
      "loss": 0.9872,
      "step": 1536
    },
    {
      "epoch": 4.222527472527473,
      "grad_norm": 1.3337805271148682,
      "learning_rate": 5e-05,
      "loss": 1.0557,
      "step": 1537
    },
    {
      "epoch": 4.225274725274725,
      "grad_norm": 1.2646331787109375,
      "learning_rate": 5e-05,
      "loss": 0.9828,
      "step": 1538
    },
    {
      "epoch": 4.228021978021978,
      "grad_norm": 1.2763341665267944,
      "learning_rate": 5e-05,
      "loss": 1.0076,
      "step": 1539
    },
    {
      "epoch": 4.230769230769231,
      "grad_norm": 1.2394908666610718,
      "learning_rate": 5e-05,
      "loss": 0.9736,
      "step": 1540
    },
    {
      "epoch": 4.233516483516484,
      "grad_norm": 1.268638253211975,
      "learning_rate": 5e-05,
      "loss": 1.0274,
      "step": 1541
    },
    {
      "epoch": 4.236263736263736,
      "grad_norm": 1.3492436408996582,
      "learning_rate": 5e-05,
      "loss": 1.0795,
      "step": 1542
    },
    {
      "epoch": 4.239010989010989,
      "grad_norm": 1.2635445594787598,
      "learning_rate": 5e-05,
      "loss": 0.9843,
      "step": 1543
    },
    {
      "epoch": 4.241758241758242,
      "grad_norm": 1.2842826843261719,
      "learning_rate": 5e-05,
      "loss": 0.984,
      "step": 1544
    },
    {
      "epoch": 4.2445054945054945,
      "grad_norm": 1.2507179975509644,
      "learning_rate": 5e-05,
      "loss": 0.9502,
      "step": 1545
    },
    {
      "epoch": 4.247252747252747,
      "grad_norm": 1.3302125930786133,
      "learning_rate": 5e-05,
      "loss": 1.0412,
      "step": 1546
    },
    {
      "epoch": 4.25,
      "grad_norm": 1.2526085376739502,
      "learning_rate": 5e-05,
      "loss": 0.9956,
      "step": 1547
    },
    {
      "epoch": 4.252747252747253,
      "grad_norm": 1.2774354219436646,
      "learning_rate": 5e-05,
      "loss": 0.9875,
      "step": 1548
    },
    {
      "epoch": 4.2554945054945055,
      "grad_norm": 1.2594770193099976,
      "learning_rate": 5e-05,
      "loss": 0.9656,
      "step": 1549
    },
    {
      "epoch": 4.258241758241758,
      "grad_norm": 1.3014510869979858,
      "learning_rate": 5e-05,
      "loss": 1.021,
      "step": 1550
    },
    {
      "epoch": 4.260989010989011,
      "grad_norm": 1.2994499206542969,
      "learning_rate": 5e-05,
      "loss": 1.0062,
      "step": 1551
    },
    {
      "epoch": 4.263736263736264,
      "grad_norm": 1.2800744771957397,
      "learning_rate": 5e-05,
      "loss": 0.9929,
      "step": 1552
    },
    {
      "epoch": 4.266483516483516,
      "grad_norm": 1.311000108718872,
      "learning_rate": 5e-05,
      "loss": 1.0364,
      "step": 1553
    },
    {
      "epoch": 4.269230769230769,
      "grad_norm": 1.2727457284927368,
      "learning_rate": 5e-05,
      "loss": 1.0164,
      "step": 1554
    },
    {
      "epoch": 4.271978021978022,
      "grad_norm": 1.2724785804748535,
      "learning_rate": 5e-05,
      "loss": 0.9305,
      "step": 1555
    },
    {
      "epoch": 4.274725274725275,
      "grad_norm": 1.3482697010040283,
      "learning_rate": 5e-05,
      "loss": 0.994,
      "step": 1556
    },
    {
      "epoch": 4.277472527472527,
      "grad_norm": 1.2985392808914185,
      "learning_rate": 5e-05,
      "loss": 0.9976,
      "step": 1557
    },
    {
      "epoch": 4.28021978021978,
      "grad_norm": 1.3124077320098877,
      "learning_rate": 5e-05,
      "loss": 1.0083,
      "step": 1558
    },
    {
      "epoch": 4.282967032967033,
      "grad_norm": 1.2366324663162231,
      "learning_rate": 5e-05,
      "loss": 0.9968,
      "step": 1559
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 1.3129137754440308,
      "learning_rate": 5e-05,
      "loss": 0.9786,
      "step": 1560
    },
    {
      "epoch": 4.288461538461538,
      "grad_norm": 1.3217313289642334,
      "learning_rate": 5e-05,
      "loss": 1.066,
      "step": 1561
    },
    {
      "epoch": 4.291208791208791,
      "grad_norm": 1.279543161392212,
      "learning_rate": 5e-05,
      "loss": 0.9418,
      "step": 1562
    },
    {
      "epoch": 4.293956043956044,
      "grad_norm": 1.2269244194030762,
      "learning_rate": 5e-05,
      "loss": 0.9192,
      "step": 1563
    },
    {
      "epoch": 4.2967032967032965,
      "grad_norm": 1.2838205099105835,
      "learning_rate": 5e-05,
      "loss": 1.0284,
      "step": 1564
    },
    {
      "epoch": 4.299450549450549,
      "grad_norm": 1.2501777410507202,
      "learning_rate": 5e-05,
      "loss": 0.9484,
      "step": 1565
    },
    {
      "epoch": 4.302197802197802,
      "grad_norm": 1.289597511291504,
      "learning_rate": 5e-05,
      "loss": 1.017,
      "step": 1566
    },
    {
      "epoch": 4.304945054945055,
      "grad_norm": 1.3069987297058105,
      "learning_rate": 5e-05,
      "loss": 1.0105,
      "step": 1567
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 1.3064805269241333,
      "learning_rate": 5e-05,
      "loss": 1.0527,
      "step": 1568
    },
    {
      "epoch": 4.31043956043956,
      "grad_norm": 1.254596471786499,
      "learning_rate": 5e-05,
      "loss": 0.9761,
      "step": 1569
    },
    {
      "epoch": 4.313186813186813,
      "grad_norm": 1.2911406755447388,
      "learning_rate": 5e-05,
      "loss": 0.9904,
      "step": 1570
    },
    {
      "epoch": 4.315934065934066,
      "grad_norm": 1.3006013631820679,
      "learning_rate": 5e-05,
      "loss": 1.0392,
      "step": 1571
    },
    {
      "epoch": 4.318681318681318,
      "grad_norm": 1.2920562028884888,
      "learning_rate": 5e-05,
      "loss": 1.0338,
      "step": 1572
    },
    {
      "epoch": 4.321428571428571,
      "grad_norm": 1.3132658004760742,
      "learning_rate": 5e-05,
      "loss": 1.0336,
      "step": 1573
    },
    {
      "epoch": 4.324175824175824,
      "grad_norm": 1.327011227607727,
      "learning_rate": 5e-05,
      "loss": 0.954,
      "step": 1574
    },
    {
      "epoch": 4.326923076923077,
      "grad_norm": 1.2603203058242798,
      "learning_rate": 5e-05,
      "loss": 0.9243,
      "step": 1575
    },
    {
      "epoch": 4.329670329670329,
      "grad_norm": 1.2974255084991455,
      "learning_rate": 5e-05,
      "loss": 1.0077,
      "step": 1576
    },
    {
      "epoch": 4.332417582417582,
      "grad_norm": 1.2524218559265137,
      "learning_rate": 5e-05,
      "loss": 0.9809,
      "step": 1577
    },
    {
      "epoch": 4.335164835164835,
      "grad_norm": 1.2752457857131958,
      "learning_rate": 5e-05,
      "loss": 0.9579,
      "step": 1578
    },
    {
      "epoch": 4.337912087912088,
      "grad_norm": 1.3273332118988037,
      "learning_rate": 5e-05,
      "loss": 1.0294,
      "step": 1579
    },
    {
      "epoch": 4.34065934065934,
      "grad_norm": 1.2954143285751343,
      "learning_rate": 5e-05,
      "loss": 0.9584,
      "step": 1580
    },
    {
      "epoch": 4.343406593406593,
      "grad_norm": 1.304118037223816,
      "learning_rate": 5e-05,
      "loss": 1.0027,
      "step": 1581
    },
    {
      "epoch": 4.346153846153846,
      "grad_norm": 1.285975694656372,
      "learning_rate": 5e-05,
      "loss": 0.9591,
      "step": 1582
    },
    {
      "epoch": 4.3489010989010985,
      "grad_norm": 1.2943896055221558,
      "learning_rate": 5e-05,
      "loss": 0.997,
      "step": 1583
    },
    {
      "epoch": 4.351648351648351,
      "grad_norm": 1.2797424793243408,
      "learning_rate": 5e-05,
      "loss": 0.9586,
      "step": 1584
    },
    {
      "epoch": 4.354395604395604,
      "grad_norm": 1.308440923690796,
      "learning_rate": 5e-05,
      "loss": 0.9989,
      "step": 1585
    },
    {
      "epoch": 4.357142857142857,
      "grad_norm": 1.2425049543380737,
      "learning_rate": 5e-05,
      "loss": 0.9529,
      "step": 1586
    },
    {
      "epoch": 4.3598901098901095,
      "grad_norm": 1.2671887874603271,
      "learning_rate": 5e-05,
      "loss": 0.9481,
      "step": 1587
    },
    {
      "epoch": 4.362637362637362,
      "grad_norm": 1.273606777191162,
      "learning_rate": 5e-05,
      "loss": 1.0073,
      "step": 1588
    },
    {
      "epoch": 4.365384615384615,
      "grad_norm": 1.233068823814392,
      "learning_rate": 5e-05,
      "loss": 0.953,
      "step": 1589
    },
    {
      "epoch": 4.368131868131869,
      "grad_norm": 1.3107407093048096,
      "learning_rate": 5e-05,
      "loss": 1.0039,
      "step": 1590
    },
    {
      "epoch": 4.3708791208791204,
      "grad_norm": 1.267099380493164,
      "learning_rate": 5e-05,
      "loss": 0.9692,
      "step": 1591
    },
    {
      "epoch": 4.373626373626374,
      "grad_norm": 1.2572132349014282,
      "learning_rate": 5e-05,
      "loss": 0.9548,
      "step": 1592
    },
    {
      "epoch": 4.376373626373626,
      "grad_norm": 1.3265728950500488,
      "learning_rate": 5e-05,
      "loss": 1.0065,
      "step": 1593
    },
    {
      "epoch": 4.3791208791208796,
      "grad_norm": 1.2902003526687622,
      "learning_rate": 5e-05,
      "loss": 0.9563,
      "step": 1594
    },
    {
      "epoch": 4.381868131868131,
      "grad_norm": 1.2961523532867432,
      "learning_rate": 5e-05,
      "loss": 0.953,
      "step": 1595
    },
    {
      "epoch": 4.384615384615385,
      "grad_norm": 1.3004080057144165,
      "learning_rate": 5e-05,
      "loss": 0.9966,
      "step": 1596
    },
    {
      "epoch": 4.387362637362638,
      "grad_norm": 1.3222938776016235,
      "learning_rate": 5e-05,
      "loss": 1.0166,
      "step": 1597
    },
    {
      "epoch": 4.3901098901098905,
      "grad_norm": 1.251949667930603,
      "learning_rate": 5e-05,
      "loss": 0.937,
      "step": 1598
    },
    {
      "epoch": 4.392857142857143,
      "grad_norm": 1.2599003314971924,
      "learning_rate": 5e-05,
      "loss": 0.9779,
      "step": 1599
    },
    {
      "epoch": 4.395604395604396,
      "grad_norm": 1.2799407243728638,
      "learning_rate": 5e-05,
      "loss": 0.9307,
      "step": 1600
    },
    {
      "epoch": 4.398351648351649,
      "grad_norm": 1.2968535423278809,
      "learning_rate": 5e-05,
      "loss": 0.984,
      "step": 1601
    },
    {
      "epoch": 4.4010989010989015,
      "grad_norm": 1.2983694076538086,
      "learning_rate": 5e-05,
      "loss": 1.0352,
      "step": 1602
    },
    {
      "epoch": 4.403846153846154,
      "grad_norm": 1.340163230895996,
      "learning_rate": 5e-05,
      "loss": 1.0275,
      "step": 1603
    },
    {
      "epoch": 4.406593406593407,
      "grad_norm": 1.2220826148986816,
      "learning_rate": 5e-05,
      "loss": 0.923,
      "step": 1604
    },
    {
      "epoch": 4.40934065934066,
      "grad_norm": 1.2878576517105103,
      "learning_rate": 5e-05,
      "loss": 0.9435,
      "step": 1605
    },
    {
      "epoch": 4.412087912087912,
      "grad_norm": 1.294143795967102,
      "learning_rate": 5e-05,
      "loss": 1.0419,
      "step": 1606
    },
    {
      "epoch": 4.414835164835165,
      "grad_norm": 1.2800849676132202,
      "learning_rate": 5e-05,
      "loss": 0.967,
      "step": 1607
    },
    {
      "epoch": 4.417582417582418,
      "grad_norm": 1.2958288192749023,
      "learning_rate": 5e-05,
      "loss": 0.9969,
      "step": 1608
    },
    {
      "epoch": 4.420329670329671,
      "grad_norm": 1.2643555402755737,
      "learning_rate": 5e-05,
      "loss": 0.9704,
      "step": 1609
    },
    {
      "epoch": 4.423076923076923,
      "grad_norm": 1.2613848447799683,
      "learning_rate": 5e-05,
      "loss": 0.9727,
      "step": 1610
    },
    {
      "epoch": 4.425824175824176,
      "grad_norm": 1.2800333499908447,
      "learning_rate": 5e-05,
      "loss": 0.9662,
      "step": 1611
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 1.276883840560913,
      "learning_rate": 5e-05,
      "loss": 1.0324,
      "step": 1612
    },
    {
      "epoch": 4.431318681318682,
      "grad_norm": 1.3039206266403198,
      "learning_rate": 5e-05,
      "loss": 1.0647,
      "step": 1613
    },
    {
      "epoch": 4.434065934065934,
      "grad_norm": 1.2715703248977661,
      "learning_rate": 5e-05,
      "loss": 0.9703,
      "step": 1614
    },
    {
      "epoch": 4.436813186813187,
      "grad_norm": 1.2880182266235352,
      "learning_rate": 5e-05,
      "loss": 0.9943,
      "step": 1615
    },
    {
      "epoch": 4.43956043956044,
      "grad_norm": 1.3220770359039307,
      "learning_rate": 5e-05,
      "loss": 1.0131,
      "step": 1616
    },
    {
      "epoch": 4.4423076923076925,
      "grad_norm": 1.2903411388397217,
      "learning_rate": 5e-05,
      "loss": 1.0059,
      "step": 1617
    },
    {
      "epoch": 4.445054945054945,
      "grad_norm": 1.267003059387207,
      "learning_rate": 5e-05,
      "loss": 0.9566,
      "step": 1618
    },
    {
      "epoch": 4.447802197802198,
      "grad_norm": 1.3309112787246704,
      "learning_rate": 5e-05,
      "loss": 1.0522,
      "step": 1619
    },
    {
      "epoch": 4.450549450549451,
      "grad_norm": 1.3038551807403564,
      "learning_rate": 5e-05,
      "loss": 0.9499,
      "step": 1620
    },
    {
      "epoch": 4.4532967032967035,
      "grad_norm": 1.2804299592971802,
      "learning_rate": 5e-05,
      "loss": 0.9846,
      "step": 1621
    },
    {
      "epoch": 4.456043956043956,
      "grad_norm": 1.287401795387268,
      "learning_rate": 5e-05,
      "loss": 0.9237,
      "step": 1622
    },
    {
      "epoch": 4.458791208791209,
      "grad_norm": 1.2654780149459839,
      "learning_rate": 5e-05,
      "loss": 0.9362,
      "step": 1623
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 1.2542109489440918,
      "learning_rate": 5e-05,
      "loss": 0.951,
      "step": 1624
    },
    {
      "epoch": 4.464285714285714,
      "grad_norm": 1.3001726865768433,
      "learning_rate": 5e-05,
      "loss": 0.9965,
      "step": 1625
    },
    {
      "epoch": 4.467032967032967,
      "grad_norm": 1.2921326160430908,
      "learning_rate": 5e-05,
      "loss": 1.0028,
      "step": 1626
    },
    {
      "epoch": 4.46978021978022,
      "grad_norm": 1.3001376390457153,
      "learning_rate": 5e-05,
      "loss": 1.0246,
      "step": 1627
    },
    {
      "epoch": 4.472527472527473,
      "grad_norm": 1.2504633665084839,
      "learning_rate": 5e-05,
      "loss": 0.9259,
      "step": 1628
    },
    {
      "epoch": 4.475274725274725,
      "grad_norm": 1.3068804740905762,
      "learning_rate": 5e-05,
      "loss": 1.0286,
      "step": 1629
    },
    {
      "epoch": 4.478021978021978,
      "grad_norm": 1.272333025932312,
      "learning_rate": 5e-05,
      "loss": 0.9722,
      "step": 1630
    },
    {
      "epoch": 4.480769230769231,
      "grad_norm": 1.3163114786148071,
      "learning_rate": 5e-05,
      "loss": 1.0202,
      "step": 1631
    },
    {
      "epoch": 4.483516483516484,
      "grad_norm": 1.3051913976669312,
      "learning_rate": 5e-05,
      "loss": 0.9447,
      "step": 1632
    },
    {
      "epoch": 4.486263736263736,
      "grad_norm": 1.2881194353103638,
      "learning_rate": 5e-05,
      "loss": 1.0155,
      "step": 1633
    },
    {
      "epoch": 4.489010989010989,
      "grad_norm": 1.2762197256088257,
      "learning_rate": 5e-05,
      "loss": 0.9347,
      "step": 1634
    },
    {
      "epoch": 4.491758241758242,
      "grad_norm": 1.2466378211975098,
      "learning_rate": 5e-05,
      "loss": 0.9275,
      "step": 1635
    },
    {
      "epoch": 4.4945054945054945,
      "grad_norm": 1.2479742765426636,
      "learning_rate": 5e-05,
      "loss": 0.9861,
      "step": 1636
    },
    {
      "epoch": 4.497252747252747,
      "grad_norm": 1.2103432416915894,
      "learning_rate": 5e-05,
      "loss": 0.905,
      "step": 1637
    },
    {
      "epoch": 4.5,
      "grad_norm": 1.2775578498840332,
      "learning_rate": 5e-05,
      "loss": 0.9347,
      "step": 1638
    },
    {
      "epoch": 4.5,
      "eval_loss": 1.6875070333480835,
      "eval_runtime": 107.7199,
      "eval_samples_per_second": 384.377,
      "eval_steps_per_second": 3.008,
      "step": 1638
    },
    {
      "epoch": 4.502747252747253,
      "grad_norm": 1.3222073316574097,
      "learning_rate": 5e-05,
      "loss": 0.9885,
      "step": 1639
    },
    {
      "epoch": 4.5054945054945055,
      "grad_norm": 1.2864075899124146,
      "learning_rate": 5e-05,
      "loss": 0.9852,
      "step": 1640
    },
    {
      "epoch": 4.508241758241758,
      "grad_norm": 1.2795947790145874,
      "learning_rate": 5e-05,
      "loss": 0.923,
      "step": 1641
    },
    {
      "epoch": 4.510989010989011,
      "grad_norm": 1.2879953384399414,
      "learning_rate": 5e-05,
      "loss": 1.006,
      "step": 1642
    },
    {
      "epoch": 4.513736263736264,
      "grad_norm": 1.2819328308105469,
      "learning_rate": 5e-05,
      "loss": 0.969,
      "step": 1643
    },
    {
      "epoch": 4.516483516483516,
      "grad_norm": 1.322898030281067,
      "learning_rate": 5e-05,
      "loss": 1.0035,
      "step": 1644
    },
    {
      "epoch": 4.519230769230769,
      "grad_norm": 1.268500804901123,
      "learning_rate": 5e-05,
      "loss": 0.9695,
      "step": 1645
    },
    {
      "epoch": 4.521978021978022,
      "grad_norm": 1.2982498407363892,
      "learning_rate": 5e-05,
      "loss": 1.0341,
      "step": 1646
    },
    {
      "epoch": 4.524725274725275,
      "grad_norm": 1.3208293914794922,
      "learning_rate": 5e-05,
      "loss": 0.9738,
      "step": 1647
    },
    {
      "epoch": 4.527472527472527,
      "grad_norm": 1.3084250688552856,
      "learning_rate": 5e-05,
      "loss": 0.9734,
      "step": 1648
    },
    {
      "epoch": 4.53021978021978,
      "grad_norm": 1.2924392223358154,
      "learning_rate": 5e-05,
      "loss": 0.9211,
      "step": 1649
    },
    {
      "epoch": 4.532967032967033,
      "grad_norm": 1.2729146480560303,
      "learning_rate": 5e-05,
      "loss": 0.9767,
      "step": 1650
    },
    {
      "epoch": 4.535714285714286,
      "grad_norm": 1.283241629600525,
      "learning_rate": 5e-05,
      "loss": 0.9816,
      "step": 1651
    },
    {
      "epoch": 4.538461538461538,
      "grad_norm": 1.3113542795181274,
      "learning_rate": 5e-05,
      "loss": 1.0309,
      "step": 1652
    },
    {
      "epoch": 4.541208791208791,
      "grad_norm": 1.2657604217529297,
      "learning_rate": 5e-05,
      "loss": 0.9901,
      "step": 1653
    },
    {
      "epoch": 4.543956043956044,
      "grad_norm": 1.339352011680603,
      "learning_rate": 5e-05,
      "loss": 1.0612,
      "step": 1654
    },
    {
      "epoch": 4.5467032967032965,
      "grad_norm": 1.2946476936340332,
      "learning_rate": 5e-05,
      "loss": 0.995,
      "step": 1655
    },
    {
      "epoch": 4.549450549450549,
      "grad_norm": 1.2892019748687744,
      "learning_rate": 5e-05,
      "loss": 0.983,
      "step": 1656
    },
    {
      "epoch": 4.552197802197802,
      "grad_norm": 1.3096606731414795,
      "learning_rate": 5e-05,
      "loss": 1.0185,
      "step": 1657
    },
    {
      "epoch": 4.554945054945055,
      "grad_norm": 1.2692209482192993,
      "learning_rate": 5e-05,
      "loss": 0.9646,
      "step": 1658
    },
    {
      "epoch": 4.5576923076923075,
      "grad_norm": 1.3004167079925537,
      "learning_rate": 5e-05,
      "loss": 0.9861,
      "step": 1659
    },
    {
      "epoch": 4.56043956043956,
      "grad_norm": 1.3003547191619873,
      "learning_rate": 5e-05,
      "loss": 0.9371,
      "step": 1660
    },
    {
      "epoch": 4.563186813186813,
      "grad_norm": 1.3170112371444702,
      "learning_rate": 5e-05,
      "loss": 1.0419,
      "step": 1661
    },
    {
      "epoch": 4.565934065934066,
      "grad_norm": 1.2575443983078003,
      "learning_rate": 5e-05,
      "loss": 0.9699,
      "step": 1662
    },
    {
      "epoch": 4.568681318681318,
      "grad_norm": 1.2547953128814697,
      "learning_rate": 5e-05,
      "loss": 0.9285,
      "step": 1663
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 1.302790880203247,
      "learning_rate": 5e-05,
      "loss": 1.0013,
      "step": 1664
    },
    {
      "epoch": 4.574175824175824,
      "grad_norm": 1.277238368988037,
      "learning_rate": 5e-05,
      "loss": 0.9165,
      "step": 1665
    },
    {
      "epoch": 4.576923076923077,
      "grad_norm": 1.2888789176940918,
      "learning_rate": 5e-05,
      "loss": 0.9751,
      "step": 1666
    },
    {
      "epoch": 4.579670329670329,
      "grad_norm": 1.2544267177581787,
      "learning_rate": 5e-05,
      "loss": 0.9302,
      "step": 1667
    },
    {
      "epoch": 4.582417582417582,
      "grad_norm": 1.2771278619766235,
      "learning_rate": 5e-05,
      "loss": 0.9375,
      "step": 1668
    },
    {
      "epoch": 4.585164835164835,
      "grad_norm": 1.2616552114486694,
      "learning_rate": 5e-05,
      "loss": 0.8754,
      "step": 1669
    },
    {
      "epoch": 4.587912087912088,
      "grad_norm": 1.2924363613128662,
      "learning_rate": 5e-05,
      "loss": 0.9607,
      "step": 1670
    },
    {
      "epoch": 4.59065934065934,
      "grad_norm": 1.277541995048523,
      "learning_rate": 5e-05,
      "loss": 0.9328,
      "step": 1671
    },
    {
      "epoch": 4.593406593406593,
      "grad_norm": 1.3037230968475342,
      "learning_rate": 5e-05,
      "loss": 1.035,
      "step": 1672
    },
    {
      "epoch": 4.596153846153846,
      "grad_norm": 1.350354790687561,
      "learning_rate": 5e-05,
      "loss": 0.9803,
      "step": 1673
    },
    {
      "epoch": 4.5989010989010985,
      "grad_norm": 1.2427600622177124,
      "learning_rate": 5e-05,
      "loss": 0.9014,
      "step": 1674
    },
    {
      "epoch": 4.601648351648351,
      "grad_norm": 1.3162120580673218,
      "learning_rate": 5e-05,
      "loss": 0.96,
      "step": 1675
    },
    {
      "epoch": 4.604395604395604,
      "grad_norm": 1.2988992929458618,
      "learning_rate": 5e-05,
      "loss": 1.0196,
      "step": 1676
    },
    {
      "epoch": 4.607142857142857,
      "grad_norm": 1.2810481786727905,
      "learning_rate": 5e-05,
      "loss": 0.9125,
      "step": 1677
    },
    {
      "epoch": 4.6098901098901095,
      "grad_norm": 1.334313154220581,
      "learning_rate": 5e-05,
      "loss": 1.0463,
      "step": 1678
    },
    {
      "epoch": 4.612637362637363,
      "grad_norm": 1.2896554470062256,
      "learning_rate": 5e-05,
      "loss": 0.9349,
      "step": 1679
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 1.2712969779968262,
      "learning_rate": 5e-05,
      "loss": 0.9662,
      "step": 1680
    },
    {
      "epoch": 4.618131868131869,
      "grad_norm": 1.3223130702972412,
      "learning_rate": 5e-05,
      "loss": 1.02,
      "step": 1681
    },
    {
      "epoch": 4.6208791208791204,
      "grad_norm": 1.2391202449798584,
      "learning_rate": 5e-05,
      "loss": 0.9116,
      "step": 1682
    },
    {
      "epoch": 4.623626373626374,
      "grad_norm": 1.3026375770568848,
      "learning_rate": 5e-05,
      "loss": 0.9942,
      "step": 1683
    },
    {
      "epoch": 4.626373626373626,
      "grad_norm": 1.3292502164840698,
      "learning_rate": 5e-05,
      "loss": 1.0222,
      "step": 1684
    },
    {
      "epoch": 4.6291208791208796,
      "grad_norm": 1.2743537425994873,
      "learning_rate": 5e-05,
      "loss": 0.945,
      "step": 1685
    },
    {
      "epoch": 4.631868131868131,
      "grad_norm": 1.2947065830230713,
      "learning_rate": 5e-05,
      "loss": 0.9616,
      "step": 1686
    },
    {
      "epoch": 4.634615384615385,
      "grad_norm": 1.2849838733673096,
      "learning_rate": 5e-05,
      "loss": 0.932,
      "step": 1687
    },
    {
      "epoch": 4.637362637362637,
      "grad_norm": 1.2669237852096558,
      "learning_rate": 5e-05,
      "loss": 1.0075,
      "step": 1688
    },
    {
      "epoch": 4.6401098901098905,
      "grad_norm": 1.281837821006775,
      "learning_rate": 5e-05,
      "loss": 0.9693,
      "step": 1689
    },
    {
      "epoch": 4.642857142857143,
      "grad_norm": 1.3537100553512573,
      "learning_rate": 5e-05,
      "loss": 1.0024,
      "step": 1690
    },
    {
      "epoch": 4.645604395604396,
      "grad_norm": 1.295398473739624,
      "learning_rate": 5e-05,
      "loss": 0.987,
      "step": 1691
    },
    {
      "epoch": 4.648351648351649,
      "grad_norm": 1.3668568134307861,
      "learning_rate": 5e-05,
      "loss": 0.9814,
      "step": 1692
    },
    {
      "epoch": 4.6510989010989015,
      "grad_norm": 1.3192996978759766,
      "learning_rate": 5e-05,
      "loss": 1.0094,
      "step": 1693
    },
    {
      "epoch": 4.653846153846154,
      "grad_norm": 1.2930821180343628,
      "learning_rate": 5e-05,
      "loss": 0.9148,
      "step": 1694
    },
    {
      "epoch": 4.656593406593407,
      "grad_norm": 1.2285451889038086,
      "learning_rate": 5e-05,
      "loss": 0.8986,
      "step": 1695
    },
    {
      "epoch": 4.65934065934066,
      "grad_norm": 1.3287146091461182,
      "learning_rate": 5e-05,
      "loss": 0.9672,
      "step": 1696
    },
    {
      "epoch": 4.662087912087912,
      "grad_norm": 1.2719262838363647,
      "learning_rate": 5e-05,
      "loss": 0.9504,
      "step": 1697
    },
    {
      "epoch": 4.664835164835165,
      "grad_norm": 1.262009859085083,
      "learning_rate": 5e-05,
      "loss": 0.9345,
      "step": 1698
    },
    {
      "epoch": 4.667582417582418,
      "grad_norm": 1.2832458019256592,
      "learning_rate": 5e-05,
      "loss": 0.9724,
      "step": 1699
    },
    {
      "epoch": 4.670329670329671,
      "grad_norm": 1.306747555732727,
      "learning_rate": 5e-05,
      "loss": 1.0312,
      "step": 1700
    },
    {
      "epoch": 4.673076923076923,
      "grad_norm": 1.2443256378173828,
      "learning_rate": 5e-05,
      "loss": 0.9034,
      "step": 1701
    },
    {
      "epoch": 4.675824175824176,
      "grad_norm": 1.2956475019454956,
      "learning_rate": 5e-05,
      "loss": 1.0067,
      "step": 1702
    },
    {
      "epoch": 4.678571428571429,
      "grad_norm": 1.2760692834854126,
      "learning_rate": 5e-05,
      "loss": 0.9659,
      "step": 1703
    },
    {
      "epoch": 4.681318681318682,
      "grad_norm": 1.3063815832138062,
      "learning_rate": 5e-05,
      "loss": 0.9886,
      "step": 1704
    },
    {
      "epoch": 4.684065934065934,
      "grad_norm": 1.3251575231552124,
      "learning_rate": 5e-05,
      "loss": 1.0164,
      "step": 1705
    },
    {
      "epoch": 4.686813186813187,
      "grad_norm": 1.2920653820037842,
      "learning_rate": 5e-05,
      "loss": 0.9542,
      "step": 1706
    },
    {
      "epoch": 4.68956043956044,
      "grad_norm": 1.3123058080673218,
      "learning_rate": 5e-05,
      "loss": 1.0035,
      "step": 1707
    },
    {
      "epoch": 4.6923076923076925,
      "grad_norm": 1.3140292167663574,
      "learning_rate": 5e-05,
      "loss": 1.0138,
      "step": 1708
    },
    {
      "epoch": 4.695054945054945,
      "grad_norm": 1.3084135055541992,
      "learning_rate": 5e-05,
      "loss": 0.9967,
      "step": 1709
    },
    {
      "epoch": 4.697802197802198,
      "grad_norm": 1.2939022779464722,
      "learning_rate": 5e-05,
      "loss": 0.9331,
      "step": 1710
    },
    {
      "epoch": 4.700549450549451,
      "grad_norm": 1.3157896995544434,
      "learning_rate": 5e-05,
      "loss": 0.9649,
      "step": 1711
    },
    {
      "epoch": 4.7032967032967035,
      "grad_norm": 1.276822805404663,
      "learning_rate": 5e-05,
      "loss": 0.9671,
      "step": 1712
    },
    {
      "epoch": 4.706043956043956,
      "grad_norm": 1.291975975036621,
      "learning_rate": 5e-05,
      "loss": 0.9472,
      "step": 1713
    },
    {
      "epoch": 4.708791208791209,
      "grad_norm": 1.2986565828323364,
      "learning_rate": 5e-05,
      "loss": 0.9427,
      "step": 1714
    },
    {
      "epoch": 4.711538461538462,
      "grad_norm": 1.3440810441970825,
      "learning_rate": 5e-05,
      "loss": 0.9936,
      "step": 1715
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 1.2633872032165527,
      "learning_rate": 5e-05,
      "loss": 0.9568,
      "step": 1716
    },
    {
      "epoch": 4.717032967032967,
      "grad_norm": 1.30418062210083,
      "learning_rate": 5e-05,
      "loss": 0.9584,
      "step": 1717
    },
    {
      "epoch": 4.71978021978022,
      "grad_norm": 1.2572479248046875,
      "learning_rate": 5e-05,
      "loss": 0.9789,
      "step": 1718
    },
    {
      "epoch": 4.722527472527473,
      "grad_norm": 1.2722833156585693,
      "learning_rate": 5e-05,
      "loss": 0.9592,
      "step": 1719
    },
    {
      "epoch": 4.725274725274725,
      "grad_norm": 1.278315544128418,
      "learning_rate": 5e-05,
      "loss": 0.9884,
      "step": 1720
    },
    {
      "epoch": 4.728021978021978,
      "grad_norm": 1.3082866668701172,
      "learning_rate": 5e-05,
      "loss": 0.9443,
      "step": 1721
    },
    {
      "epoch": 4.730769230769231,
      "grad_norm": 1.2862234115600586,
      "learning_rate": 5e-05,
      "loss": 0.9856,
      "step": 1722
    },
    {
      "epoch": 4.733516483516484,
      "grad_norm": 1.3313748836517334,
      "learning_rate": 5e-05,
      "loss": 0.8917,
      "step": 1723
    },
    {
      "epoch": 4.736263736263736,
      "grad_norm": 1.3781834840774536,
      "learning_rate": 5e-05,
      "loss": 1.0422,
      "step": 1724
    },
    {
      "epoch": 4.739010989010989,
      "grad_norm": 1.2934937477111816,
      "learning_rate": 5e-05,
      "loss": 0.9745,
      "step": 1725
    },
    {
      "epoch": 4.741758241758242,
      "grad_norm": 1.2790600061416626,
      "learning_rate": 5e-05,
      "loss": 0.9326,
      "step": 1726
    },
    {
      "epoch": 4.7445054945054945,
      "grad_norm": 1.3365824222564697,
      "learning_rate": 5e-05,
      "loss": 0.9644,
      "step": 1727
    },
    {
      "epoch": 4.747252747252747,
      "grad_norm": 1.3138214349746704,
      "learning_rate": 5e-05,
      "loss": 0.9812,
      "step": 1728
    },
    {
      "epoch": 4.75,
      "grad_norm": 1.3216309547424316,
      "learning_rate": 5e-05,
      "loss": 0.991,
      "step": 1729
    },
    {
      "epoch": 4.752747252747253,
      "grad_norm": 1.3107056617736816,
      "learning_rate": 5e-05,
      "loss": 0.9334,
      "step": 1730
    },
    {
      "epoch": 4.7554945054945055,
      "grad_norm": 1.3025541305541992,
      "learning_rate": 5e-05,
      "loss": 0.9942,
      "step": 1731
    },
    {
      "epoch": 4.758241758241758,
      "grad_norm": 1.271776556968689,
      "learning_rate": 5e-05,
      "loss": 0.9563,
      "step": 1732
    },
    {
      "epoch": 4.760989010989011,
      "grad_norm": 1.296298623085022,
      "learning_rate": 5e-05,
      "loss": 0.9953,
      "step": 1733
    },
    {
      "epoch": 4.763736263736264,
      "grad_norm": 1.3392878770828247,
      "learning_rate": 5e-05,
      "loss": 0.9795,
      "step": 1734
    },
    {
      "epoch": 4.766483516483516,
      "grad_norm": 1.2791379690170288,
      "learning_rate": 5e-05,
      "loss": 0.9616,
      "step": 1735
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 1.2801365852355957,
      "learning_rate": 5e-05,
      "loss": 0.9437,
      "step": 1736
    },
    {
      "epoch": 4.771978021978022,
      "grad_norm": 1.3708691596984863,
      "learning_rate": 5e-05,
      "loss": 1.0249,
      "step": 1737
    },
    {
      "epoch": 4.774725274725275,
      "grad_norm": 1.290283441543579,
      "learning_rate": 5e-05,
      "loss": 1.0023,
      "step": 1738
    },
    {
      "epoch": 4.777472527472527,
      "grad_norm": 1.3195459842681885,
      "learning_rate": 5e-05,
      "loss": 1.0246,
      "step": 1739
    },
    {
      "epoch": 4.78021978021978,
      "grad_norm": 1.3037190437316895,
      "learning_rate": 5e-05,
      "loss": 0.9727,
      "step": 1740
    },
    {
      "epoch": 4.782967032967033,
      "grad_norm": 1.3143093585968018,
      "learning_rate": 5e-05,
      "loss": 0.9709,
      "step": 1741
    },
    {
      "epoch": 4.785714285714286,
      "grad_norm": 1.3037954568862915,
      "learning_rate": 5e-05,
      "loss": 0.9334,
      "step": 1742
    },
    {
      "epoch": 4.788461538461538,
      "grad_norm": 1.3100144863128662,
      "learning_rate": 5e-05,
      "loss": 1.0083,
      "step": 1743
    },
    {
      "epoch": 4.791208791208791,
      "grad_norm": 1.326008677482605,
      "learning_rate": 5e-05,
      "loss": 1.0362,
      "step": 1744
    },
    {
      "epoch": 4.793956043956044,
      "grad_norm": 1.3224304914474487,
      "learning_rate": 5e-05,
      "loss": 0.9471,
      "step": 1745
    },
    {
      "epoch": 4.7967032967032965,
      "grad_norm": 1.2867246866226196,
      "learning_rate": 5e-05,
      "loss": 0.9481,
      "step": 1746
    },
    {
      "epoch": 4.799450549450549,
      "grad_norm": 1.3109241724014282,
      "learning_rate": 5e-05,
      "loss": 0.9459,
      "step": 1747
    },
    {
      "epoch": 4.802197802197802,
      "grad_norm": 1.2781637907028198,
      "learning_rate": 5e-05,
      "loss": 0.941,
      "step": 1748
    },
    {
      "epoch": 4.804945054945055,
      "grad_norm": 1.2774076461791992,
      "learning_rate": 5e-05,
      "loss": 0.904,
      "step": 1749
    },
    {
      "epoch": 4.8076923076923075,
      "grad_norm": 1.2592825889587402,
      "learning_rate": 5e-05,
      "loss": 0.9361,
      "step": 1750
    },
    {
      "epoch": 4.81043956043956,
      "grad_norm": 1.2995795011520386,
      "learning_rate": 5e-05,
      "loss": 0.9383,
      "step": 1751
    },
    {
      "epoch": 4.813186813186813,
      "grad_norm": 1.2978333234786987,
      "learning_rate": 5e-05,
      "loss": 0.8991,
      "step": 1752
    },
    {
      "epoch": 4.815934065934066,
      "grad_norm": 1.3272696733474731,
      "learning_rate": 5e-05,
      "loss": 0.9642,
      "step": 1753
    },
    {
      "epoch": 4.818681318681318,
      "grad_norm": 1.31813383102417,
      "learning_rate": 5e-05,
      "loss": 1.0031,
      "step": 1754
    },
    {
      "epoch": 4.821428571428571,
      "grad_norm": 1.2356863021850586,
      "learning_rate": 5e-05,
      "loss": 0.9185,
      "step": 1755
    },
    {
      "epoch": 4.824175824175824,
      "grad_norm": 1.2802406549453735,
      "learning_rate": 5e-05,
      "loss": 0.9109,
      "step": 1756
    },
    {
      "epoch": 4.826923076923077,
      "grad_norm": 1.3492437601089478,
      "learning_rate": 5e-05,
      "loss": 1.0328,
      "step": 1757
    },
    {
      "epoch": 4.829670329670329,
      "grad_norm": 1.3078341484069824,
      "learning_rate": 5e-05,
      "loss": 0.9732,
      "step": 1758
    },
    {
      "epoch": 4.832417582417582,
      "grad_norm": 1.3027223348617554,
      "learning_rate": 5e-05,
      "loss": 0.977,
      "step": 1759
    },
    {
      "epoch": 4.835164835164835,
      "grad_norm": 1.2855172157287598,
      "learning_rate": 5e-05,
      "loss": 0.8948,
      "step": 1760
    },
    {
      "epoch": 4.837912087912088,
      "grad_norm": 1.3102229833602905,
      "learning_rate": 5e-05,
      "loss": 0.9659,
      "step": 1761
    },
    {
      "epoch": 4.84065934065934,
      "grad_norm": 1.3254777193069458,
      "learning_rate": 5e-05,
      "loss": 0.9517,
      "step": 1762
    },
    {
      "epoch": 4.843406593406593,
      "grad_norm": 1.276340126991272,
      "learning_rate": 5e-05,
      "loss": 0.9318,
      "step": 1763
    },
    {
      "epoch": 4.846153846153846,
      "grad_norm": 1.2613509893417358,
      "learning_rate": 5e-05,
      "loss": 0.9096,
      "step": 1764
    },
    {
      "epoch": 4.8489010989010985,
      "grad_norm": 1.2771389484405518,
      "learning_rate": 5e-05,
      "loss": 0.9641,
      "step": 1765
    },
    {
      "epoch": 4.851648351648351,
      "grad_norm": 1.3017617464065552,
      "learning_rate": 5e-05,
      "loss": 1.018,
      "step": 1766
    },
    {
      "epoch": 4.854395604395604,
      "grad_norm": 1.314270257949829,
      "learning_rate": 5e-05,
      "loss": 0.9598,
      "step": 1767
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 1.2967389822006226,
      "learning_rate": 5e-05,
      "loss": 0.9762,
      "step": 1768
    },
    {
      "epoch": 4.8598901098901095,
      "grad_norm": 1.21867036819458,
      "learning_rate": 5e-05,
      "loss": 0.8967,
      "step": 1769
    },
    {
      "epoch": 4.862637362637363,
      "grad_norm": 1.235298752784729,
      "learning_rate": 5e-05,
      "loss": 0.8586,
      "step": 1770
    },
    {
      "epoch": 4.865384615384615,
      "grad_norm": 1.2849183082580566,
      "learning_rate": 5e-05,
      "loss": 0.9685,
      "step": 1771
    },
    {
      "epoch": 4.868131868131869,
      "grad_norm": 1.2994704246520996,
      "learning_rate": 5e-05,
      "loss": 0.9702,
      "step": 1772
    },
    {
      "epoch": 4.8708791208791204,
      "grad_norm": 1.2907202243804932,
      "learning_rate": 5e-05,
      "loss": 0.9523,
      "step": 1773
    },
    {
      "epoch": 4.873626373626374,
      "grad_norm": 1.3006153106689453,
      "learning_rate": 5e-05,
      "loss": 0.913,
      "step": 1774
    },
    {
      "epoch": 4.876373626373626,
      "grad_norm": 1.3507591485977173,
      "learning_rate": 5e-05,
      "loss": 1.0743,
      "step": 1775
    },
    {
      "epoch": 4.8791208791208796,
      "grad_norm": 1.3394442796707153,
      "learning_rate": 5e-05,
      "loss": 1.0006,
      "step": 1776
    },
    {
      "epoch": 4.881868131868131,
      "grad_norm": 1.3382149934768677,
      "learning_rate": 5e-05,
      "loss": 0.9887,
      "step": 1777
    },
    {
      "epoch": 4.884615384615385,
      "grad_norm": 1.287244439125061,
      "learning_rate": 5e-05,
      "loss": 1.0079,
      "step": 1778
    },
    {
      "epoch": 4.887362637362637,
      "grad_norm": 1.258374810218811,
      "learning_rate": 5e-05,
      "loss": 0.8975,
      "step": 1779
    },
    {
      "epoch": 4.8901098901098905,
      "grad_norm": 1.2417171001434326,
      "learning_rate": 5e-05,
      "loss": 0.8872,
      "step": 1780
    },
    {
      "epoch": 4.892857142857143,
      "grad_norm": 1.2909783124923706,
      "learning_rate": 5e-05,
      "loss": 0.904,
      "step": 1781
    },
    {
      "epoch": 4.895604395604396,
      "grad_norm": 1.2639278173446655,
      "learning_rate": 5e-05,
      "loss": 0.922,
      "step": 1782
    },
    {
      "epoch": 4.898351648351649,
      "grad_norm": 1.3298436403274536,
      "learning_rate": 5e-05,
      "loss": 1.0301,
      "step": 1783
    },
    {
      "epoch": 4.9010989010989015,
      "grad_norm": 1.2691956758499146,
      "learning_rate": 5e-05,
      "loss": 0.9184,
      "step": 1784
    },
    {
      "epoch": 4.903846153846154,
      "grad_norm": 1.27550208568573,
      "learning_rate": 5e-05,
      "loss": 0.9391,
      "step": 1785
    },
    {
      "epoch": 4.906593406593407,
      "grad_norm": 1.2513247728347778,
      "learning_rate": 5e-05,
      "loss": 0.9162,
      "step": 1786
    },
    {
      "epoch": 4.90934065934066,
      "grad_norm": 1.2677310705184937,
      "learning_rate": 5e-05,
      "loss": 0.9433,
      "step": 1787
    },
    {
      "epoch": 4.912087912087912,
      "grad_norm": 1.2816296815872192,
      "learning_rate": 5e-05,
      "loss": 0.9755,
      "step": 1788
    },
    {
      "epoch": 4.914835164835165,
      "grad_norm": 1.318981409072876,
      "learning_rate": 5e-05,
      "loss": 0.8992,
      "step": 1789
    },
    {
      "epoch": 4.917582417582418,
      "grad_norm": 1.2578532695770264,
      "learning_rate": 5e-05,
      "loss": 0.8811,
      "step": 1790
    },
    {
      "epoch": 4.920329670329671,
      "grad_norm": 1.3141945600509644,
      "learning_rate": 5e-05,
      "loss": 0.9495,
      "step": 1791
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 1.2261414527893066,
      "learning_rate": 5e-05,
      "loss": 0.8638,
      "step": 1792
    },
    {
      "epoch": 4.925824175824176,
      "grad_norm": 1.245194673538208,
      "learning_rate": 5e-05,
      "loss": 0.9309,
      "step": 1793
    },
    {
      "epoch": 4.928571428571429,
      "grad_norm": 1.3135775327682495,
      "learning_rate": 5e-05,
      "loss": 0.9332,
      "step": 1794
    },
    {
      "epoch": 4.931318681318682,
      "grad_norm": 1.242186188697815,
      "learning_rate": 5e-05,
      "loss": 0.8832,
      "step": 1795
    },
    {
      "epoch": 4.934065934065934,
      "grad_norm": 1.2853575944900513,
      "learning_rate": 5e-05,
      "loss": 0.9538,
      "step": 1796
    },
    {
      "epoch": 4.936813186813187,
      "grad_norm": 1.3186756372451782,
      "learning_rate": 5e-05,
      "loss": 0.9671,
      "step": 1797
    },
    {
      "epoch": 4.93956043956044,
      "grad_norm": 1.2998517751693726,
      "learning_rate": 5e-05,
      "loss": 0.9695,
      "step": 1798
    },
    {
      "epoch": 4.9423076923076925,
      "grad_norm": 1.3360916376113892,
      "learning_rate": 5e-05,
      "loss": 0.9383,
      "step": 1799
    },
    {
      "epoch": 4.945054945054945,
      "grad_norm": 1.304797649383545,
      "learning_rate": 5e-05,
      "loss": 0.9468,
      "step": 1800
    },
    {
      "epoch": 4.947802197802198,
      "grad_norm": 1.3076562881469727,
      "learning_rate": 5e-05,
      "loss": 0.932,
      "step": 1801
    },
    {
      "epoch": 4.950549450549451,
      "grad_norm": 1.276331901550293,
      "learning_rate": 5e-05,
      "loss": 0.915,
      "step": 1802
    },
    {
      "epoch": 4.9532967032967035,
      "grad_norm": 1.2599139213562012,
      "learning_rate": 5e-05,
      "loss": 0.9635,
      "step": 1803
    },
    {
      "epoch": 4.956043956043956,
      "grad_norm": 1.308229684829712,
      "learning_rate": 5e-05,
      "loss": 0.9372,
      "step": 1804
    },
    {
      "epoch": 4.958791208791209,
      "grad_norm": 1.2872706651687622,
      "learning_rate": 5e-05,
      "loss": 0.9677,
      "step": 1805
    },
    {
      "epoch": 4.961538461538462,
      "grad_norm": 1.282889485359192,
      "learning_rate": 5e-05,
      "loss": 0.9314,
      "step": 1806
    },
    {
      "epoch": 4.964285714285714,
      "grad_norm": 1.3079301118850708,
      "learning_rate": 5e-05,
      "loss": 0.9868,
      "step": 1807
    },
    {
      "epoch": 4.967032967032967,
      "grad_norm": 1.2789748907089233,
      "learning_rate": 5e-05,
      "loss": 0.969,
      "step": 1808
    },
    {
      "epoch": 4.96978021978022,
      "grad_norm": 1.3122763633728027,
      "learning_rate": 5e-05,
      "loss": 1.0032,
      "step": 1809
    },
    {
      "epoch": 4.972527472527473,
      "grad_norm": 1.2747212648391724,
      "learning_rate": 5e-05,
      "loss": 0.8885,
      "step": 1810
    },
    {
      "epoch": 4.975274725274725,
      "grad_norm": 1.334976077079773,
      "learning_rate": 5e-05,
      "loss": 0.9985,
      "step": 1811
    },
    {
      "epoch": 4.978021978021978,
      "grad_norm": 1.2883808612823486,
      "learning_rate": 5e-05,
      "loss": 0.9717,
      "step": 1812
    },
    {
      "epoch": 4.980769230769231,
      "grad_norm": 1.3175386190414429,
      "learning_rate": 5e-05,
      "loss": 0.9919,
      "step": 1813
    },
    {
      "epoch": 4.983516483516484,
      "grad_norm": 1.2769542932510376,
      "learning_rate": 5e-05,
      "loss": 0.9665,
      "step": 1814
    },
    {
      "epoch": 4.986263736263736,
      "grad_norm": 1.2352865934371948,
      "learning_rate": 5e-05,
      "loss": 0.8732,
      "step": 1815
    },
    {
      "epoch": 4.989010989010989,
      "grad_norm": 1.2869725227355957,
      "learning_rate": 5e-05,
      "loss": 0.9114,
      "step": 1816
    },
    {
      "epoch": 4.991758241758242,
      "grad_norm": 1.3187203407287598,
      "learning_rate": 5e-05,
      "loss": 0.9743,
      "step": 1817
    },
    {
      "epoch": 4.9945054945054945,
      "grad_norm": 1.3081530332565308,
      "learning_rate": 5e-05,
      "loss": 0.9311,
      "step": 1818
    },
    {
      "epoch": 4.997252747252747,
      "grad_norm": 1.2425063848495483,
      "learning_rate": 5e-05,
      "loss": 0.8754,
      "step": 1819
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.2762789726257324,
      "learning_rate": 5e-05,
      "loss": 0.9236,
      "step": 1820
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7455734014511108,
      "eval_runtime": 81.2824,
      "eval_samples_per_second": 509.397,
      "eval_steps_per_second": 3.986,
      "step": 1820
    },
    {
      "epoch": 5.0,
      "step": 1820,
      "total_flos": 0.0,
      "train_loss": 1.569366199662397,
      "train_runtime": 6497.5164,
      "train_samples_per_second": 286.764,
      "train_steps_per_second": 0.28
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 1820,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 182,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
